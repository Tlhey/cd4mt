{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# CD4MT"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 1. env"]}, {"cell_type": "code", "execution_count": 30, "metadata": {}, "outputs": [], "source": ["# old ver: from flash_attn.flash_attention import FlashAttention"]}, {"cell_type": "code", "execution_count": 31, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["Device: cuda\n", "2.7.1+cu126\n"]}], "source": ["import torch\n", "device = torch.device(\"cuda:4\" if torch.cuda.is_available() else \"cpu\")\n", "print(f\"Device: {device}\")\n", "print(torch.__version__)\n", "# env: cdp10\n", "# Python 3.10.18\n", "# torch 2.8.0\n", "# cu12"]}, {"cell_type": "code", "execution_count": 32, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["Working directory: /data1/yuchen/cd4mt\n", "Device: cuda:4\n"]}], "source": ["%matplotlib inline\n", "import os, sys, yaml, torch, numpy as np, matplotlib.pyplot as plt\n", "import soundfile as sf\n", "from pathlib import Path\n", "from IPython.display import Audio, display, HTML, Markdown\n", "import matplotlib.font_manager as fm\n", "\n", "ROOT = \"/data1/yuchen/cd4mt\"\n", "sys.path.append(\"/data1/yuchen/MusicLDM-Ext/src\")\n", "sys.path.append(ROOT)\n", "sys.path.append(f\"{ROOT}/ldm\")\n", "os.chdir(ROOT)\n", "\n", "print(f\"Working directory: {os.getcwd()}\")\n", "from src.music2latent.music2latent import EncoderDecoder\n", "\n", "from ldm.data.multitrack_datamodule import DataModuleFromConfig\n", "\n", "device = torch.device(\"cuda:4\" if torch.cuda.is_available() else \"cpu\")\n", "print(f\"Device: {device}\")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 2. config and load_data"]}, {"cell_type": "code", "execution_count": 33, "metadata": {}, "outputs": [], "source": ["CFG_PATH = \"configs/cd4mt_medium.yaml\"\n", "\n", "with open(CFG_PATH, 'r') as f:\n", "    cfg = yaml.safe_load(f)"]}, {"cell_type": "code", "execution_count": 34, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["ct_hparams loaded from /data1/yuchen/cd4mt/configs/cd4mt_medium.yaml\n", "ct_hparams: {'batch_size': 6, 'lr': 0.0001, 'ema_decay': 0.95, 'num_scales': 32, 'sigma_min': 0.002, 'sigma_max': 80.0, 'sigma_data': 0.5, 'epochs': 120, 'grad_clip': 1.0, 'log_interval': 20} | types: lr float batch_size int sigma_data float\n"]}], "source": ["# Load ct_config and ct_hparams from medium YAML (for CT) with robust typing\n", "import yaml, torch, numpy as np\n", "HCONF_PATH = '/data1/yuchen/cd4mt/configs/cd4mt_medium.yaml'\n", "with open(HCONF_PATH, 'r') as _f:\n", "    ct_config = yaml.safe_load(_f)\n", "\n", "def _get(d, path, default=None):\n", "    cur = d\n", "    for k in path.split('.'):\n", "        if not isinstance(cur, dict) or k not in cur:\n", "            return default\n", "        cur = cur[k]\n", "    return cur\n", "\n", "# Robust type casting (YAML可能把科学计数值读成字符串)\n", "_lr_raw = _get(ct_config, 'model.params.base_learning_rate', 5e-5)\n", "try:\n", "    lr = float(_lr_raw)\n", "except Exception:\n", "    print('[ct_hparams] WARN: invalid lr, fallback to 5e-5; raw=', _lr_raw)\n", "    lr = 5e-5\n", "\n", "_bs_raw = _get(ct_config, 'data.params.batch_size', 4)\n", "try:\n", "    batch_size = int(_bs_raw)\n", "except Exception:\n", "    print('[ct_hparams] WARN: invalid batch_size, fallback to 4; raw=', _bs_raw)\n", "    batch_size = 4\n", "\n", "_sigma_data_raw = _get(ct_config, 'model.params.diffusion_sigma_data', 0.5)\n", "try:\n", "    sigma_data = float(_sigma_data_raw)\n", "except Exception:\n", "    print('[ct_hparams] WARN: invalid sigma_data, fallback to 0.5; raw=', _sigma_data_raw)\n", "    sigma_data = 0.5\n", "\n", "ct_hparams = {\n", "    'batch_size': batch_size,\n", "    'lr': lr,\n", "    'ema_decay': 0.95,\n", "    'num_scales': 32,\n", "    'sigma_min': 0.002,\n", "    'sigma_max': 80.0,\n", "    'sigma_data': sigma_data,\n", "    'epochs': 120,\n", "    'grad_clip': 1.0,\n", "    'log_interval': 20,\n", "}\n", "print('ct_hparams loaded from', HCONF_PATH)\n", "print('ct_hparams:', ct_hparams, '| types: lr', type(lr).__name__, 'batch_size', type(batch_size).__name__, 'sigma_data', type(sigma_data).__name__)\n"]}, {"cell_type": "code", "execution_count": 35, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["Found 1290 tracks.\n", "sr=44100, min: 10, max: 600\n", "Keeping 1289 of 1290 tracks\n", "Data size: 26309\n", "Use mixup rate of 0.0; Use SpecAug (T,F) of (0, 0); Use blurring effect or not False\n", "| Audiostock Dataset Length:26309 | Epoch Length: 26309\n", "Found 271 tracks.\n", "sr=44100, min: 10, max: 600\n", "Keeping 270 of 271 tracks\n", "Data size: 5422\n", "Use mixup rate of 0.0; Use SpecAug (T,F) of (0, 0); Use blurring effect or not False\n", "| Audiostock Dataset Length:5422 | Epoch Length: 5422\n", "Found 152 tracks.\n", "sr=44100, min: 10, max: 600\n", "Keeping 151 of 152 tracks\n", "Data size: 3249\n", "Use mixup rate of 0.0; Use SpecAug (T,F) of (0, 0); Use blurring effect or not False\n", "| Audiostock Dataset Length:3249 | Epoch Length: 3249\n", "train_loader: 4385\n", "batch.keys(): ['fname', 'fbank_stems', 'waveform_stems', 'waveform', 'fbank']\n", "wav_stems: torch.Size([6, 4, 524288])\n", "wav_mix: torch.Size([6, 524288])\n", "Batch=6, Stems=4, Time=524288\n"]}], "source": ["dm = DataModuleFromConfig(**cfg[\"data\"][\"params\"])\n", "dm.prepare_data()\n", "dm.setup(stage=\"fit\")\n", "\n", "train_loader = dm.train_dataloader()\n", "print(f\"train_loader: {len(train_loader)}\")\n", "batch = next(iter(train_loader))\n", "print(f\"batch.keys(): {list(batch.keys())}\")\n", "\n", "wav_stems = batch[\"waveform_stems\"]  # (B, S, T)\n", "wav_mix = batch.get(\"waveform\", None)  # (B, T)\n", "\n", "print(f\"wav_stems: {wav_stems.shape}\")\n", "if wav_mix is not None:\n", "    print(f\"wav_mix: {wav_mix.shape}\")\n", "\n", "B, S, T = wav_stems.shape\n", "print(f\"Batch={B}, Stems={S}, Time={T}\")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 3. CAE test"]}, {"cell_type": "code", "execution_count": 36, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["/data1/yuchen/cd4mt/src/music2latent/music2latent\n", "\n", " stem_num 4 \n", "\n", " bass stem 0:\n", "stem_audio : (6, 524288)\n", "stem_latents.shape torch.Size([6, 64, 127]), tem_latents.dtype torch.float16, range: [-4.859, 4.062]\n", "\n", " drums stem 1:\n", "stem_audio : (6, 524288)\n", "stem_latents.shape torch.Size([6, 64, 127]), tem_latents.dtype torch.float16, range: [-4.777, 3.918]\n", "\n", " guitar stem 2:\n", "stem_audio : (6, 524288)\n", "stem_latents.shape torch.Size([6, 64, 127]), tem_latents.dtype torch.float16, range: [-4.762, 3.900]\n", "\n", " piano stem 3:\n", "stem_audio : (6, 524288)\n", "stem_latents.shape torch.Size([6, 64, 127]), tem_latents.dtype torch.float16, range: [-4.582, 4.078]\n", "latents_stacked.shape: torch.Size([6, 4, 64, 127])\n", " Batch=6, Stems=4, Channels=64, Length=127\n", " latents on : cuda:4\n"]}], "source": ["ae = EncoderDecoder(device=device)\n", "print(f\"\\n stem_num {S} \")\n", "\n", "stem_names = cfg['model']['params']['stem_names']\n", "latents_list = []\n", "encode_shapes = []\n", "\n", "for s in range(S):\n", "    stem_name = stem_names[s] if s < len(stem_names) else f\"stem_{s}\"\n", "    print(f\"\\n {stem_name} stem {s}:\")\n", "    stem_audio = wav_stems[:, s].cpu().numpy()  \n", "    print(f\"stem_audio : {stem_audio.shape}\")\n", "\n", "    stem_latents = ae.encode(stem_audio)\n", "    if isinstance(stem_latents, np.ndarray):\n", "        stem_latents = torch.from_numpy(stem_latents)\n", "    \n", "    print(f\"stem_latents.shape {stem_latents.shape}, tem_latents.dtype {stem_latents.dtype}, range: [{stem_latents.min():.3f}, {stem_latents.max():.3f}]\")\n", "    latents_list.append(stem_latents)\n", "    encode_shapes.append(stem_latents.shape)\n", "\n", "\n", "latents_stacked = torch.stack(latents_list, dim=1)  # (B, S, C, L)\n", "print(f\"latents_stacked.shape: {latents_stacked.shape}\")\n", "print(f\" Batch={latents_stacked.shape[0]}, Stems={latents_stacked.shape[1]}, Channels={latents_stacked.shape[2]}, Length={latents_stacked.shape[3]}\")\n", "latents = latents_stacked.to(device)\n", "print(f\" latents on : {device}\")"]}, {"cell_type": "code", "execution_count": 37, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["\n", "Decode bass\n", "recst.shape: torch.Size([6, 521728])\n", "range: [-0.236, 0.232]\n", "\n", "Decode drums\n", "recst.shape: torch.Size([6, 521728])\n", "range: [-0.582, 0.739]\n", "\n", "Decode guitar\n", "recst.shape: torch.Size([6, 521728])\n", "range: [-0.566, 0.462]\n", "\n", "Decode piano\n", "recst.shape: torch.Size([6, 521728])\n", "range: [-0.408, 0.432]\n", "\n", "recst shape: (6, 4, 524288)\n", "original lenght: 524288, recst length: 524288\n", "MSE: 0.002702\n"]}], "source": ["recst_list = []\n", "for s in range(S):\n", "    stem_name = stem_names[s] if s < len(stem_names) else f\"stem_{s}\"\n", "    print(f\"\\nDecode {stem_name}\")\n", "    stem_latents = latents[:, s].cpu().numpy()  # (B, C, L)\n", "    \n", "    try:\n", "        recst = ae.decode(stem_latents)\n", "        print(f\"recst.shape: {recst.shape}\")\n", "        print(f\"range: [{recst.min():.3f}, {recst.max():.3f}]\")\n", "\n", "        if isinstance(recst, torch.Tensor):\n", "            recst = recst.cpu().numpy() \n", "        current_length = recst.shape[-1]\n", "        if current_length > T:\n", "            excess = current_length - T\n", "            start_trim = excess // 2\n", "            end_trim = excess - start_trim\n", "            recst = recst[..., start_trim:current_length-end_trim]\n", "            \n", "        elif current_length < T:\n", "            deficit = T - current_length\n", "            pad_left = deficit // 2\n", "            pad_right = deficit - pad_left\n", "            recst = np.pad(recst, ((0,0), (pad_left, pad_right)), mode='constant', constant_values=0)\n", "        \n", "        recst_list.append(recst)\n", "        \n", "    except Exception as e:\n", "        raise e\n", "\n", "recst_aud = np.stack(recst_list, axis=1)  # (B, S, T')\n", "recst_tensor = torch.from_numpy(recst_aud).to(device)\n", "\n", "print(f\"\\nrecst shape: {recst_aud.shape}\")\n", "print(f\"original lenght: {T}, recst length: {recst_aud.shape[2]}\")\n", "\n", "if recst_aud.shape[2] == T:\n", "    mse_error = np.mean((wav_stems.cpu().numpy() - recst_aud)**2)\n", "    print(f\"MSE: {mse_error:.6f}\")\n", "else:\n", "    print(\"length error\")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 4. CD4MT 扩散模型初始化"]}, {"cell_type": "code", "execution_count": 38, "metadata": {}, "outputs": [], "source": ["# !pip install flash_attn\n", "# !pip install piq\n", "# !pip install blobfile"]}, {"cell_type": "code", "execution_count": 39, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["[CT] diffusion.loss_norm = l2\n", "[CT] model.convert_to_fp16() done\n", "CM UNet params: {'sigma_min': 0.0001, 'sigma_max': 3.0, 'image_size': 32, 'num_channels': 192, 'num_res_blocks': 2, 'num_heads': 6, 'num_heads_upsample': -1, 'num_head_channels': 32, 'attention_resolutions': '16,8,4', 'channel_mult': '1,2,4', 'dropout': 0.1, 'class_cond': False, 'use_checkpoint': False, 'use_scale_shift_norm': True, 'resblock_updown': False, 'use_fp16': False, 'use_new_attention_order': False, 'learn_sigma': False, 'weight_schedule': 'karras'}\n"]}], "source": ["from src.cm.script_util import create_model_and_diffusion, model_and_diffusion_defaults\n", "import torch.nn as nn\n", "\n", "with open(CFG_PATH, \"r\") as f:\n", "    cfg_fresh = yaml.safe_load(f)\n", "\n", "cm_model_params = model_and_diffusion_defaults()\n", "\n", "# Merge override from YAML if present, else derive from UNet config\n", "ovr = (cfg_fresh.get('model',{}).get('params',{}).get('cm_model_override'))\n", "if not ovr:\n", "    u = cfg_fresh['model']['params']['unet_']['params']\n", "    def _to_str_list(v):\n", "        if isinstance(v,str): return v\n", "        if isinstance(v,(list,tuple)): return ','.join(str(int(x)) for x in v)\n", "        return str(v)\n", "    ovr = {\n", "        'image_size': u.get('image_size', cm_model_params['image_size']),\n", "        'num_channels': u.get('model_channels', cm_model_params['num_channels']),\n", "        'num_res_blocks': u.get('num_res_blocks', cm_model_params['num_res_blocks']),\n", "        'channel_mult': _to_str_list(u.get('channel_mult','1,2,4')),\n", "        'num_heads': u.get('num_heads', cm_model_params['num_heads']),\n", "        'num_head_channels': u.get('num_head_channels', 32),\n", "        'num_heads_upsample': -1,\n", "        'attention_resolutions': _to_str_list(u.get('attention_resolutions',[8,4,2])),\n", "        'dropout': u.get('dropout', 0.1),\n", "        'class_cond': False,\n", "        'use_checkpoint': False,\n", "        'use_scale_shift_norm': True,\n", "        'resblock_updown': False,\n", "        'use_fp16': False,\n", "        'use_new_attention_order': False,\n", "        'learn_sigma': False,\n", "        'weight_schedule': 'karras',\n", "        'sigma_min': cfg_fresh['model']['params'].get('sigma_min', cm_model_params['sigma_min']),\n", "        'sigma_max': cfg_fresh['model']['params'].get('sigma_max', cm_model_params['sigma_max']),\n", "    }\n", "else:\n", "    ovr = dict(ovr)\n", "# Enforce flash-attn constraint\n", "ovr['num_head_channels'] = int(ovr.get('num_head_channels',32))\n", "if ovr['num_head_channels'] not in (16,32,64):\n", "    ovr['num_head_channels'] = 32\n", "# Normalize string fields\n", "for k in ['channel_mult','attention_resolutions']:\n", "    if k in ovr and not isinstance(ovr[k], str):\n", "        if isinstance(ovr[k], (list,tuple)):\n", "            ovr[k] = ','.join(str(int(x)) for x in ovr[k])\n", "        else:\n", "            ovr[k] = str(ovr[k])\n", "cm_model_params.update(ovr)\n", "\n", "# Sanity check divisibility per level\n", "_ch_mult = [int(x) for x in cm_model_params['channel_mult'].split(',')]\n", "assert all((cm_model_params['num_channels']*m) % cm_model_params['num_head_channels'] == 0 for m in _ch_mult),     f\"num_channels*channel_mult must be divisible by num_head_channels; got num_channels={cm_model_params['num_channels']}, channel_mult={_ch_mult}, num_head_channels={cm_model_params['num_head_channels']}\"\n", "\n", "# Build model and diffusion\n", "in_ch = cfg_fresh['model']['params']['unet_']['params']['in_channels']\n", "out_ch = cfg_fresh['model']['params']['unet_']['params']['out_channels']\n", "sigma_data = cfg_fresh['model']['params'].get('diffusion_sigma_data', 0.5)\n", "model, diffusion = create_model_and_diffusion(distillation=False, **cm_model_params)\n", "diffusion.sigma_data = sigma_data\n", "\n", "# Speed-ups: loss=L2, convert model to fp16 torso\n", "try:\n", "    diffusion.loss_norm = 'l2'\n", "    print('[CT] diffusion.loss_norm = l2')\n", "except Exception as e:\n", "    print('[CT] set loss_norm failed:', e)\n", "try:\n", "    if hasattr(model,'convert_to_fp16'): model.convert_to_fp16()\n", "    print('[CT] model.convert_to_fp16() done')\n", "except Exception as e:\n", "    print('[CT] convert_to_fp16 failed:', e)\n", "\n", "# Patch I/O convs to match diffusion channels\n", "new_in = nn.Conv2d(in_ch, model.input_blocks[0][0].out_channels,\n", "                   kernel_size=model.input_blocks[0][0].kernel_size,\n", "                   stride=model.input_blocks[0][0].stride,\n", "                   padding=model.input_blocks[0][0].padding,\n", "                   bias=model.input_blocks[0][0].bias is not None)\n", "nn.init.kaiming_normal_(new_in.weight, mode='fan_out', nonlinearity='relu')\n", "if new_in.bias is not None: nn.init.zeros_(new_in.bias)\n", "model.input_blocks[0][0] = new_in\n", "\n", "new_out = nn.Conv2d(model.out[-1].in_channels, out_ch,\n", "                    kernel_size=model.out[-1].kernel_size,\n", "                    stride=model.out[-1].stride,\n", "                    padding=model.out[-1].padding,\n", "                    bias=model.out[-1].bias is not None)\n", "nn.init.zeros_(new_out.weight)\n", "if new_out.bias is not None: nn.init.zeros_(new_out.bias)\n", "model.out[-1] = new_out\n", "\n", "model = model.to(device).eval()\n", "print('CM UNet params:', cm_model_params)\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 4. Consistency Training (CAE → Karras)\n"]}, {"cell_type": "code", "execution_count": 40, "metadata": {}, "outputs": [], "source": ["import math\n", "import copy\n", "import random\n", "\n", "import numpy as np\n", "import torch\n", "import torch.nn.functional as F\n", "from torch.utils.data import DataLoader, TensorDataset\n", "\n", "import swanlab\n", "\n", "random.seed(42)\n", "np.random.seed(42)\n", "torch.manual_seed(42)\n", "\n", "def reshape_latents(latent_tensor: torch.Tensor):\n", "    B, S, C, L = latent_tensor.shape\n", "    flat = latent_tensor.view(B, S * C, L)\n", "    side = int(math.sqrt(L))\n", "    if side * side < L:\n", "        side += 1\n", "    pad = side * side - L\n", "    if pad > 0:\n", "        flat = F.pad(flat, (0, pad))\n", "    imgs = flat.view(B, S * C, side, side)\n", "    return imgs, {'side': side, 'pad': pad, 'latent_len': L}\n"]}, {"cell_type": "code", "execution_count": 41, "metadata": {}, "outputs": [], "source": ["# Helper: encode a dataloader batch of waveforms to 2D imgs for CM (no interface mismatch)\n", "import math\n", "import torch\n", "import numpy as np\n", "import torch.nn.functional as F\n", "\n", "def encode_batch_to_imgs(ae, batch, to_float32=True):\n", "    # batch['waveform_stems']: (B, S, T)\n", "    wav_stems = batch['waveform_stems']\n", "    if isinstance(wav_stems, np.ndarray):\n", "        wav_stems = torch.from_numpy(wav_stems)\n", "    if wav_stems.dim() != 3:\n", "        raise ValueError(f\"expected wav_stems of shape (B,S,T); got {tuple(wav_stems.shape)}\")\n", "\n", "    B, S, T = wav_stems.shape\n", "    latents_list = []\n", "    for s in range(S):\n", "        # (B, T) -> CAE encode 将 B 视作 audio_channels，输出 (B, C, L)\n", "        stem_audio = wav_stems[:, s].cpu().numpy()\n", "        stem_lat = ae.encode(stem_audio)  # (B, C, L)\n", "        if isinstance(stem_lat, np.ndarray):\n", "            stem_lat = torch.from_numpy(stem_lat)\n", "        if to_float32:\n", "            stem_lat = stem_lat.to(torch.float32)\n", "        latents_list.append(stem_lat)\n", "\n", "    # 堆叠成 (B, S, C, L)\n", "    latents = torch.stack(latents_list, dim=1).contiguous()\n", "\n", "    # reshape: (B, S*C, H, W)，H=W=ceil(sqrt(L))\n", "    B, S, C, L = latents.shape\n", "    flat = latents.view(B, S * C, L)\n", "    side = int(math.sqrt(L))\n", "    if side * side < L:\n", "        side += 1\n", "    pad = side * side - L\n", "    if pad > 0:\n", "        flat = F.pad(flat, (0, pad))\n", "    imgs = flat.view(B, S * C, side, side)\n", "    return imgs, {\"side\": side, \"pad\": pad, \"latent_len\": L}\n", "\n"]}, {"cell_type": "code", "execution_count": 42, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["Latent reshape -> torch.Size([6, 256, 12, 12]), meta={'side': 12, 'pad': 17, 'latent_len': 127}\n"]}], "source": ["from src.cm.script_util import create_model\n", "from src.cm.karras_diffusion import KarrasDenoiser\n", "from src.cm.nn import update_ema\n", "\n", "latents_cpu = latents.detach().to(torch.float32).cpu()\n", "imgs_sample, reshape_meta = reshape_latents(latents_cpu)\n", "print(f'Latent reshape -> {imgs_sample.shape}, meta={reshape_meta}')\n", "\n", "# steps_per_epoch will use dm.train_dataloader() later in training cell\n", "\n", "model = create_model(\n", "    image_size=imgs_sample.shape[-1],\n", "    num_channels=64,\n", "    num_res_blocks=1,\n", "    channel_mult='1',\n", "    learn_sigma=False,\n", "    class_cond=False,\n", "    use_checkpoint=False,\n", "    attention_resolutions='1024',\n", "    num_heads=1,\n", "    num_head_channels=-1,\n", "    num_heads_upsample=-1,\n", "    use_scale_shift_norm=True,\n", "    dropout=0.05,\n", "    resblock_updown=False,\n", "    use_fp16=False,\n", "    use_new_attention_order=False,\n", ")\n", "\n", "in_channels = imgs_sample.shape[1]\n", "model.input_blocks[0][0] = torch.nn.Conv2d(\n", "    in_channels,\n", "    model.input_blocks[0][0].out_channels,\n", "    kernel_size=model.input_blocks[0][0].kernel_size,\n", "    stride=model.input_blocks[0][0].stride,\n", "    padding=model.input_blocks[0][0].padding,\n", "    bias=model.input_blocks[0][0].bias is not None,\n", ")\n", "model.out[-1] = torch.nn.Conv2d(\n", "    model.out[-1].in_channels,\n", "    in_channels,\n", "    kernel_size=model.out[-1].kernel_size,\n", "    stride=model.out[-1].stride,\n", "    padding=model.out[-1].padding,\n", "    bias=model.out[-1].bias is not None,\n", ")\n", "\n", "diffusion = KarrasDenoiser(\n", "    sigma_data=ct_hparams['sigma_data'],\n", "    sigma_min=ct_hparams['sigma_min'],\n", "    sigma_max=ct_hparams['sigma_max'],\n", "    weight_schedule='karras',\n", "    loss_norm='l2',\n", ")\n", "\n", "device = torch.device(\"cuda:4\" if torch.cuda.is_available() else \"cpu\")\n", "model.to(device)\n", "\n", "target_model = copy.deepcopy(model)\n", "for p in target_model.parameters():\n", "    p.requires_grad_(False)\n", "target_model.to(device)\n", "target_model.eval()\n", "\n", "optimizer = torch.optim.Adam(model.parameters(), lr=ct_hparams['lr'])\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["/data1/yuchen/cd4mt/src/music2latent/music2latent\n"]}, {"name": "stderr", "output_type": "stream", "text": ["/tmp/ipykernel_420620/3398033056.py:7: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n", "  scaler = GradScaler()\n", "/tmp/ipykernel_420620/3398033056.py:27: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n", "  with autocast(dtype=torch.float16):\n"]}, {"name": "stdout", "output_type": "stream", "text": ["[train] epoch=0 step=0/4385 loss=0.1740\n", "[train] epoch=0 step=20/4385 loss=0.1079\n", "[train] epoch=0 step=40/4385 loss=0.0654\n", "[train] epoch=0 step=60/4385 loss=0.0380\n", "[train] epoch=0 step=80/4385 loss=0.0343\n", "[train] epoch=0 step=100/4385 loss=0.0624\n", "[train] epoch=0 step=120/4385 loss=0.0809\n", "[train] epoch=0 step=140/4385 loss=0.0042\n", "[train] epoch=0 step=160/4385 loss=0.0775\n", "[train] epoch=0 step=180/4385 loss=0.1109\n", "[train] epoch=0 step=200/4385 loss=0.0865\n", "[train] epoch=0 step=220/4385 loss=0.1531\n", "[train] epoch=0 step=240/4385 loss=0.0077\n", "[train] epoch=0 step=260/4385 loss=0.0257\n", "[train] epoch=0 step=280/4385 loss=0.0380\n", "[train] epoch=0 step=300/4385 loss=0.0953\n", "[train] epoch=0 step=320/4385 loss=0.0129\n", "[train] epoch=0 step=340/4385 loss=0.1168\n", "[train] epoch=0 step=360/4385 loss=0.0378\n", "[train] epoch=0 step=380/4385 loss=0.0220\n", "[train] epoch=0 step=400/4385 loss=0.0695\n", "[train] epoch=0 step=420/4385 loss=0.0159\n", "[train] epoch=0 step=440/4385 loss=0.0048\n", "[train] epoch=0 step=460/4385 loss=0.0335\n", "[train] epoch=0 step=480/4385 loss=0.0566\n", "[train] epoch=0 step=500/4385 loss=0.0563\n", "[train] epoch=0 step=520/4385 loss=0.0746\n", "[train] epoch=0 step=540/4385 loss=0.0755\n", "[train] epoch=0 step=560/4385 loss=0.0932\n", "[train] epoch=0 step=580/4385 loss=0.0868\n", "[train] epoch=0 step=600/4385 loss=0.0358\n", "[train] epoch=0 step=620/4385 loss=0.0021\n", "[train] epoch=0 step=640/4385 loss=0.0815\n", "[train] epoch=0 step=660/4385 loss=0.0876\n", "[train] epoch=0 step=680/4385 loss=0.1286\n", "[train] epoch=0 step=700/4385 loss=0.0631\n", "[train] epoch=0 step=720/4385 loss=0.1219\n", "[train] epoch=0 step=740/4385 loss=0.0619\n", "[train] epoch=0 step=760/4385 loss=0.0864\n", "[train] epoch=0 step=780/4385 loss=0.0168\n", "[train] epoch=0 step=800/4385 loss=0.0340\n", "[train] epoch=0 step=820/4385 loss=0.0101\n", "[train] epoch=0 step=840/4385 loss=0.0519\n", "[train] epoch=0 step=860/4385 loss=0.0381\n", "[train] epoch=0 step=880/4385 loss=0.0655\n", "[train] epoch=0 step=900/4385 loss=0.0467\n", "[train] epoch=0 step=920/4385 loss=0.0230\n", "[train] epoch=0 step=940/4385 loss=0.0418\n", "[train] epoch=0 step=960/4385 loss=0.0196\n"]}], "source": ["# CM training loop (epoch-level avg; deterministic val; best EMA ckpt)\n", "import torch, os, copy, numpy as np\n", "from torch.cuda.amp import autocast, GradScaler\n", "\n", "ae = EncoderDecoder(device=device)\n", "optimizer = torch.optim.Adam(model.parameters(), lr=float(ct_hparams['lr']))\n", "scaler = GradScaler()\n", "\n", "from src.cm.nn import update_ema\n", "# persistent EMA target model\n", "target_model = copy.deepcopy(model).to(device)\n", "for p in target_model.parameters():\n", "    p.requires_grad_(False)\n", "target_model.eval()\n", "\n", "best, best_path = float('inf'), None\n", "os.makedirs('checkpoints', exist_ok=True)\n", "\n", "for epoch in range(ct_hparams['epochs']):\n", "    # Train\n", "    model.train();\n", "    train_loader = dm.train_dataloader()\n", "    run, steps = 0.0, 0\n", "    for step, batch in enumerate(train_loader):\n", "        imgs, meta = encode_batch_to_imgs(ae, batch)\n", "        imgs = imgs.to(device)\n", "        with autocast(dtype=torch.float16):\n", "            losses = diffusion.consistency_losses(\n", "                model,\n", "                imgs.half(),\n", "                num_scales=ct_hparams['num_scales'],\n", "                target_model=target_model,\n", "                teacher_model=None,\n", "                teacher_diffusion=None,\n", "            )\n", "            loss = losses['loss'].mean()\n", "        optimizer.zero_grad()\n", "        scaler.scale(loss).backward()\n", "        scaler.unscale_(optimizer)\n", "        torch.nn.utils.clip_grad_norm_(model.parameters(), ct_hparams['grad_clip'])\n", "        scaler.step(optimizer)\n", "        scaler.update()\n", "        update_ema(target_model.parameters(), model.parameters(), rate=ct_hparams['ema_decay'])\n", "        run += float(loss.item()); steps += 1\n", "        if step % ct_hparams['log_interval'] == 0:\n", "            print(f\"[train] epoch={epoch} step={step}/{len(train_loader)} loss={loss.item():.4f}\")\n", "    avg_train = run / max(1, steps)\n", "    print(f\"[train] epoch={epoch} avg={avg_train:.4f}\")\n", "\n", "    # Val (deterministic)\n", "    model.eval();\n", "    val_loader = dm.val_dataloader()\n", "    vrun, vsteps = 0.0, 0\n", "    # save/restore RNG\n", "    _cpu = torch.get_rng_state(); _cuda = torch.cuda.get_rng_state_all() if torch.cuda.is_available() else None\n", "    torch.manual_seed(12345); np.random.seed(12345)\n", "    if torch.cuda.is_available(): torch.cuda.manual_seed_all(12345)\n", "    with torch.no_grad():\n", "        for vstep, vbatch in enumerate(val_loader):\n", "            vimgs, vmeta = encode_batch_to_imgs(ae, vbatch)\n", "            vimgs = vimgs.to(device)\n", "            gen = torch.Generator(device=vimgs.device); gen.manual_seed(12345 + vstep)\n", "            vnoise = torch.randn_like(vimgs, generator=gen)\n", "            with autocast(dtype=torch.float16):\n", "                vloss = diffusion.consistency_losses(\n", "                    model,\n", "                    vimgs.half(),\n", "                    num_scales=ct_hparams['num_scales'],\n", "                    target_model=target_model,\n", "                    teacher_model=None,\n", "                    teacher_diffusion=None,\n", "                    noise=vnoise,\n", "                )['loss'].mean()\n", "            vrun += float(vloss.item()); vsteps += 1\n", "    # restore RNG\n", "    torch.set_rng_state(_cpu); \n", "    if _cuda is not None: torch.cuda.set_rng_state_all(_cuda)\n", "    avg_val = vrun / max(1, vsteps)\n", "    print(f\"[val]  epoch={epoch} avg={avg_val:.4f}\")\n", "\n", "    # Save best by val\n", "    if avg_val < best:\n", "        best = avg_val\n", "        best_path = f\"checkpoints/ct_unet_ema_best_val{best:.6f}.pth\"\n", "        torch.save({'state_dict': target_model.state_dict(), 'epoch': epoch, 'ct_hparams': ct_hparams, 'meta': meta}, best_path)\n", "        print(f\"[ckpt] saved best -> {best_path}\")\n", "\n", "print(f\"Best ckpt: {best_path}, val={best:.6f}\")\n"]}], "metadata": {"kernelspec": {"display_name": "Python (cdp10)", "language": "python", "name": "cdp10"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.10.18"}}, "nbformat": 4, "nbformat_minor": 4}