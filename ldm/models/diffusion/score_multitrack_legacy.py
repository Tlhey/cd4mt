"""
MyModelScoreDiffusion: Score-based Diffusion Model for Multi-track Music Generation
====================================================================================

é‡æ„ç‰ˆæœ¬ï¼šç»§æ‰¿AudioDiffusionModel_2dï¼Œé›†æˆCAEéŸ³é¢‘ç¼–è§£ç å™¨

è®¾è®¡ç†å¿µ:
1. ç»§æ‰¿AudioDiffusionModel_2dè·å¾—å®Œæ•´çš„æ‰©æ•£åŠŸèƒ½
2. é›†æˆmusic2latent CAEæ›¿ä»£ä¼ ç»Ÿçš„VAE
3. æ”¯æŒmulti-track (stems)éŸ³ä¹ç”Ÿæˆ
4. ä½¿ç”¨EDM/VP-SDEå™ªå£°è°ƒåº¦å’ŒKarrasé‡‡æ ·

Author: Based on MusicLDM architecture with CAE integration
"""

import os
import torch
import torch.nn as nn
import torch.nn.functional as F
import pytorch_lightning as pl
import numpy as np
from typing import Dict, Any, Optional, Union, List, Tuple
from pathlib import Path
from einops import rearrange, reduce

# Music2Latent CAE
from music2latent import EncoderDecoder

# Audio Diffusion Components
from ctm_pl.audio_diffusion_pytorch_.model import AudioDiffusionModel_2d
from ctm_pl.audio_diffusion_pytorch_.diffusion import (
    LogNormalDistribution,
    KarrasSchedule,
    ADPM2Sampler
)

# OpenAI UNet
from ldm.modules.modules.diffusionmodules.openaimodel import UNetModel

# Utilities
from ldm.modules.util import instantiate_from_config, count_params


class MyModelScoreDiffusion(pl.LightningModule):
    """
    Score-based Diffusion Model for Multi-track Music Generation
    ç»§æ‰¿AudioDiffusionModel_2dï¼Œé›†æˆCAEéŸ³é¢‘ç¼–è§£ç å™¨
    
    Pipeline: Waveform â†’ CAE Encode â†’ Score Diffusion â†’ CAE Decode â†’ Waveform
    """
    
    def __init__(
        self,
        # UNet configuration
        unet_config: Dict[str, Any],
        
        # CAE configuration  
        cae_latent_dim: int = 64,
        cae_z_channels: int = 64,
        sample_rate: int = 44100,
        
        # Diffusion configuration
        diffusion_sigma_distribution: Optional[Dict] = None,
        diffusion_sigma_data: float = 0.5,
        diffusion_dynamic_threshold: float = 0.0,
        lambda_perceptual: float = 0.0,
        
        # Multi-track configuration
        num_stems: int = 4,
        stem_names: List[str] = ["bass", "drums", "guitar", "piano"],
        support_mixture: bool = True,
        
        # Training configuration
        base_learning_rate: float = 1e-4,
        
        # Sampling configuration
        sampling_steps: int = 50,
        sigma_min: float = 0.0001,
        sigma_max: float = 3.0,
        rho: float = 7.0,
        
        # Monitoring
        monitor: str = "val/loss",
        
        **kwargs
    ):
        super().__init__()
        self.save_hyperparameters()
        
        # =========================
        # 1. Audio Auto-Encoder (CAE)
        # =========================
        self.autoencoder = EncoderDecoder()
        self.cae_latent_dim = cae_latent_dim
        self.cae_z_channels = cae_z_channels
        self.sample_rate = sample_rate
        
        # =========================
        # 2. Multi-track Configuration  
        # =========================
        self.num_stems = num_stems
        self.stem_names = stem_names
        self.support_mixture = support_mixture
        
        # =========================
        # 3. Diffusion Model Setup
        # =========================
        
        # 3.1 è®¡ç®—UNetè¾“å…¥é€šé“æ•°
        if support_mixture:
            # æ”¯æŒ mixture + stems: (1+S)*C_lat
            diffusion_in_channels = num_stems * cae_z_channels
        else:
            # ä»…æ”¯æŒ stems: S*C_lat
            diffusion_in_channels = num_stems * cae_z_channels
            
        # 3.2 è®¾ç½®æ‰©æ•£å‚æ•°
        if diffusion_sigma_distribution is None:
            diffusion_sigma_distribution = LogNormalDistribution(mean=-1.2, std=1.2)
        else:
            diffusion_sigma_distribution = instantiate_from_config(diffusion_sigma_distribution)
        
        # 3.3 æ›´æ–°UNeté…ç½®
        unet_config = unet_config.copy()
        unet_config["params"] = unet_config["params"].copy()
        unet_config["params"]["in_channels"] = diffusion_in_channels
        unet_config["params"]["out_channels"] = diffusion_in_channels
        
        print(f"ğŸ—ï¸ Creating UNet with {diffusion_in_channels} channels")
        print(f"   UNet config: {unet_config}")
        
        # 3.4 åˆ›å»ºAudioDiffusionModel_2d
        self.audio_diffusion = AudioDiffusionModel_2d(
            diffusion_sigma_distribution=diffusion_sigma_distribution,
            diffusion_sigma_data=diffusion_sigma_data,
            diffusion_dynamic_threshold=diffusion_dynamic_threshold,
            lambda_perceptual=lambda_perceptual,
            unet_type='openAI',  # ä½¿ç”¨OpenAI UNet
            **unet_config["params"]
        )
        
        # 3.5 é‡‡æ ·ç»„ä»¶
        self.sigma_schedule = KarrasSchedule(
            sigma_min=sigma_min, 
            sigma_max=sigma_max, 
            rho=rho
        )
        
        self.sampler = ADPM2Sampler(rho=1.0)
        self.sampling_steps = sampling_steps
        
        # =========================
        # 4. Training Configuration
        # =========================
        self.learning_rate = float(base_learning_rate)
        self.monitor = monitor
        
        # =========================
        # 5. Logging
        # =========================
        print(f"âœ… MyModelScoreDiffusion initialized:")
        print(f"   - CAE latent dim: {cae_latent_dim}")
        print(f"   - CAE z channels: {cae_z_channels}")
        print(f"   - Num stems: {num_stems}")
        print(f"   - Support mixture: {support_mixture}")
        print(f"   - Diffusion in channels: {diffusion_in_channels}")
        print(f"   - Learning rate: {self.learning_rate}")
        print(f"   - Sampling steps: {sampling_steps}")
        
        # è®¡ç®—å‚æ•°æ•°é‡
        count_params(self.audio_diffusion, verbose=True)
    
    def encode_audio(self, audio: torch.Tensor) -> torch.Tensor:
        """
        ä½¿ç”¨CAEç¼–ç éŸ³é¢‘åˆ°æ½œåœ¨ç©ºé—´
        
        Args:
            audio: (B, S, T) - å¤šsteméŸ³é¢‘æ³¢å½¢
            
        Returns:
            latents: (B, S, C_lat, L) - CAEæ½œåœ¨è¡¨ç¤º
        """
        # ç¡®ä¿è¾“å…¥æ˜¯3D: (B, S, T)
        if audio.dim() == 2:
            # (B, T) -> (B, 1, T)
            audio = audio.unsqueeze(1)
        
        batch_size, num_stems, audio_length = audio.shape
        
        # æ£€æŸ¥stemsæ•°é‡
        if num_stems != self.num_stems:
            print(f"Warning: Expected {self.num_stems} stems, got {num_stems}. Using available stems.")
        
        # åˆ†åˆ«ç¼–ç æ¯ä¸ªstem
        latents_list = []
        for b in range(batch_size):
            batch_latents = []
            for s in range(num_stems):
                try:
                    # è·å–å•ä¸ªæ ·æœ¬çš„å•ä¸ªstem: [T]
                    stem_audio = audio[b, s].cpu().numpy()  # [T]
                    
                    # è½¬æ¢ä¸ºCAEæœŸæœ›çš„æ ¼å¼: [1, T] (channels, samples)
                    stem_audio = stem_audio.reshape(1, -1)  # [1, T]
                    
                    # CAEç¼–ç : [1, T] -> [1, 64, L]
                    # ä½¿ç”¨æ›´å®‰å…¨çš„å‚æ•°è®¾ç½®
                    stem_latents = self.autoencoder.encode(
                        stem_audio, 
                        max_waveform_length=32768,  # é™åˆ¶æœ€å¤§é•¿åº¦ä»¥é¿å…å†…å­˜é—®é¢˜
                        max_batch_size=1            # å•ä¸ªæ ·æœ¬å¤„ç†
                    )
                    if isinstance(stem_latents, np.ndarray):
                        stem_latents = torch.from_numpy(stem_latents).to(audio.device)
                    stem_latents = stem_latents.to(dtype=torch.float32)
                    
                    # ç¡®ä¿ç»´åº¦æ­£ç¡® [C, L] æ ¼å¼
                    if stem_latents.dim() == 3 and stem_latents.size(0) == 1:
                        stem_latents = stem_latents.squeeze(0)  # [1, 64, L] -> [64, L]
                    elif stem_latents.dim() == 2:
                        pass  # å·²ç»æ˜¯ [64, L] æ ¼å¼
                    else:
                        raise ValueError(f"Unexpected CAE output shape: {stem_latents.shape}")
                    
                    batch_latents.append(stem_latents)
                    
                except Exception as e:
                    print(f"Error encoding stem {s} of batch {b}: {e}")
                    raise e
            
            # å †å å½“å‰batchçš„æ‰€æœ‰stems: [S, 64, L]
            batch_latents = torch.stack(batch_latents, dim=0)
            latents_list.append(batch_latents)
        
        # å †å æ‰€æœ‰batch: [B, S, 64, L]
        latents = torch.stack(latents_list, dim=0)
        
        return latents
    
    def decode_audio(self, latents: torch.Tensor) -> torch.Tensor:
        """
        ä½¿ç”¨CAEè§£ç æ½œåœ¨è¡¨ç¤ºåˆ°éŸ³é¢‘
        
        Args:
            latents: (B, S, C_lat, L) - CAEæ½œåœ¨è¡¨ç¤º
            
        Returns:
            audio: (B, S, T) - éŸ³é¢‘æ³¢å½¢
        """
        batch_size, num_channels, latent_channels, latent_length = latents.shape
        
        # åˆ†åˆ«è§£ç æ¯ä¸ªé€šé“
        audio_list = []
        for i in range(num_channels):
            channel_latents = latents[:, i]  # (B, C_lat, L)
            channel_audio = self.autoencoder.decode(channel_latents)  # (B, T)
            audio_list.append(channel_audio.unsqueeze(1))  # (B, 1, T)
        
        # åˆå¹¶æ‰€æœ‰é€šé“
        audio = torch.cat(audio_list, dim=1)  # (B, S, T)
        
        return audio
    
    def prepare_diffusion_input(self, latents: torch.Tensor) -> torch.Tensor:
        """
        å°†CAEæ½œåœ¨è¡¨ç¤ºè½¬æ¢ä¸ºæ‰©æ•£æ¨¡å‹è¾“å…¥æ ¼å¼
        
        Args:
            latents: (B, S, C_lat, L) - CAEæ½œåœ¨è¡¨ç¤º
            
        Returns:
            diffusion_input: (B, S*C_lat, H, W) - æ‰©æ•£æ¨¡å‹è¾“å…¥
        """
        B, S, C, L = latents.shape
        
        # é‡å¡‘ä¸º2Dè¾“å…¥ï¼šéœ€è¦ç¡®ä¿Hå’ŒWç»´åº¦éƒ½å¤§äº1
        # å°†(B, S, C, L)è½¬æ¢ä¸ºæ–¹å½¢çš„2Dè¡¨ç¤º
        total_channels = S * C
        
        # å°†åºåˆ—é•¿åº¦è½¬æ¢ä¸ºæ¥è¿‘æ­£æ–¹å½¢çš„HÃ—W
        import math
        side_length = int(math.sqrt(L))
        if side_length * side_length < L:
            side_length += 1
        
        # Padåˆ°æ­£æ–¹å½¢
        padded_length = side_length * side_length
        padding = padded_length - L
        
        # (B, S, C, L) -> (B, S*C, L) -> pad -> (B, S*C, H, W)
        flat_latents = latents.view(B, total_channels, L)
        if padding > 0:
            flat_latents = torch.nn.functional.pad(flat_latents, (0, padding))
        
        diffusion_input = flat_latents.view(B, total_channels, side_length, side_length)
        
        # ç¡®ä¿æ•°æ®ç±»å‹ä¸€è‡´
        print(f"Diffusion input shape: {diffusion_input.shape}, dtype: {diffusion_input.dtype}")
        
        return diffusion_input
    
    def restore_diffusion_output(self, diffusion_output: torch.Tensor, target_shape: torch.Size) -> torch.Tensor:
        """
        å°†æ‰©æ•£æ¨¡å‹è¾“å‡ºè½¬æ¢å›CAEæ½œåœ¨è¡¨ç¤ºæ ¼å¼
        
        Args:
            diffusion_output: (B, S*C_lat, H, W) - æ‰©æ•£æ¨¡å‹è¾“å‡º
            target_shape: (B, S, C_lat, L) - ç›®æ ‡å½¢çŠ¶
            
        Returns:
            latents: (B, S, C_lat, L) - CAEæ½œåœ¨è¡¨ç¤º
        """
        B, S, C, L = target_shape
        
        # æ£€æŸ¥ç»´åº¦åŒ¹é…
        if diffusion_output.dim() == 4:
            diffusion_output = diffusion_output.squeeze(2)  # ç§»é™¤Hç»´åº¦
        
        # é‡å¡‘å›åŸå§‹å½¢çŠ¶
        latents = diffusion_output.view(B, S, C, L)
        
        return latents
    
    def forward(self, x: torch.Tensor, **kwargs) -> torch.Tensor:
        """
        å‰å‘ä¼ æ’­ - è®¡ç®—æ‰©æ•£æŸå¤±
        
        Args:
            x: (B, S, C_lat, L) - CAEæ½œåœ¨è¡¨ç¤º
            
        Returns:
            loss: æ‰©æ•£æŸå¤±
        """
        # å‡†å¤‡æ‰©æ•£è¾“å…¥
        diffusion_input = self.prepare_diffusion_input(x)
        
        # è®¡ç®—æ‰©æ•£æŸå¤±
        loss = self.audio_diffusion(diffusion_input, **kwargs)
        
        return loss
    
    def sample(
        self, 
        shape: Tuple[int, ...], 
        num_steps: Optional[int] = None,
        **kwargs
    ) -> torch.Tensor:
        """
        é‡‡æ ·ç”Ÿæˆæ–°çš„éŸ³é¢‘
        
        Args:
            shape: (B, S, C_lat, L) - ç”Ÿæˆå½¢çŠ¶
            num_steps: é‡‡æ ·æ­¥æ•°
            
        Returns:
            generated_audio: (B, S, T) - ç”Ÿæˆçš„éŸ³é¢‘
        """
        if num_steps is None:
            num_steps = self.sampling_steps
        
        B, S, C, L = shape
        
        # åˆ›å»ºå™ªå£°è¾“å…¥
        noise_shape = (B, S * C, 1, L)
        noise = torch.randn(noise_shape, device=self.device)
        
        # ä½¿ç”¨æ‰©æ•£æ¨¡å‹é‡‡æ ·
        generated_latents = self.audio_diffusion.sample(
            noise=noise,
            num_steps=num_steps,
            sigma_schedule=self.sigma_schedule,
            sampler=self.sampler,
            **kwargs
        )
        
        # æ¢å¤æ½œåœ¨è¡¨ç¤ºå½¢çŠ¶
        generated_latents = self.restore_diffusion_output(generated_latents, shape)
        
        # è§£ç ä¸ºéŸ³é¢‘
        generated_audio = self.decode_audio(generated_latents)
        
        return generated_audio
    
    def training_step(self, batch, batch_idx):
        """è®­ç»ƒæ­¥éª¤"""
        try:
            # è·å–éŸ³é¢‘æ•°æ® - ä¼˜å…ˆè·å–stemsæ•°æ®
            if isinstance(batch, dict):
                audio = batch.get('waveform_stems', batch.get('waveform', batch.get('audio', None)))
            else:
                audio = batch
            
            if audio is None:
                raise ValueError("æ— æ³•ä»batchä¸­è·å–éŸ³é¢‘æ•°æ®")
            
            # ç¼–ç éŸ³é¢‘
            latents = self.encode_audio(audio)
            
            # è®¡ç®—æ‰©æ•£æŸå¤±
            loss = self.forward(latents)
            
            # è®°å½•æŸå¤±
            self.log("train/loss", loss, prog_bar=True, on_step=True, on_epoch=True)
            
            return loss
            
        except Exception as e:
            print(f"âŒ Error in training_step: {e}")
            raise e
    
    def validation_step(self, batch, batch_idx):
        """éªŒè¯æ­¥éª¤"""
        try:
            # è·å–éŸ³é¢‘æ•°æ® - ä¼˜å…ˆè·å–stemsæ•°æ®
            if isinstance(batch, dict):
                audio = batch.get('waveform_stems', batch.get('waveform', batch.get('audio', None)))
            else:
                audio = batch
            
            if audio is None:
                raise ValueError("æ— æ³•ä»batchä¸­è·å–éŸ³é¢‘æ•°æ®")
            
            # ç¼–ç éŸ³é¢‘
            latents = self.encode_audio(audio)
            
            # è®¡ç®—æ‰©æ•£æŸå¤± (forwardå‡½æ•°å†…éƒ¨ä¼šè°ƒç”¨prepare_diffusion_input)
            val_loss = self.forward(latents)
            
            # è®°å½•æŸå¤±
            self.log("val/loss", val_loss, prog_bar=True, on_step=False, on_epoch=True)
            
            return val_loss
            
        except Exception as e:
            print(f"âŒ Error in validation_step: {e}")
            raise e
    
    def configure_optimizers(self):
        """é…ç½®ä¼˜åŒ–å™¨"""
        optimizer = torch.optim.AdamW(
            self.parameters(), 
            lr=self.learning_rate,
            betas=(0.9, 0.999),
            weight_decay=0.01
        )

        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
            optimizer, 
            T_max=1000,
            eta_min=self.learning_rate * 0.1
        )
        
        return {
            "optimizer": optimizer,
            "lr_scheduler": {
                "scheduler": scheduler,
                "monitor": self.monitor,
                "interval": "epoch",
                "frequency": 1,
            }
        }


# å…¼å®¹æ€§å‡½æ•°
def create_model_from_config(config: Dict[str, Any]) -> MyModelScoreDiffusion:
    return MyModelScoreDiffusion(**config)

