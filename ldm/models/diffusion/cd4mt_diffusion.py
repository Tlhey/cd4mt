"""cd4mt score diffusion model - multitrack audio generation with cae"""

import os
import torch
import torch.nn as nn
import torch.nn.functional as F
import pytorch_lightning as pl
import numpy as np
from typing import Dict, Any, Optional, Union, List, Tuple
from pathlib import Path
from einops import rearrange, reduce

# cae codec
from src.music2latent.music2latent import EncoderDecoder

# diffusion core
from ldm.models.diffusion.ctm.audio_diffusion_pytorch_.model import AudioDiffusionModel_2d
from ldm.models.diffusion.ctm.audio_diffusion_pytorch_.diffusion import (
    LogNormalDistribution,
    KarrasSchedule,
    ADPM2Sampler
)

# unet
from ldm.modules.modules.diffusionmodules.openaimodel import UNetModel

# utils
from ldm.modules.util import instantiate_from_config, count_params, summarize_params


class ScoreDiffusionModel(pl.LightningModule):
    """score diffusion with cae codec for multitrack gen"""
    
    def __init__(
        self,
        # unet config
        unet_config: Dict[str, Any],
        
        # cae config  
        cae_latent_dim: int = 64,
        cae_z_channels: int = 64,
        sample_rate: int = 44100,
        
        # diffusion params
        diffusion_sigma_distribution: Optional[Dict] = None,
        diffusion_sigma_data: float = 0.5,
        diffusion_dynamic_threshold: float = 0.0,
        lambda_perceptual: float = 0.0,
        
        # multitrack setup
        num_stems: int = 4,
        stem_names: List[str] = ["bass", "drums", "guitar", "piano"],
        support_mixture: bool = True,
        
        # training
        base_learning_rate: float = 1e-4,
        
        # sampling
        sampling_steps: int = 50,
        sigma_min: float = 0.0001,
        sigma_max: float = 3.0,
        rho: float = 7.0,
        
        # monitoring
        monitor: str = "val/loss",
        
        **kwargs
    ):
        super().__init__()
        self.save_hyperparameters()
        
        # =========================
        # 1. Audio Auto-Encoder (CAE)
        # =========================
        # Lazily construct CAE on the correct device when first used
        self.autoencoder = None
        self.cae_latent_dim = cae_latent_dim
        self.cae_z_channels = cae_z_channels
        self.sample_rate = sample_rate
        
        # =========================
        # 2. Multi-track Configuration  
        # =========================
        self.num_stems = num_stems
        self.stem_names = stem_names
        self.support_mixture = support_mixture
        
        # =========================
        # 3. Diffusion Model Setup
        # =========================
        
        # 3.1 è®¡ç®—UNetè¾“å…¥é€šé“æ•°
        if support_mixture:
            # æ”¯æŒ mixture + stems: (1+S)*C_lat
            diffusion_in_channels = num_stems * cae_z_channels
        else:
            # ä»…æ”¯æŒ stems: S*C_lat
            diffusion_in_channels = num_stems * cae_z_channels
            
        # 3.2 è®¾ç½®æ‰©æ•£å‚æ•°
        if diffusion_sigma_distribution is None:
            diffusion_sigma_distribution = LogNormalDistribution(mean=-1.2, std=1.2)
        else:
            diffusion_sigma_distribution = instantiate_from_config(diffusion_sigma_distribution)
        
        # 3.3 æ›´æ–°UNeté…ç½®
        unet_config = unet_config.copy()
        unet_config["params"] = unet_config["params"].copy()
        unet_config["params"]["in_channels"] = diffusion_in_channels
        unet_config["params"]["out_channels"] = diffusion_in_channels
        
        print(f"[Init] Creating UNet with in/out channels = {diffusion_in_channels}")
        print(f"        UNet config(model_channels={unet_config['params'].get('model_channels')}, "
              f"num_res_blocks={unet_config['params'].get('num_res_blocks')}, "
              f"channel_mult={unet_config['params'].get('channel_mult')}, "
              f"num_head_channels={unet_config['params'].get('num_head_channels')})")
        
        # 3.4 åˆ›å»ºAudioDiffusionModel_2d
        self.audio_diffusion = AudioDiffusionModel_2d(
            diffusion_sigma_distribution=diffusion_sigma_distribution,
            diffusion_sigma_data=diffusion_sigma_data,
            diffusion_dynamic_threshold=diffusion_dynamic_threshold,
            lambda_perceptual=lambda_perceptual,
            unet_type='openAI',  # ä½¿ç”¨OpenAI UNet
            **unet_config["params"]
        )
        
        # 3.5 é‡‡æ ·ç»„ä»¶
        self.sigma_schedule = KarrasSchedule(
            sigma_min=sigma_min, 
            sigma_max=sigma_max, 
            rho=rho
        )
        
        self.sampler = ADPM2Sampler(rho=1.0)
        self.sampling_steps = sampling_steps
        
        # =========================
        # 4. Training Configuration
        # =========================
        self.learning_rate = float(base_learning_rate)
        self.monitor = monitor
        
        # =========================
        # 5. Logging
        # =========================
        print(f"âœ… ScoreDiffusionModel initialized:")
        print(f"   - CAE latent dim: {cae_latent_dim}")
        print(f"   - CAE z channels: {cae_z_channels}")
        print(f"   - Num stems: {num_stems}")
        print(f"   - Support mixture: {support_mixture}")
        print(f"   - Diffusion in channels: {diffusion_in_channels}")
        print(f"   - Learning rate: {self.learning_rate}")
        print(f"   - Sampling steps: {sampling_steps}")

        # å‚æ•°è§„æ¨¡ä¸å†…å­˜å ç”¨ï¼ˆæ›´è¯¦ç»†ï¼‰
        try:
            summarize_params(self.audio_diffusion, name="AudioDiffusionModel_2d")
            if hasattr(self.audio_diffusion, 'unet'):
                summarize_params(self.audio_diffusion.unet, name="UNet (inner)")
        except Exception as _e:
            # å›é€€åˆ°ç®€å•ç»Ÿè®¡ï¼Œé¿å…å›  dtype ç­‰å¯¼è‡´åˆå§‹åŒ–æŠ¥é”™
            count_params(self.audio_diffusion, verbose=True)

        # å½¢çŠ¶/è¶…å‚å®‰å…¨æ£€æŸ¥ï¼ˆæå‰å‘ç°å¸¸è§æ–­è¨€ï¼‰
        try:
            mc = int(unet_config['params'].get('model_channels'))
            cmult = list(unet_config['params'].get('channel_mult'))
            head_dim = int(unet_config['params'].get('num_head_channels', -1))
            if head_dim != -1:
                bad = [mc * int(m) for m in cmult if (mc * int(m)) % head_dim != 0]
                if bad:
                    print(f"[Warn] num_head_channels={head_dim} does not divide channels at levels: {bad}")
        except Exception:
            pass
    
    def encode_audio(self, audio: torch.Tensor) -> torch.Tensor:
        """
        ä½¿ç”¨CAEç¼–ç éŸ³é¢‘åˆ°æ½œåœ¨ç©ºé—´
        
        Args:
            audio: (B, S, T) - å¤šsteméŸ³é¢‘æ³¢å½¢
            
        Returns:
            latents: (B, S, C_lat, L) - CAEæ½œåœ¨è¡¨ç¤º
        """
        # ç¡®ä¿å·²åˆå§‹åŒ– CAE åˆ°æ­£ç¡®è®¾å¤‡
        if self.autoencoder is None:
            dev = getattr(self, 'device', None) or audio.device
            self.autoencoder = EncoderDecoder(device=dev)

        # ç¡®ä¿è¾“å…¥æ˜¯3D: (B, S, T)
        if audio.dim() == 2:
            # (B, T) -> (B, 1, T)
            audio = audio.unsqueeze(1)
        
        batch_size, num_stems, audio_length = audio.shape
        
        # æ£€æŸ¥stemsæ•°é‡
        if num_stems != self.num_stems:
            print(f"Warning: Expected {self.num_stems} stems, got {num_stems}. Using available stems.")
        
        # åˆ†åˆ«ç¼–ç æ¯ä¸ªstem
        latents_list = []
        for b in range(batch_size):
            batch_latents = []
            for s in range(num_stems):
                try:
                    # è·å–å•ä¸ªæ ·æœ¬çš„å•ä¸ªstem: [T]
                    stem_audio = audio[b, s].cpu().numpy()  # [T]
                    
                    # è½¬æ¢ä¸ºCAEæœŸæœ›çš„æ ¼å¼: [1, T] (channels, samples)
                    stem_audio = stem_audio.reshape(1, -1)  # [1, T]
                    
                    # CAEç¼–ç : [1, T] -> [1, 64, L]
                    # ä½¿ç”¨æ›´å®‰å…¨çš„å‚æ•°è®¾ç½®
                    stem_latents = self.autoencoder.encode(
                        stem_audio, 
                        max_waveform_length=32768,  # é™åˆ¶æœ€å¤§é•¿åº¦ä»¥é¿å…å†…å­˜é—®é¢˜
                        max_batch_size=1            # å•ä¸ªæ ·æœ¬å¤„ç†
                    )
                    
                    # è½¬æ¢ä¸ºtorch tensorå¹¶ç¡®ä¿æ•°æ®ç±»å‹ä¸€è‡´
                    if isinstance(stem_latents, np.ndarray):
                        stem_latents = torch.from_numpy(stem_latents).to(audio.device)
                    
                    # å¼ºåˆ¶è½¬æ¢ä¸ºfloat32ä»¥é¿å…mixed precisioné—®é¢˜
                    stem_latents = stem_latents.to(dtype=torch.float32)
                    
                    # ç¡®ä¿ç»´åº¦æ­£ç¡® [C, L] æ ¼å¼
                    if stem_latents.dim() == 3 and stem_latents.size(0) == 1:
                        stem_latents = stem_latents.squeeze(0)  # [1, 64, L] -> [64, L]
                    elif stem_latents.dim() == 2:
                        pass  # å·²ç»æ˜¯ [64, L] æ ¼å¼
                    else:
                        raise ValueError(f"Unexpected CAE output shape: {stem_latents.shape}")
                    
                    batch_latents.append(stem_latents)
                    
                except Exception as e:
                    print(f"Error encoding stem {s} of batch {b}: {e}")
                    raise e
            
            # å †å å½“å‰batchçš„æ‰€æœ‰stems: [S, 64, L]
            batch_latents = torch.stack(batch_latents, dim=0)
            latents_list.append(batch_latents)
        
        # å †å æ‰€æœ‰batch: [B, S, 64, L]
        latents = torch.stack(latents_list, dim=0)
        
        return latents
    
    def decode_audio(self, latents: torch.Tensor) -> torch.Tensor:
        """
        ä½¿ç”¨CAEè§£ç æ½œåœ¨è¡¨ç¤ºåˆ°éŸ³é¢‘
        
        Args:
            latents: (B, S, C_lat, L) - CAEæ½œåœ¨è¡¨ç¤º
            
        Returns:
            audio: (B, S, T) - éŸ³é¢‘æ³¢å½¢
        """
        # ç¡®ä¿å·²åˆå§‹åŒ– CAE åˆ°æ­£ç¡®è®¾å¤‡
        if self.autoencoder is None:
            dev = getattr(self, 'device', None) or latents.device
            self.autoencoder = EncoderDecoder(device=dev)

        batch_size, num_channels, latent_channels, latent_length = latents.shape
        
        # åˆ†åˆ«è§£ç æ¯ä¸ªé€šé“
        audio_list = []
        for i in range(num_channels):
            channel_latents = latents[:, i]  # (B, C_lat, L)
            channel_audio = self.autoencoder.decode(channel_latents)  # (B, T)
            audio_list.append(channel_audio.unsqueeze(1))  # (B, 1, T)
        
        # åˆå¹¶æ‰€æœ‰é€šé“
        audio = torch.cat(audio_list, dim=1)  # (B, S, T)
        
        return audio
    
    def prepare_diffusion_input(self, latents: torch.Tensor) -> torch.Tensor:
        """
        å°†CAEæ½œåœ¨è¡¨ç¤ºè½¬æ¢ä¸ºæ‰©æ•£æ¨¡å‹è¾“å…¥æ ¼å¼
        
        Args:
            latents: (B, S, C_lat, L) - CAEæ½œåœ¨è¡¨ç¤º
            
        Returns:
            diffusion_input: (B, S*C_lat, H, W) - æ‰©æ•£æ¨¡å‹è¾“å…¥
        """
        B, S, C, L = latents.shape
        
        # é‡å¡‘ä¸º2Dè¾“å…¥ï¼šéœ€è¦ç¡®ä¿Hå’ŒWç»´åº¦éƒ½å¤§äº1
        # å°†(B, S, C, L)è½¬æ¢ä¸ºæ–¹å½¢çš„2Dè¡¨ç¤º
        total_channels = S * C
        
        # å°†åºåˆ—é•¿åº¦è½¬æ¢ä¸ºæ¥è¿‘æ­£æ–¹å½¢çš„HÃ—W
        import math
        side_length = int(math.sqrt(L))
        if side_length * side_length < L:
            side_length += 1
        
        # Padåˆ°æ­£æ–¹å½¢
        padded_length = side_length * side_length
        padding = padded_length - L
        
        # (B, S, C, L) -> (B, S*C, L) -> pad -> (B, S*C, H, W)
        flat_latents = latents.view(B, total_channels, L)
        if padding > 0:
            flat_latents = torch.nn.functional.pad(flat_latents, (0, padding))
        
        diffusion_input = flat_latents.view(B, total_channels, side_length, side_length)
        
        # ç¡®ä¿æ•°æ®ç±»å‹ä¸€è‡´
        print(f"Diffusion input shape: {diffusion_input.shape}, dtype: {diffusion_input.dtype}")
        
        return diffusion_input
    
    def restore_diffusion_output(self, diffusion_output: torch.Tensor, target_shape: torch.Size) -> torch.Tensor:
        """
        å°†æ‰©æ•£æ¨¡å‹è¾“å‡ºè½¬æ¢å›CAEæ½œåœ¨è¡¨ç¤ºæ ¼å¼
        
        Args:
            diffusion_output: (B, S*C_lat, H, W) - æ‰©æ•£æ¨¡å‹è¾“å‡º
            target_shape: (B, S, C_lat, L) - ç›®æ ‡å½¢çŠ¶
            
        Returns:
            latents: (B, S, C_lat, L) - CAEæ½œåœ¨è¡¨ç¤º
        """
        B, S, C, L = target_shape
        
        print(f"ğŸ”„ æ¢å¤æ‰©æ•£è¾“å‡º: {diffusion_output.shape} -> {target_shape}")
        
        # æ­£ç¡®å¤„ç†4Dåˆ°ç›®æ ‡å½¢çŠ¶çš„è½¬æ¢
        if diffusion_output.dim() == 4:
            batch_size, total_channels, H, W = diffusion_output.shape
            
            # éªŒè¯é€šé“æ•°åŒ¹é…
            expected_channels = S * C
            if total_channels != expected_channels:
                raise ValueError(f"é€šé“æ•°ä¸åŒ¹é…: æœŸæœ› {expected_channels}, å¾—åˆ° {total_channels}")
            
            # å±•å¹³Hå’ŒWç»´åº¦: (B, S*C, H, W) -> (B, S*C, H*W)
            flat_output = diffusion_output.view(batch_size, total_channels, H * W)
            
            # æˆªå–å‰Lä¸ªå…ƒç´ : (B, S*C, H*W) -> (B, S*C, L)
            if H * W >= L:
                flat_output = flat_output[:, :, :L]
            else:
                # å¦‚æœç”Ÿæˆçš„é•¿åº¦ä¸å¤Ÿï¼Œç”¨é›¶å¡«å……
                padding = L - (H * W)
                flat_output = torch.nn.functional.pad(flat_output, (0, padding))
            
        elif diffusion_output.dim() == 3:
            # å·²ç»æ˜¯(B, S*C, L)æ ¼å¼
            flat_output = diffusion_output
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ‰©æ•£è¾“å‡ºç»´åº¦: {diffusion_output.dim()}")
        
        # é‡å¡‘ä¸ºç›®æ ‡å½¢çŠ¶: (B, S*C, L) -> (B, S, C, L)
        latents = flat_output.view(B, S, C, L)
        
        print(f"âœ… æ¢å¤å®Œæˆ: {latents.shape}")
        return latents
    
    def forward(self, x: torch.Tensor, **kwargs) -> torch.Tensor:
        """
        å‰å‘ä¼ æ’­ - è®¡ç®—æ‰©æ•£æŸå¤±
        
        Args:
            x: (B, S, C_lat, L) - CAEæ½œåœ¨è¡¨ç¤º
            
        Returns:
            loss: æ‰©æ•£æŸå¤±
        """
        # å‡†å¤‡æ‰©æ•£è¾“å…¥
        diffusion_input = self.prepare_diffusion_input(x)
        
        # è®¡ç®—æ‰©æ•£æŸå¤±
        loss = self.audio_diffusion(diffusion_input, **kwargs)
        
        return loss
    
    def sample(
        self, 
        shape: Tuple[int, ...], 
        num_steps: Optional[int] = None,
        **kwargs
    ) -> torch.Tensor:
        """
        é‡‡æ ·ç”Ÿæˆæ–°çš„éŸ³é¢‘
        
        Args:
            shape: (B, S, C_lat, L) - ç”Ÿæˆå½¢çŠ¶
            num_steps: é‡‡æ ·æ­¥æ•°
            
        Returns:
            generated_audio: (B, S, T) - ç”Ÿæˆçš„éŸ³é¢‘
        """
        if num_steps is None:
            num_steps = self.sampling_steps
        
        B, S, C, L = shape
        
        # åˆ›å»ºå™ªå£°è¾“å…¥ - å¿…é¡»ä¸è®­ç»ƒæ—¶prepare_diffusion_inputçš„è¾“å‡ºå½¢çŠ¶ä¸€è‡´
        total_channels = S * C  # 256 for 4 stems * 64 channels
        
        # è®¡ç®—ä¸prepare_diffusion_inputç›¸åŒçš„2Dç»´åº¦
        import math
        side_length = int(math.sqrt(L))
        if side_length * side_length < L:
            side_length += 1
        
        # å™ªå£°å½¢çŠ¶å¿…é¡»åŒ¹é…è®­ç»ƒæ—¶çš„(B, S*C, H, W)æ ¼å¼
        noise_shape = (B, total_channels, side_length, side_length)
        noise = torch.randn(noise_shape, device=self.device)
        
        print(f"âœ… å™ªå£°å½¢çŠ¶: {noise_shape} (åŒ¹é…è®­ç»ƒæ—¶prepare_diffusion_inputè¾“å‡º)")
        
        # ä½¿ç”¨æ‰©æ•£æ¨¡å‹é‡‡æ ·
        generated_latents = self.audio_diffusion.sample(
            noise=noise,
            num_steps=num_steps,
            sigma_schedule=self.sigma_schedule,
            sampler=self.sampler,
            **kwargs
        )
        
        # æ¢å¤æ½œåœ¨è¡¨ç¤ºå½¢çŠ¶
        generated_latents = self.restore_diffusion_output(generated_latents, shape)
        
        # è§£ç ä¸ºéŸ³é¢‘
        generated_audio = self.decode_audio(generated_latents)
        
        return generated_audio
    
    def training_step(self, batch, batch_idx):
        """è®­ç»ƒæ­¥éª¤"""
        try:
            # è·å–éŸ³é¢‘æ•°æ® - ä¼˜å…ˆè·å–stemsæ•°æ®
            if isinstance(batch, dict):
                audio = batch.get('waveform_stems', batch.get('waveform', batch.get('audio', None)))
            else:
                audio = batch
            
            if audio is None:
                raise ValueError("æ— æ³•ä»batchä¸­è·å–éŸ³é¢‘æ•°æ®")
            
            # ç¼–ç éŸ³é¢‘
            latents = self.encode_audio(audio)
            
            # è®¡ç®—æ‰©æ•£æŸå¤±
            loss = self.forward(latents)
            
            # è®°å½•æŸå¤±
            self.log("train/loss", loss, prog_bar=True, on_step=True, on_epoch=True)
            
            return loss
            
        except Exception as e:
            print(f"âŒ Error in training_step: {e}")
            raise e
    
    def validation_step(self, batch, batch_idx):
        """éªŒè¯æ­¥éª¤"""
        try:
            # è·å–éŸ³é¢‘æ•°æ® - ä¼˜å…ˆè·å–stemsæ•°æ®
            if isinstance(batch, dict):
                audio = batch.get('waveform_stems', batch.get('waveform', batch.get('audio', None)))
            else:
                audio = batch
            
            if audio is None:
                raise ValueError("æ— æ³•ä»batchä¸­è·å–éŸ³é¢‘æ•°æ®")
            
            # ç¼–ç éŸ³é¢‘
            latents = self.encode_audio(audio)
            
            # è®¡ç®—æ‰©æ•£æŸå¤± (forwardå‡½æ•°å†…éƒ¨ä¼šè°ƒç”¨prepare_diffusion_input)
            val_loss = self.forward(latents)
            
            # è®°å½•æŸå¤±
            self.log("val/loss", val_loss, prog_bar=True, on_step=False, on_epoch=True)
            
            return val_loss
            
        except Exception as e:
            print(f"âŒ Error in validation_step: {e}")
            raise e
    
    def configure_optimizers(self):
        """é…ç½®ä¼˜åŒ–å™¨"""
        optimizer = torch.optim.AdamW(
            self.parameters(), 
            lr=self.learning_rate,
            betas=(0.9, 0.999),
            weight_decay=0.01
        )
        
        # å­¦ä¹ ç‡è°ƒåº¦å™¨
        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
            optimizer, 
            T_max=1000,
            eta_min=self.learning_rate * 0.1
        )
        
        return {
            "optimizer": optimizer,
            "lr_scheduler": {
                "scheduler": scheduler,
                "monitor": self.monitor,
                "interval": "epoch",
                "frequency": 1,
            }
        }


# å…¼å®¹æ€§å‡½æ•°
def create_model_from_config(config: Dict[str, Any]) -> ScoreDiffusionModel:
    """Create model from configuration"""
    return ScoreDiffusionModel(**config)


# Legacy aliases for backwards compatibility
MyModelScoreDiffusion = ScoreDiffusionModel
MyModelScoreDiffusionSampler = ScoreDiffusionModel
