{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ok] project paths configured\n",
      "Active env: cdp10\n"
     ]
    }
   ],
   "source": [
    "# Ensure project libs are on sys.path before imports\n",
    "import sys, os\n",
    "ROOT = '/data1/yuchen/cd4mt'\n",
    "sys.path.append(f'{ROOT}/src')\n",
    "sys.path.append(ROOT)\n",
    "sys.path.append(f'{ROOT}/ldm')\n",
    "print('[ok] project paths configured')\n",
    "\n",
    "\n",
    "# Require micromamba env 'cdp10'\n",
    "_env = os.environ.get('CONDA_DEFAULT_ENV') or os.environ.get('MAMBA_DEFAULT_ENV') or ''\n",
    "print(f\"Active env: {_env}\")\n",
    "assert 'cdp10' in _env, \"Please 'micromamba activate cdp10' before running this notebook.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CD4MT \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook presents a clean, student-style walkthrough for CD4MT.\n",
    "\n",
    "- Goal: show how to train a small CT model and run inference.\n",
    "- Dataset: `dataset/slakh_44100` (44100 Hz).\n",
    "- Checkpoints: under `checkpoints/`.\n",
    "\n",
    "Structure:\n",
    "1. Overview\n",
    "2. Environment\n",
    "3. Config & Data\n",
    "4. Model\n",
    "5. Inference (pretrained)\n",
    "6. Training (short run)\n",
    "7. Visualization & Metrics\n",
    "8. Checkpoints & Notes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Flow: load config/data, encode stems to CAE latents, sanity-check decode, build CT UNet, inspect shapes, visualize and play audio.\n",
    "\n",
    "Key shapes used below:\n",
    "- Waveforms: `(B, S, T)` e.g. `(4, 4, 524288)`; mix `(B, T)`.\n",
    "- CAE latent per stem: `(B, C_lat, L)` e.g. `(4, 64, 127)`.\n",
    "- Stacked latents: `(B, S, C_lat, L)` e.g. `(4, 4, 64, 127)`.\n",
    "- 2D map for UNet: `(B, S*C_lat, H, W)` with `H=W=ceil(sqrt(L))` (for `L=127`, `H=W=12`).\n",
    "\n",
    "Section guide:\n",
    "- 1. env - device and imports.\n",
    "- 2. config and load_data - prints config and fetches a batch; logs `(B,S,T)`.\n",
    "- 3. CAE encoder test - encodes to `(B,64,L)`, stacks to `(B,4,64,L)`; then decode + MSE.\n",
    "- 4. CD4MT model init - CT UNet with in/out channels `S*C_lat=256`, EMA ckpt.\n",
    "- 5. Shape instrumentation - param shapes and optional short-run logger.\n",
    "- 6. Generation test - use Karras sampler (test.py) or Lightning `ScoreDiffusionModel.sample(...)`.\n",
    "- 7-9. Visualization and audio players."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda:0\n",
      "2.7.1+cu126\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device: {device}\")\n",
    "import torch\n",
    "print(torch.__version__)\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import yaml\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import soundfile as sf\n",
    "from pathlib import Path\n",
    "import logging\n",
    "from datetime import datetime\n",
    "import pytorch_lightning as pl\n",
    "import torch.cuda\n",
    "\n",
    "if hasattr(torch, 'set_float32_matmul_precision'):\n",
    "    torch.set_float32_matmul_precision('medium')\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "\n",
    "# optional: swanlab for experiment tracking\n",
    "import importlib.util as _ilu\n",
    "swanlab = None\n",
    "if _ilu.find_spec('swanlab') is not None:\n",
    "    import swanlab\n",
    "\n",
    "\n",
    "from IPython.display import Audio, display, HTML, Markdown\n",
    "import matplotlib.font_manager as fm\n",
    "from src.music2latent.music2latent import EncoderDecoder\n",
    "from ldm.data.multitrack_datamodule import DataModuleFromConfig\n",
    "from src.cm.script_util import (\n",
    "    model_and_diffusion_defaults,\n",
    "    create_model_and_diffusion,\n",
    "    cm_train_defaults,\n",
    "    args_to_dict,\n",
    "    add_dict_to_argparser,\n",
    "    create_ema_and_scales_fn,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install /data1/yuchen/cd4mt/src/env/torchvision-0.22.0+cu118-cp39-cp39-manylinux_2_28_x86_64.whl\n",
    "# !pip install /data1/yuchen/cd4mt/src/env/torchvision-0.22.0+cu118-cp39-cp39-manylinux_2_28_x86_64.whl\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Config & Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explainer - Config and First Batch (Shapes):\n",
    "\n",
    "- Reads YAML config and prints key settings (stems, batch size, sample rate).\n",
    "- Builds DataModule and pulls one batch to inspect shapes.\n",
    "- Expected keys: `['fname', 'fbank_stems', 'waveform_stems', 'waveform', 'fbank']`.\n",
    "- Typical shapes here:\n",
    "  - `waveform_stems`: `(B, S, T)`, e.g. `(4, 4, 524288)`.\n",
    "  - `waveform` (mix): `(B, T)`, e.g. `(4, 524288)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded config: configs/cd4mt_small.yaml\n",
      "Found 1290 tracks.\n",
      "sr=44100, min: 10, max: 600\n",
      "Keeping 1289 of 1290 tracks\n",
      "Data size: 26309\n",
      "Use mixup rate of 0.0; Use SpecAug (T,F) of (0, 0); Use blurring effect or not False\n",
      "| Audiostock Dataset Length:26309 | Epoch Length: 26309\n",
      "Found 271 tracks.\n",
      "sr=44100, min: 10, max: 600\n",
      "Keeping 270 of 271 tracks\n",
      "Data size: 5422\n",
      "Use mixup rate of 0.0; Use SpecAug (T,F) of (0, 0); Use blurring effect or not False\n",
      "| Audiostock Dataset Length:5422 | Epoch Length: 5422\n",
      "Found 152 tracks.\n",
      "sr=44100, min: 10, max: 600\n",
      "Keeping 151 of 152 tracks\n",
      "Data size: 3249\n",
      "Use mixup rate of 0.0; Use SpecAug (T,F) of (0, 0); Use blurring effect or not False\n",
      "| Audiostock Dataset Length:3249 | Epoch Length: 3249\n",
      "train_loader: 6578\n"
     ]
    }
   ],
   "source": [
    "# Build DataModule (self-contained; no try/except)\n",
    "CFG_PATH = \"configs/cd4mt_small.yaml\"\n",
    "import yaml\n",
    "with open(CFG_PATH, 'r') as f:\n",
    "    cfg = yaml.safe_load(f)\n",
    "print(f\"Loaded config: {CFG_PATH}\")\n",
    "from ldm.data.multitrack_datamodule import DataModuleFromConfig\n",
    "\n",
    "dm = DataModuleFromConfig(**cfg[\"data\"][\"params\"])\n",
    "dm.prepare_data()\n",
    "dm.setup(stage=\"fit\")\n",
    "\n",
    "train_loader = dm.train_dataloader()\n",
    "print(f\"train_loader: {len(train_loader)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. CAE Encoder Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explainer - CAE Encode/Stack/Decode (Shapes):\n",
    "\n",
    "- For each stem `s` in 4 stems, encode `(B, T)` to `(B, C_lat, L)` with CAE (e.g. `(4, 64, 127)`).\n",
    "- Stack across stems → `latents_stacked`: `(B, S, C_lat, L)` (e.g. `(4, 4, 64, 127)`).\n",
    "- Decode each stem back to waveform and align to original `T` via crop/pad.\n",
    "- Report reconstruction MSE for sanity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch shapes: wav_stems=(4, 4, 524288), waveform=(4, 524288)\n"
     ]
    }
   ],
   "source": [
    "# Fetch one batch and define waveform tensors\n",
    "batch = next(iter(train_loader))\n",
    "assert 'waveform_stems' in batch, 'Expected key waveform_stems in batch'\n",
    "\n",
    "wav_stems = batch['waveform_stems']  # Tensor (B, S, T)\n",
    "wave_mix = batch['waveform'] if 'waveform' in batch else wav_stems.sum(dim=1)\n",
    "B, S_batch, T = wav_stems.shape\n",
    "sample_rate = int(cfg['data']['params']['preprocessing']['audio']['sampling_rate'])\n",
    "print(f'Batch shapes: wav_stems={tuple(wav_stems.shape)}, waveform={tuple(wave_mix.shape)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data1/yuchen/cd4mt/src/music2latent/music2latent\n",
      "\n",
      " stem_num 4 \n",
      "\n",
      " bass stem 0:\n",
      "stem_audio : (4, 524288)\n",
      "stem_latents.shape torch.Size([4, 64, 127]), tem_latents.dtype torch.float32, range: [-4.180, 3.848]\n",
      "\n",
      " drums stem 1:\n",
      "stem_audio : (4, 524288)\n",
      "stem_latents.shape torch.Size([4, 64, 127]), tem_latents.dtype torch.float32, range: [-4.422, 3.328]\n",
      "\n",
      " guitar stem 2:\n",
      "stem_audio : (4, 524288)\n",
      "stem_latents.shape torch.Size([4, 64, 127]), tem_latents.dtype torch.float32, range: [-5.012, 3.721]\n",
      "\n",
      " piano stem 3:\n",
      "stem_audio : (4, 524288)\n",
      "stem_latents.shape torch.Size([4, 64, 127]), tem_latents.dtype torch.float32, range: [-4.805, 4.062]\n",
      "latents_stacked.shape: torch.Size([4, 4, 64, 127])\n",
      " Batch=4, Stems=4, Channels=64, Length=127\n",
      " latents on : cuda:0\n"
     ]
    }
   ],
   "source": [
    "S = 4\n",
    "stem_names = cfg['model']['params']['stem_names']\n",
    "latents_list = []\n",
    "encode_shapes = []\n",
    "ae = EncoderDecoder(device=device)\n",
    "print(f\"\\n stem_num {S} \")\n",
    "\n",
    "for s in range(S):\n",
    "    stem_name = stem_names[s] if s < len(stem_names) else f\"stem_{s}\"\n",
    "    print(f\"\\n {stem_name} stem {s}:\")\n",
    "    stem_audio = wav_stems[:, s].cpu().numpy()  \n",
    "    print(f\"stem_audio : {stem_audio.shape}\")\n",
    "\n",
    "    stem_latents = ae.encode(stem_audio)\n",
    "    if isinstance(stem_latents, np.ndarray):\n",
    "        stem_latents = torch.from_numpy(stem_latents)\n",
    "    \n",
    "    # unify dtype to float32 to avoid AMP mix issues\n",
    "    stem_latents = stem_latents.to(dtype=torch.float32)\n",
    "    print(f\"stem_latents.shape {stem_latents.shape}, tem_latents.dtype {stem_latents.dtype}, range: [{stem_latents.min():.3f}, {stem_latents.max():.3f}]\")\n",
    "    latents_list.append(stem_latents)\n",
    "    encode_shapes.append(stem_latents.shape)\n",
    "    \n",
    "# ensure same latent length across stems for stacking\n",
    "min_L = min(t.shape[-1] for t in latents_list)\n",
    "if any(t.shape[-1] != min_L for t in latents_list):\n",
    "    print(f\"[Warn] Latent length mismatch across stems, cropping all to L={min_L}\")\n",
    "    latents_list = [t[..., :min_L] for t in latents_list]\n",
    "\n",
    "\n",
    "\n",
    "latents_stacked = torch.stack(latents_list, dim=1)  # (B, S, C, L)\n",
    "print(f\"latents_stacked.shape: {latents_stacked.shape}\")\n",
    "print(f\" Batch={latents_stacked.shape[0]}, Stems={latents_stacked.shape[1]}, Channels={latents_stacked.shape[2]}, Length={latents_stacked.shape[3]}\")\n",
    "latents = latents_stacked.to(device)\n",
    "print(f\" latents on : {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Decode bass\n",
      "recst.shape: torch.Size([4, 521728])\n",
      "range: [-0.192, 0.281]\n",
      "\n",
      "Decode drums\n",
      "recst.shape: torch.Size([4, 521728])\n",
      "range: [-0.612, 0.625]\n",
      "\n",
      "Decode guitar\n",
      "recst.shape: torch.Size([4, 521728])\n",
      "range: [-0.329, 0.316]\n",
      "\n",
      "Decode piano\n",
      "recst.shape: torch.Size([4, 521728])\n",
      "range: [-0.349, 0.352]\n",
      "\n",
      "recst shape: (4, 4, 524288)\n",
      "original lenght: 524288, recst length: 524288\n",
      "MSE: 0.001932\n"
     ]
    }
   ],
   "source": [
    "recst_list = []\n",
    "for s in range(S):\n",
    "    stem_name = stem_names[s] if s < len(stem_names) else f\"stem_{s}\"\n",
    "    print(f\"\\nDecode {stem_name}\")\n",
    "    stem_latents = latents[:, s].cpu().numpy()  # (B, C, L)\n",
    "    recst = ae.decode(stem_latents)\n",
    "    print(f\"recst.shape: {recst.shape}\")\n",
    "    print(f\"range: [{recst.min():.3f}, {recst.max():.3f}]\")\n",
    "\n",
    "    if isinstance(recst, torch.Tensor):\n",
    "        recst = recst.cpu().numpy()\n",
    "    current_length = recst.shape[-1]\n",
    "    if current_length > T:\n",
    "        excess = current_length - T\n",
    "        start_trim = excess // 2\n",
    "        end_trim = excess - start_trim\n",
    "        recst = recst[..., start_trim:current_length-end_trim]\n",
    "    elif current_length < T:\n",
    "        deficit = T - current_length\n",
    "        pad_left = deficit // 2\n",
    "        pad_right = deficit - pad_left\n",
    "        recst = np.pad(recst, ((0,0), (pad_left, pad_right)), mode='constant', constant_values=0)\n",
    "\n",
    "    recst_list.append(recst)\n",
    "\n",
    "recst_aud = np.stack(recst_list, axis=1)  # (B, S, T')\n",
    "recst_tensor = torch.from_numpy(recst_aud).to(device)\n",
    "\n",
    "print(f\"\\nrecst shape: {recst_aud.shape}\")\n",
    "print(f\"original lenght: {T}, recst length: {recst_aud.shape[2]}\")\n",
    "\n",
    "if recst_aud.shape[2] == T:\n",
    "    mse_error = np.mean((wav_stems.cpu().numpy() - recst_aud)**2)\n",
    "    print(f\"MSE: {mse_error:.6f}\")\n",
    "else:\n",
    "    print(\"length error\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. CD4MT Model Initialization (CT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CT UNet Setup \n",
    "- Compute `in_channels = S * C_lat` (here 4×64=256) and replace first/last convs to match.\n",
    "- Load best EMA checkpoint from `checkpoints/ct_unet_ema_best_val*.pth`.\n",
    "- Print total parameter count and confirm device."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Model Init and Shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build CT model from medium config and load best EMA checkpoint\n",
    "import yaml, torch, re\n",
    "from pathlib import Path\n",
    "from src.cm.script_util import create_model\n",
    "\n",
    "CFG_PATH_LOCAL = 'configs/cd4mt_medium.yaml'\n",
    "with open(CFG_PATH_LOCAL, 'r') as f:\n",
    "    cfg_local = yaml.safe_load(f)\n",
    "S = int(cfg_local['model']['params'].get('num_stems', 4))\n",
    "C = int(cfg_local['model']['params'].get('cae_z_channels', 64))\n",
    "in_channels = S * C\n",
    "\n",
    "cmov = cfg_local['model']['params'].get('cm_model_override', {})\n",
    "getv = lambda k,d: cmov.get(k,d)\n",
    "model = create_model(\n",
    "    image_size=int(getv('image_size', 32)),\n",
    "    num_channels=int(getv('num_channels', 192)),\n",
    "    num_res_blocks=int(getv('num_res_blocks', 2)),\n",
    "    channel_mult=str(getv('channel_mult', '1,2,4')),\n",
    "    learn_sigma=bool(getv('learn_sigma', False)),\n",
    "    class_cond=bool(getv('class_cond', False)),\n",
    "    use_checkpoint=bool(getv('use_checkpoint', False)),\n",
    "    attention_resolutions=str(getv('attention_resolutions', '16,8,4')),\n",
    "    num_heads=int(getv('num_heads', 6)),\n",
    "    num_head_channels=int(getv('num_head_channels', 32)),\n",
    "    num_heads_upsample=int(getv('num_heads_upsample', -1)),\n",
    "    use_scale_shift_norm=bool(getv('use_scale_shift_norm', True)),\n",
    "    dropout=float(getv('dropout', 0.1)),\n",
    "    resblock_updown=bool(getv('resblock_updown', False)),\n",
    "    use_fp16=bool(getv('use_fp16', False)),\n",
    "    use_new_attention_order=bool(getv('use_new_attention_order', False)),\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Model Param Shapes] name -> shape\n",
      "time_embed.0.weight: (768, 192)\n",
      "time_embed.0.bias: (768,)\n",
      "time_embed.2.weight: (768, 768)\n",
      "time_embed.2.bias: (768,)\n",
      "input_blocks.0.0.weight: (192, 3, 3, 3)\n",
      "input_blocks.0.0.bias: (192,)\n",
      "input_blocks.1.0.in_layers.0.weight: (192,)\n",
      "input_blocks.1.0.in_layers.0.bias: (192,)\n",
      "input_blocks.1.0.in_layers.2.weight: (192, 192, 3, 3)\n",
      "input_blocks.1.0.in_layers.2.bias: (192,)\n",
      "input_blocks.1.0.emb_layers.1.weight: (384, 768)\n",
      "input_blocks.1.0.emb_layers.1.bias: (384,)\n",
      "input_blocks.1.0.out_layers.0.weight: (192,)\n",
      "input_blocks.1.0.out_layers.0.bias: (192,)\n",
      "input_blocks.1.0.out_layers.3.weight: (192, 192, 3, 3)\n",
      "input_blocks.1.0.out_layers.3.bias: (192,)\n",
      "input_blocks.2.0.in_layers.0.weight: (192,)\n",
      "input_blocks.2.0.in_layers.0.bias: (192,)\n",
      "input_blocks.2.0.in_layers.2.weight: (192, 192, 3, 3)\n",
      "input_blocks.2.0.in_layers.2.bias: (192,)\n",
      "input_blocks.2.0.emb_layers.1.weight: (384, 768)\n",
      "input_blocks.2.0.emb_layers.1.bias: (384,)\n",
      "input_blocks.2.0.out_layers.0.weight: (192,)\n",
      "input_blocks.2.0.out_layers.0.bias: (192,)\n",
      "input_blocks.2.0.out_layers.3.weight: (192, 192, 3, 3)\n",
      "input_blocks.2.0.out_layers.3.bias: (192,)\n",
      "input_blocks.3.0.op.weight: (192, 192, 3, 3)\n",
      "input_blocks.3.0.op.bias: (192,)\n",
      "input_blocks.4.0.in_layers.0.weight: (192,)\n",
      "input_blocks.4.0.in_layers.0.bias: (192,)\n",
      "input_blocks.4.0.in_layers.2.weight: (384, 192, 3, 3)\n",
      "input_blocks.4.0.in_layers.2.bias: (384,)\n",
      "input_blocks.4.0.emb_layers.1.weight: (768, 768)\n",
      "input_blocks.4.0.emb_layers.1.bias: (768,)\n",
      "input_blocks.4.0.out_layers.0.weight: (384,)\n",
      "input_blocks.4.0.out_layers.0.bias: (384,)\n",
      "input_blocks.4.0.out_layers.3.weight: (384, 384, 3, 3)\n",
      "input_blocks.4.0.out_layers.3.bias: (384,)\n",
      "input_blocks.4.0.skip_connection.weight: (384, 192, 1, 1)\n",
      "input_blocks.4.0.skip_connection.bias: (384,)\n",
      "input_blocks.4.1.norm.weight: (384,)\n",
      "input_blocks.4.1.norm.bias: (384,)\n",
      "input_blocks.4.1.qkv.weight: (1152, 384, 1, 1)\n",
      "input_blocks.4.1.qkv.bias: (1152,)\n",
      "input_blocks.4.1.proj_out.weight: (384, 384, 1, 1)\n",
      "input_blocks.4.1.proj_out.bias: (384,)\n",
      "input_blocks.5.0.in_layers.0.weight: (384,)\n",
      "input_blocks.5.0.in_layers.0.bias: (384,)\n",
      "input_blocks.5.0.in_layers.2.weight: (384, 384, 3, 3)\n",
      "input_blocks.5.0.in_layers.2.bias: (384,)\n",
      "input_blocks.5.0.emb_layers.1.weight: (768, 768)\n",
      "input_blocks.5.0.emb_layers.1.bias: (768,)\n",
      "input_blocks.5.0.out_layers.0.weight: (384,)\n",
      "input_blocks.5.0.out_layers.0.bias: (384,)\n",
      "input_blocks.5.0.out_layers.3.weight: (384, 384, 3, 3)\n",
      "input_blocks.5.0.out_layers.3.bias: (384,)\n",
      "input_blocks.5.1.norm.weight: (384,)\n",
      "input_blocks.5.1.norm.bias: (384,)\n",
      "input_blocks.5.1.qkv.weight: (1152, 384, 1, 1)\n",
      "input_blocks.5.1.qkv.bias: (1152,)\n",
      "input_blocks.5.1.proj_out.weight: (384, 384, 1, 1)\n",
      "input_blocks.5.1.proj_out.bias: (384,)\n",
      "input_blocks.6.0.op.weight: (384, 384, 3, 3)\n",
      "input_blocks.6.0.op.bias: (384,)\n",
      "input_blocks.7.0.in_layers.0.weight: (384,)\n",
      "input_blocks.7.0.in_layers.0.bias: (384,)\n",
      "input_blocks.7.0.in_layers.2.weight: (768, 384, 3, 3)\n",
      "input_blocks.7.0.in_layers.2.bias: (768,)\n",
      "input_blocks.7.0.emb_layers.1.weight: (1536, 768)\n",
      "input_blocks.7.0.emb_layers.1.bias: (1536,)\n",
      "input_blocks.7.0.out_layers.0.weight: (768,)\n",
      "input_blocks.7.0.out_layers.0.bias: (768,)\n",
      "input_blocks.7.0.out_layers.3.weight: (768, 768, 3, 3)\n",
      "input_blocks.7.0.out_layers.3.bias: (768,)\n",
      "input_blocks.7.0.skip_connection.weight: (768, 384, 1, 1)\n",
      "input_blocks.7.0.skip_connection.bias: (768,)\n",
      "input_blocks.7.1.norm.weight: (768,)\n",
      "input_blocks.7.1.norm.bias: (768,)\n",
      "input_blocks.7.1.qkv.weight: (2304, 768, 1, 1)\n",
      "input_blocks.7.1.qkv.bias: (2304,)\n",
      "input_blocks.7.1.proj_out.weight: (768, 768, 1, 1)\n",
      "input_blocks.7.1.proj_out.bias: (768,)\n",
      "input_blocks.8.0.in_layers.0.weight: (768,)\n",
      "input_blocks.8.0.in_layers.0.bias: (768,)\n",
      "input_blocks.8.0.in_layers.2.weight: (768, 768, 3, 3)\n",
      "input_blocks.8.0.in_layers.2.bias: (768,)\n",
      "input_blocks.8.0.emb_layers.1.weight: (1536, 768)\n",
      "input_blocks.8.0.emb_layers.1.bias: (1536,)\n",
      "input_blocks.8.0.out_layers.0.weight: (768,)\n",
      "input_blocks.8.0.out_layers.0.bias: (768,)\n",
      "input_blocks.8.0.out_layers.3.weight: (768, 768, 3, 3)\n",
      "input_blocks.8.0.out_layers.3.bias: (768,)\n",
      "input_blocks.8.1.norm.weight: (768,)\n",
      "input_blocks.8.1.norm.bias: (768,)\n",
      "input_blocks.8.1.qkv.weight: (2304, 768, 1, 1)\n",
      "input_blocks.8.1.qkv.bias: (2304,)\n",
      "input_blocks.8.1.proj_out.weight: (768, 768, 1, 1)\n",
      "input_blocks.8.1.proj_out.bias: (768,)\n",
      "middle_block.0.in_layers.0.weight: (768,)\n",
      "middle_block.0.in_layers.0.bias: (768,)\n",
      "middle_block.0.in_layers.2.weight: (768, 768, 3, 3)\n",
      "middle_block.0.in_layers.2.bias: (768,)\n",
      "middle_block.0.emb_layers.1.weight: (1536, 768)\n",
      "middle_block.0.emb_layers.1.bias: (1536,)\n",
      "middle_block.0.out_layers.0.weight: (768,)\n",
      "middle_block.0.out_layers.0.bias: (768,)\n",
      "middle_block.0.out_layers.3.weight: (768, 768, 3, 3)\n",
      "middle_block.0.out_layers.3.bias: (768,)\n",
      "middle_block.1.norm.weight: (768,)\n",
      "middle_block.1.norm.bias: (768,)\n",
      "middle_block.1.qkv.weight: (2304, 768, 1, 1)\n",
      "middle_block.1.qkv.bias: (2304,)\n",
      "middle_block.1.proj_out.weight: (768, 768, 1, 1)\n",
      "middle_block.1.proj_out.bias: (768,)\n",
      "middle_block.2.in_layers.0.weight: (768,)\n",
      "middle_block.2.in_layers.0.bias: (768,)\n",
      "middle_block.2.in_layers.2.weight: (768, 768, 3, 3)\n",
      "middle_block.2.in_layers.2.bias: (768,)\n",
      "middle_block.2.emb_layers.1.weight: (1536, 768)\n",
      "middle_block.2.emb_layers.1.bias: (1536,)\n",
      "middle_block.2.out_layers.0.weight: (768,)\n",
      "middle_block.2.out_layers.0.bias: (768,)\n",
      "middle_block.2.out_layers.3.weight: (768, 768, 3, 3)\n",
      "middle_block.2.out_layers.3.bias: (768,)\n",
      "output_blocks.0.0.in_layers.0.weight: (1536,)\n",
      "output_blocks.0.0.in_layers.0.bias: (1536,)\n",
      "output_blocks.0.0.in_layers.2.weight: (768, 1536, 3, 3)\n",
      "output_blocks.0.0.in_layers.2.bias: (768,)\n",
      "output_blocks.0.0.emb_layers.1.weight: (1536, 768)\n",
      "output_blocks.0.0.emb_layers.1.bias: (1536,)\n",
      "output_blocks.0.0.out_layers.0.weight: (768,)\n",
      "output_blocks.0.0.out_layers.0.bias: (768,)\n",
      "output_blocks.0.0.out_layers.3.weight: (768, 768, 3, 3)\n",
      "output_blocks.0.0.out_layers.3.bias: (768,)\n",
      "output_blocks.0.0.skip_connection.weight: (768, 1536, 1, 1)\n",
      "output_blocks.0.0.skip_connection.bias: (768,)\n",
      "output_blocks.0.1.norm.weight: (768,)\n",
      "output_blocks.0.1.norm.bias: (768,)\n",
      "output_blocks.0.1.qkv.weight: (2304, 768, 1, 1)\n",
      "output_blocks.0.1.qkv.bias: (2304,)\n",
      "output_blocks.0.1.proj_out.weight: (768, 768, 1, 1)\n",
      "output_blocks.0.1.proj_out.bias: (768,)\n",
      "output_blocks.1.0.in_layers.0.weight: (1536,)\n",
      "output_blocks.1.0.in_layers.0.bias: (1536,)\n",
      "output_blocks.1.0.in_layers.2.weight: (768, 1536, 3, 3)\n",
      "output_blocks.1.0.in_layers.2.bias: (768,)\n",
      "output_blocks.1.0.emb_layers.1.weight: (1536, 768)\n",
      "output_blocks.1.0.emb_layers.1.bias: (1536,)\n",
      "output_blocks.1.0.out_layers.0.weight: (768,)\n",
      "output_blocks.1.0.out_layers.0.bias: (768,)\n",
      "output_blocks.1.0.out_layers.3.weight: (768, 768, 3, 3)\n",
      "output_blocks.1.0.out_layers.3.bias: (768,)\n",
      "output_blocks.1.0.skip_connection.weight: (768, 1536, 1, 1)\n",
      "output_blocks.1.0.skip_connection.bias: (768,)\n",
      "output_blocks.1.1.norm.weight: (768,)\n",
      "output_blocks.1.1.norm.bias: (768,)\n",
      "output_blocks.1.1.qkv.weight: (2304, 768, 1, 1)\n",
      "output_blocks.1.1.qkv.bias: (2304,)\n",
      "output_blocks.1.1.proj_out.weight: (768, 768, 1, 1)\n",
      "output_blocks.1.1.proj_out.bias: (768,)\n",
      "output_blocks.2.0.in_layers.0.weight: (1152,)\n",
      "output_blocks.2.0.in_layers.0.bias: (1152,)\n",
      "output_blocks.2.0.in_layers.2.weight: (768, 1152, 3, 3)\n",
      "output_blocks.2.0.in_layers.2.bias: (768,)\n",
      "output_blocks.2.0.emb_layers.1.weight: (1536, 768)\n",
      "output_blocks.2.0.emb_layers.1.bias: (1536,)\n",
      "output_blocks.2.0.out_layers.0.weight: (768,)\n",
      "output_blocks.2.0.out_layers.0.bias: (768,)\n",
      "output_blocks.2.0.out_layers.3.weight: (768, 768, 3, 3)\n",
      "output_blocks.2.0.out_layers.3.bias: (768,)\n",
      "output_blocks.2.0.skip_connection.weight: (768, 1152, 1, 1)\n",
      "output_blocks.2.0.skip_connection.bias: (768,)\n",
      "output_blocks.2.1.norm.weight: (768,)\n",
      "output_blocks.2.1.norm.bias: (768,)\n",
      "output_blocks.2.1.qkv.weight: (2304, 768, 1, 1)\n",
      "output_blocks.2.1.qkv.bias: (2304,)\n",
      "output_blocks.2.1.proj_out.weight: (768, 768, 1, 1)\n",
      "output_blocks.2.1.proj_out.bias: (768,)\n",
      "output_blocks.2.2.conv.weight: (768, 768, 3, 3)\n",
      "output_blocks.2.2.conv.bias: (768,)\n",
      "output_blocks.3.0.in_layers.0.weight: (1152,)\n",
      "output_blocks.3.0.in_layers.0.bias: (1152,)\n",
      "output_blocks.3.0.in_layers.2.weight: (384, 1152, 3, 3)\n",
      "output_blocks.3.0.in_layers.2.bias: (384,)\n",
      "output_blocks.3.0.emb_layers.1.weight: (768, 768)\n",
      "output_blocks.3.0.emb_layers.1.bias: (768,)\n",
      "output_blocks.3.0.out_layers.0.weight: (384,)\n",
      "output_blocks.3.0.out_layers.0.bias: (384,)\n",
      "output_blocks.3.0.out_layers.3.weight: (384, 384, 3, 3)\n",
      "output_blocks.3.0.out_layers.3.bias: (384,)\n",
      "output_blocks.3.0.skip_connection.weight: (384, 1152, 1, 1)\n",
      "output_blocks.3.0.skip_connection.bias: (384,)\n",
      "output_blocks.3.1.norm.weight: (384,)\n",
      "output_blocks.3.1.norm.bias: (384,)\n",
      "output_blocks.3.1.qkv.weight: (1152, 384, 1, 1)\n",
      "output_blocks.3.1.qkv.bias: (1152,)\n",
      "output_blocks.3.1.proj_out.weight: (384, 384, 1, 1)\n",
      "output_blocks.3.1.proj_out.bias: (384,)\n",
      "output_blocks.4.0.in_layers.0.weight: (768,)\n",
      "output_blocks.4.0.in_layers.0.bias: (768,)\n",
      "output_blocks.4.0.in_layers.2.weight: (384, 768, 3, 3)\n",
      "output_blocks.4.0.in_layers.2.bias: (384,)\n",
      "output_blocks.4.0.emb_layers.1.weight: (768, 768)\n",
      "output_blocks.4.0.emb_layers.1.bias: (768,)\n",
      "output_blocks.4.0.out_layers.0.weight: (384,)\n",
      "output_blocks.4.0.out_layers.0.bias: (384,)\n",
      "output_blocks.4.0.out_layers.3.weight: (384, 384, 3, 3)\n",
      "output_blocks.4.0.out_layers.3.bias: (384,)\n",
      "output_blocks.4.0.skip_connection.weight: (384, 768, 1, 1)\n",
      "output_blocks.4.0.skip_connection.bias: (384,)\n",
      "output_blocks.4.1.norm.weight: (384,)\n",
      "output_blocks.4.1.norm.bias: (384,)\n",
      "output_blocks.4.1.qkv.weight: (1152, 384, 1, 1)\n",
      "output_blocks.4.1.qkv.bias: (1152,)\n",
      "output_blocks.4.1.proj_out.weight: (384, 384, 1, 1)\n",
      "output_blocks.4.1.proj_out.bias: (384,)\n",
      "output_blocks.5.0.in_layers.0.weight: (576,)\n",
      "output_blocks.5.0.in_layers.0.bias: (576,)\n",
      "output_blocks.5.0.in_layers.2.weight: (384, 576, 3, 3)\n",
      "output_blocks.5.0.in_layers.2.bias: (384,)\n",
      "output_blocks.5.0.emb_layers.1.weight: (768, 768)\n",
      "output_blocks.5.0.emb_layers.1.bias: (768,)\n",
      "output_blocks.5.0.out_layers.0.weight: (384,)\n",
      "output_blocks.5.0.out_layers.0.bias: (384,)\n",
      "output_blocks.5.0.out_layers.3.weight: (384, 384, 3, 3)\n",
      "output_blocks.5.0.out_layers.3.bias: (384,)\n",
      "output_blocks.5.0.skip_connection.weight: (384, 576, 1, 1)\n",
      "output_blocks.5.0.skip_connection.bias: (384,)\n",
      "output_blocks.5.1.norm.weight: (384,)\n",
      "output_blocks.5.1.norm.bias: (384,)\n",
      "output_blocks.5.1.qkv.weight: (1152, 384, 1, 1)\n",
      "output_blocks.5.1.qkv.bias: (1152,)\n",
      "output_blocks.5.1.proj_out.weight: (384, 384, 1, 1)\n",
      "output_blocks.5.1.proj_out.bias: (384,)\n",
      "output_blocks.5.2.conv.weight: (384, 384, 3, 3)\n",
      "output_blocks.5.2.conv.bias: (384,)\n",
      "output_blocks.6.0.in_layers.0.weight: (576,)\n",
      "output_blocks.6.0.in_layers.0.bias: (576,)\n",
      "output_blocks.6.0.in_layers.2.weight: (192, 576, 3, 3)\n",
      "output_blocks.6.0.in_layers.2.bias: (192,)\n",
      "output_blocks.6.0.emb_layers.1.weight: (384, 768)\n",
      "output_blocks.6.0.emb_layers.1.bias: (384,)\n",
      "output_blocks.6.0.out_layers.0.weight: (192,)\n",
      "output_blocks.6.0.out_layers.0.bias: (192,)\n",
      "output_blocks.6.0.out_layers.3.weight: (192, 192, 3, 3)\n",
      "output_blocks.6.0.out_layers.3.bias: (192,)\n",
      "output_blocks.6.0.skip_connection.weight: (192, 576, 1, 1)\n",
      "output_blocks.6.0.skip_connection.bias: (192,)\n",
      "output_blocks.7.0.in_layers.0.weight: (384,)\n",
      "output_blocks.7.0.in_layers.0.bias: (384,)\n",
      "output_blocks.7.0.in_layers.2.weight: (192, 384, 3, 3)\n",
      "output_blocks.7.0.in_layers.2.bias: (192,)\n",
      "output_blocks.7.0.emb_layers.1.weight: (384, 768)\n",
      "output_blocks.7.0.emb_layers.1.bias: (384,)\n",
      "output_blocks.7.0.out_layers.0.weight: (192,)\n",
      "output_blocks.7.0.out_layers.0.bias: (192,)\n",
      "output_blocks.7.0.out_layers.3.weight: (192, 192, 3, 3)\n",
      "output_blocks.7.0.out_layers.3.bias: (192,)\n",
      "output_blocks.7.0.skip_connection.weight: (192, 384, 1, 1)\n",
      "output_blocks.7.0.skip_connection.bias: (192,)\n",
      "output_blocks.8.0.in_layers.0.weight: (384,)\n",
      "output_blocks.8.0.in_layers.0.bias: (384,)\n",
      "output_blocks.8.0.in_layers.2.weight: (192, 384, 3, 3)\n",
      "output_blocks.8.0.in_layers.2.bias: (192,)\n",
      "output_blocks.8.0.emb_layers.1.weight: (384, 768)\n",
      "output_blocks.8.0.emb_layers.1.bias: (384,)\n",
      "output_blocks.8.0.out_layers.0.weight: (192,)\n",
      "output_blocks.8.0.out_layers.0.bias: (192,)\n",
      "output_blocks.8.0.out_layers.3.weight: (192, 192, 3, 3)\n",
      "output_blocks.8.0.out_layers.3.bias: (192,)\n",
      "output_blocks.8.0.skip_connection.weight: (192, 384, 1, 1)\n",
      "output_blocks.8.0.skip_connection.bias: (192,)\n",
      "out.0.weight: (192,)\n",
      "out.0.bias: (192,)\n",
      "out.2.weight: (3, 192, 3, 3)\n",
      "out.2.bias: (3,)\n"
     ]
    }
   ],
   "source": [
    "# Utilities: concise shape logging for batches and modules\n",
    "import os, torch, pytorch_lightning as pl\n",
    "from typing import Any\n",
    "\n",
    "def shape_of(x: Any):\n",
    "    return tuple(x.shape) if hasattr(x, 'shape') else type(x).__name__\n",
    "\n",
    "class ShapeLogger(pl.Callback):\n",
    "    \"\"\"Log batch shapes at each epoch/step/GPU (local rank)\"\"\"\n",
    "    def __init__(self, prefix=\"[Data]\"):\n",
    "        self.prefix=prefix\n",
    "    def on_train_batch_start(self, trainer, pl_module, batch, batch_idx):\n",
    "        lr = getattr(trainer.strategy, \"local_rank\", 0)\n",
    "        e = trainer.current_epoch\n",
    "        s = trainer.global_step\n",
    "        if isinstance(batch, dict):\n",
    "            shapes = {k: shape_of(v) for k,v in batch.items() if hasattr(v,\"shape\")}\n",
    "        elif isinstance(batch, (list, tuple)):\n",
    "            shapes = [shape_of(v) for v in batch]\n",
    "        else:\n",
    "            shapes = shape_of(batch)\n",
    "        print(f\"{self.prefix} epoch={e} step={s} gpu={lr} batch_idx={batch_idx} shapes={shapes}\")\n",
    "    def on_validation_batch_start(self, trainer, pl_module, batch, batch_idx, dataloader_idx=0):\n",
    "        lr = getattr(trainer.strategy, \"local_rank\", 0)\n",
    "        e = trainer.current_epoch\n",
    "        s = trainer.global_step\n",
    "        if isinstance(batch, dict):\n",
    "            shapes = {k: shape_of(v) for k,v in batch.items() if hasattr(v,\"shape\")}\n",
    "        elif isinstance(batch, (list, tuple)):\n",
    "            shapes = [shape_of(v) for v in batch]\n",
    "        else:\n",
    "            shapes = shape_of(batch)\n",
    "        print(f\"[ValData] epoch={e} step={s} gpu={lr} batch_idx={batch_idx} shapes={shapes}\")\n",
    "        \n",
    "# Parameter shapes of the whole model\n",
    "sd = model.state_dict()\n",
    "print(\"[Model Param Shapes] name -> shape\")\n",
    "for k,v in sd.items():\n",
    "    s = tuple(v.shape) if hasattr(v, 'shape') else str(type(v))\n",
    "    print(f\"{k}: {s}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Infer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CT EMA ckpt: checkpoints/ct_unet_ema_best_val0.073735.pth\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CT load_state: missing=0, unexpected=0\n",
      "CT.UNet params: 151,484,992 (~577.9 MB fp32) in_channels=256\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "# adjust first/last conv to S*C\n",
    "model.input_blocks[0][0] = nn.Conv2d(in_channels, model.input_blocks[0][0].out_channels,\n",
    "                                     kernel_size=model.input_blocks[0][0].kernel_size,\n",
    "                                     stride=model.input_blocks[0][0].stride,\n",
    "                                     padding=model.input_blocks[0][0].padding,\n",
    "                                     bias=(model.input_blocks[0][0].bias is not None))\n",
    "model.out[-1] = nn.Conv2d(model.out[-1].in_channels, in_channels,\n",
    "                          kernel_size=model.out[-1].kernel_size,\n",
    "                          stride=model.out[-1].stride,\n",
    "                          padding=model.out[-1].padding,\n",
    "                          bias=(model.out[-1].bias is not None))\n",
    "model = model.to(device); model.eval()\n",
    "\n",
    "# pick best EMA ckpt from ./checkpoints (min val)\n",
    "ckdir = Path('checkpoints')\n",
    "bests = sorted(ckdir.glob('ct_unet_ema_best_val*.pth'))\n",
    "val = lambda p: (float('inf') if not re.search(r'best_val([0-9.]+)\\.pth$', p.name) else float(re.search(r'best_val([0-9.]+)\\.pth$', p.name).group(1)))\n",
    "ckpt = min(bests, key=val) if bests else sorted(ckdir.glob('ct_unet_ema_last_e*.pth'), key=lambda p: p.stat().st_mtime, reverse=True)[0]\n",
    "print('Using CT EMA ckpt:', ckpt)\n",
    "ck = torch.load(str(ckpt), map_location='cpu')\n",
    "sd = ck.get('state_dict', ck)\n",
    "missing, unexpected = model.load_state_dict(sd, strict=False)\n",
    "print(f'CT load_state: missing={len(missing)}, unexpected={len(unexpected)}')\n",
    "# report params\n",
    "_total = sum(p.numel() for p in model.parameters())\n",
    "print(f'CT.UNet params: {_total:,} (~{_total*4/1024/1024:.1f} MB fp32) in_channels={in_channels}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Audio Generation Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explainer - Sampling Note (How to Generate):\n",
    "\n",
    "- The raw CT UNet doesn’t implement `sample(...)`.\n",
    "- Use the CT helpers (see `test.py`): `KarrasDenoiser` + `karras_sample(...)`, then invert 2D latents to `(B,S,C_lat,L)` and CAE-decode.\n",
    "- Or instantiate Lightning `ScoreDiffusionModel` and call `.sample(...)` which wraps schedule/sampler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generate with Karras sampler\n",
      "  2D shape: (B=1, SC=256, H=W=12), steps=10\n",
      "Generated audio tensor: (1, 4, 524288)\n"
     ]
    }
   ],
   "source": [
    "from src.cm.karras_diffusion import KarrasDenoiser, karras_sample\n",
    "import math, numpy as np, torch\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Derive channels and latent length\n",
    "    S_local = int(S) if 'S' in globals() else int(cfg_local['model']['params'].get('num_stems', 4))\n",
    "    C_local = int(C) if 'C' in globals() else int(cfg_local['model']['params'].get('cae_z_channels', 64))\n",
    "    if 'latents_stacked' in globals():\n",
    "        L_local = int(latents_stacked.shape[-1])\n",
    "    else:\n",
    "        L_local = int(cfg_local.get('sampling', {}).get('length', 127))\n",
    "    side = int(math.sqrt(L_local))\n",
    "    if side * side < L_local:\n",
    "        side += 1\n",
    "    in_channels = S_local * C_local\n",
    "    gen_batch_size = 1\n",
    "    gen_steps = 10\n",
    "\n",
    "    sigma_min = float(cfg_local['model']['params'].get('sigma_min', 1e-4))\n",
    "    sigma_max = float(cfg_local['model']['params'].get('sigma_max', 3.0))\n",
    "    sigma_data = float(cfg_local['model']['params'].get('diffusion_sigma_data', 0.5))\n",
    "\n",
    "    print('Generate with Karras sampler')\n",
    "    print(f'  2D shape: (B={gen_batch_size}, SC={in_channels}, H=W={side}), steps={gen_steps}')\n",
    "\n",
    "    diffusion = KarrasDenoiser(\n",
    "        sigma_data=sigma_data,\n",
    "        sigma_min=sigma_min,\n",
    "        sigma_max=sigma_max,\n",
    "        weight_schedule='karras',\n",
    "        loss_norm='l2',\n",
    "    )\n",
    "\n",
    "    shape = (gen_batch_size, in_channels, side, side)\n",
    "    torch.manual_seed(12345)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(12345)\n",
    "\n",
    "    gen_imgs = karras_sample(\n",
    "        diffusion,\n",
    "        model,\n",
    "        shape=shape,\n",
    "        steps=gen_steps,\n",
    "        device=device,\n",
    "        sigma_min=sigma_min,\n",
    "        sigma_max=sigma_max,\n",
    "        model_kwargs={},\n",
    "    )\n",
    "\n",
    "    # Invert to latents and decode\n",
    "    flat = gen_imgs.float().cpu().view(gen_batch_size, in_channels, side*side)\n",
    "    if flat.shape[-1] >= L_local:\n",
    "        flat = flat[..., :L_local]\n",
    "    else:\n",
    "        reps = (L_local + flat.shape[-1] - 1) // flat.shape[-1]\n",
    "        flat = torch.cat([flat] * reps, dim=-1)[..., :L_local]\n",
    "    gen_latents = flat.view(gen_batch_size, S_local, C_local, L_local)\n",
    "\n",
    "    # Decode (first sample) to audio stems\n",
    "    gen_wavs = []\n",
    "    T_target = int(wav_stems.shape[-1]) if 'wav_stems' in globals() else None\n",
    "    for s in range(S_local):\n",
    "        stem_lat = gen_latents[0, s].numpy()\n",
    "        audio = ae.decode(stem_lat, denoising_steps=1)\n",
    "        if isinstance(audio, torch.Tensor):\n",
    "            at = audio.detach().float().cpu()\n",
    "            if at.ndim == 2 and at.shape[0] <= 16 and at.shape[0] < at.shape[1]:\n",
    "                at = at.transpose(0, 1)\n",
    "            wav = at.numpy()\n",
    "        else:\n",
    "            wav = np.asarray(audio, dtype=np.float32)\n",
    "            if wav.ndim == 2 and wav.shape[0] <= 16 and wav.shape[0] < wav.shape[1]:\n",
    "                wav = wav.T\n",
    "        wav = np.squeeze(wav).astype(np.float32)\n",
    "        if T_target is not None:\n",
    "            if wav.shape[-1] > T_target:\n",
    "                wav = wav[:T_target]\n",
    "            elif wav.shape[-1] < T_target:\n",
    "                pad = T_target - wav.shape[-1]\n",
    "                wav = np.pad(wav, (0, pad), mode='constant')\n",
    "        gen_wavs.append(wav)\n",
    "    gen_wavs = np.stack(gen_wavs, axis=0)\n",
    "    gen_aud = torch.from_numpy(gen_wavs[None, ...])\n",
    "    print(f'Generated audio tensor: {tuple(gen_aud.shape)}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cdp10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
