{"cells": [{"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Ensure project libs are on sys.path before imports\nimport sys, os\nROOT = '/data1/yuchen/cd4mt'\nsys.path.append(f'{ROOT}/src')\nsys.path.append(ROOT)\nsys.path.append(f'{ROOT}/ldm')\nprint('[ok] project paths configured')\n\n\n# Require micromamba env 'cdp10'\n_env = os.environ.get('CONDA_DEFAULT_ENV') or os.environ.get('MAMBA_DEFAULT_ENV') or ''\nprint(f\"Active env: {_env}\")\nassert 'cdp10' in _env, \"Please 'micromamba activate cdp10' before running this notebook.\"\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# CD4MT \u2014 Student Lab Report\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Abstract\n\nThis lab-style notebook demonstrates end-to-end usage of CD4MT: preparing data,\nencoding stems with a convolutional autoencoder (CAE), initializing a conditional\ndiffusion UNet for multi-track music, and running visualization/analysis.\nWe follow the original notebook's logic while presenting the content in a clear,\nstudent report format. All code uses the same configuration and checkpoint sources.\n\nOutline:\n- Environment & Setup\n- Configuration & Data\n- Methods (CAE encoder; diffusion model)\n- Experiments (inference, short training)\n- Results (visual, audio, metrics)\n- Conclusion\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Introduction\n\nCD4MT (Conditional Diffusion for Multi\u2011Track) aims to generate or reconstruct\nmultiple instrument stems from compact latent representations. We use the Slakh\ndataset at 44.1 kHz and treat each song as a mix of four stems (bass, drums,\nguitar, piano). Shapes used throughout:\n\n- Waveforms: `(B, S, T)`; mix `(B, T)`\n- CAE latent (per stem): `(B, C_lat, L)` (e.g., `(4, 64, 127)`)\n- Stacked latents: `(B, S, C_lat, L)`\n- 2D UNet input: `(B, S*C_lat, H, W)` with `H=W=ceil(sqrt(L))`\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Environment & Setup\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import torch\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Device: {device}\")\nimport torch\nprint(torch.__version__)\n\nimport os\nimport sys\nimport yaml\nimport torch\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport soundfile as sf\nfrom pathlib import Path\nimport logging\nfrom datetime import datetime\nimport pytorch_lightning as pl\nimport torch.cuda\n\nif hasattr(torch, 'set_float32_matmul_precision'):\n    torch.set_float32_matmul_precision('medium')\nfrom pytorch_lightning.callbacks import ModelCheckpoint\n\n# optional: swanlab for experiment tracking\nimport importlib.util as _ilu\nswanlab = None\nif _ilu.find_spec('swanlab') is not None:\n    import swanlab\n\n\nfrom IPython.display import Audio, display, HTML, Markdown\nimport matplotlib.font_manager as fm\nfrom src.music2latent.music2latent import EncoderDecoder\nfrom ldm.data.multitrack_datamodule import DataModuleFromConfig\nfrom src.cm.script_util import (\n    model_and_diffusion_defaults,\n    create_model_and_diffusion,\n    cm_train_defaults,\n    args_to_dict,\n    add_dict_to_argparser,\n    create_ema_and_scales_fn,\n)\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# !pip install /data1/yuchen/cd4mt/src/env/torchvision-0.22.0+cu118-cp39-cp39-manylinux_2_28_x86_64.whl\n# !pip install /data1/yuchen/cd4mt/src/env/torchvision-0.22.0+cu118-cp39-cp39-manylinux_2_28_x86_64.whl\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Configuration & Data\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Explainer - Config and First Batch (Shapes):\n", "\n", "- Reads YAML config and prints key settings (stems, batch size, sample rate).\n", "- Builds DataModule and pulls one batch to inspect shapes.\n", "- Expected keys: `['fname', 'fbank_stems', 'waveform_stems', 'waveform', 'fbank']`.\n", "- Typical shapes here:\n", "  - `waveform_stems`: `(B, S, T)`, e.g. `(4, 4, 524288)`.\n", "  - `waveform` (mix): `(B, T)`, e.g. `(4, 524288)`."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Build DataModule\nCFG_PATH = \"configs/cd4mt_medium.yaml\"\nimport yaml\nwith open(CFG_PATH, 'r') as f:\n    cfg = yaml.safe_load(f)\nprint(f\"Loaded config: {CFG_PATH}\")\nfrom ldm.data.multitrack_datamodule import DataModuleFromConfig\n\ndm = DataModuleFromConfig(**cfg[\"data\"][\"params\"])\ndm.prepare_data()\ndm.setup(stage=\"fit\")\n\ntrain_loader = dm.train_dataloader()\nprint(f\"train_loader: {len(train_loader)}\")\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Methods \u2014 CAE Encoder Test\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Explainer - CAE Encode/Stack/Decode (Shapes):\n", "\n", "- For each stem `s` in 4 stems, encode `(B, T)` to `(B, C_lat, L)` with CAE (e.g. `(4, 64, 127)`).\n", "- Stack across stems \u2192 `latents_stacked`: `(B, S, C_lat, L)` (e.g. `(4, 4, 64, 127)`).\n", "- Decode each stem back to waveform and align to original `T` via crop/pad.\n", "- Report reconstruction MSE for sanity."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Fetch one batch and define waveform tensors\nbatch = next(iter(train_loader))\nassert 'waveform_stems' in batch, 'Expected key waveform_stems in batch'\n\nwav_stems = batch['waveform_stems']  # Tensor (B, S, T)\nwave_mix = batch['waveform'] if 'waveform' in batch else wav_stems.sum(dim=1)\nB, S_batch, T = wav_stems.shape\nsample_rate = int(cfg['data']['params']['preprocessing']['audio']['sampling_rate'])\nprint(f'Batch shapes: wav_stems={tuple(wav_stems.shape)}, waveform={tuple(wave_mix.shape)}')\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["S = 4\nstem_names = cfg['model']['params']['stem_names']\nlatents_list = []\nencode_shapes = []\nae = EncoderDecoder(device=device)\nprint(f\"\\n stem_num {S} \")\n\nfor s in range(S):\n    stem_name = stem_names[s] if s < len(stem_names) else f\"stem_{s}\"\n    print(f\"\\n {stem_name} stem {s}:\")\n    stem_audio = wav_stems[:, s].cpu().numpy()  \n    print(f\"stem_audio : {stem_audio.shape}\")\n\n    stem_latents = ae.encode(stem_audio)\n    if isinstance(stem_latents, np.ndarray):\n        stem_latents = torch.from_numpy(stem_latents)\n    \n    # unify dtype to float32 to avoid AMP mix issues\n    stem_latents = stem_latents.to(dtype=torch.float32)\n    print(f\"stem_latents.shape {stem_latents.shape}, tem_latents.dtype {stem_latents.dtype}, range: [{stem_latents.min():.3f}, {stem_latents.max():.3f}]\")\n    latents_list.append(stem_latents)\n    encode_shapes.append(stem_latents.shape)\n    \n# ensure same latent length across stems for stacking\nmin_L = min(t.shape[-1] for t in latents_list)\nif any(t.shape[-1] != min_L for t in latents_list):\n    print(f\"[Warn] Latent length mismatch across stems, cropping all to L={min_L}\")\n    latents_list = [t[..., :min_L] for t in latents_list]\n\n\n\nlatents_stacked = torch.stack(latents_list, dim=1)  # (B, S, C, L)\nprint(f\"latents_stacked.shape: {latents_stacked.shape}\")\nprint(f\" Batch={latents_stacked.shape[0]}, Stems={latents_stacked.shape[1]}, Channels={latents_stacked.shape[2]}, Length={latents_stacked.shape[3]}\")\nlatents = latents_stacked.to(device)\nprint(f\" latents on : {device}\")"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["recst_list = []\nfor s in range(S):\n    stem_name = stem_names[s] if s < len(stem_names) else f\"stem_{s}\"\n    print(f\"\\nDecode {stem_name}\")\n    stem_latents = latents[:, s].cpu().numpy()  # (B, C, L)\n    recst = ae.decode(stem_latents)\n    print(f\"recst.shape: {recst.shape}\")\n    print(f\"range: [{recst.min():.3f}, {recst.max():.3f}]\")\n\n    if isinstance(recst, torch.Tensor):\n        recst = recst.cpu().numpy()\n    current_length = recst.shape[-1]\n    if current_length > T:\n        excess = current_length - T\n        start_trim = excess // 2\n        end_trim = excess - start_trim\n        recst = recst[..., start_trim:current_length-end_trim]\n    elif current_length < T:\n        deficit = T - current_length\n        pad_left = deficit // 2\n        pad_right = deficit - pad_left\n        recst = np.pad(recst, ((0,0), (pad_left, pad_right)), mode='constant', constant_values=0)\n\n    recst_list.append(recst)\n\nrecst_aud = np.stack(recst_list, axis=1)  # (B, S, T')\nrecst_tensor = torch.from_numpy(recst_aud).to(device)\n\nprint(f\"\\nrecst shape: {recst_aud.shape}\")\nprint(f\"original lenght: {T}, recst length: {recst_aud.shape[2]}\")\n\nif recst_aud.shape[2] == T:\n    mse_error = np.mean((wav_stems.cpu().numpy() - recst_aud)**2)\n    print(f\"MSE: {mse_error:.6f}\")\nelse:\n    print(\"length error\")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Methods \u2014 Diffusion Model Initialization\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Explainer \u2014 CT UNet Setup (Channels & Checkpoint):\n\n- Compute `in_channels = S * C_lat` (here 4\u00d764=256) and adjust first/last convs to match.\n- Load the best EMA checkpoint from `checkpoints/ct_unet_ema_*.pth`.\n- Print parameter count and device.\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Experiments \u2014 Inference (Pretrained)\n\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["\n# Build CT model from config and load best EMA checkpoint\nimport torch, re\nfrom pathlib import Path\nfrom src.cm.script_util import create_model\nimport torch.nn as nn\n\n# Use the same config loaded earlier\nS = int(cfg['model']['params'].get('num_stems', 4))\nC = int(cfg['model']['params'].get('cae_z_channels', 64))\nin_channels = S * C\n\ncmov = cfg['model']['params'].get('cm_model_override', {})\ngetv = lambda k,d: cmov.get(k,d)\nmodel = create_model(\n    image_size=int(getv('image_size', 32)),\n    num_channels=int(getv('num_channels', 192)),\n    num_res_blocks=int(getv('num_res_blocks', 2)),\n    channel_mult=str(getv('channel_mult', '1,2,4')),\n    learn_sigma=bool(getv('learn_sigma', False)),\n    class_cond=bool(getv('class_cond', False)),\n    use_checkpoint=bool(getv('use_checkpoint', False)),\n    attention_resolutions=str(getv('attention_resolutions', '16,8,4')),\n    num_heads=int(getv('num_heads', 6)),\n    num_head_channels=int(getv('num_head_channels', 32)),\n    num_heads_upsample=int(getv('num_heads_upsample', -1)),\n    use_scale_shift_norm=bool(getv('use_scale_shift_norm', True)),\n    dropout=float(getv('dropout', 0.1)),\n    resblock_updown=bool(getv('resblock_updown', False)),\n    use_fp16=bool(getv('use_fp16', False)),\n    use_new_attention_order=bool(getv('use_new_attention_order', False)),\n)\n# adjust first/last conv to match S*C channels\nmodel.input_blocks[0][0] = nn.Conv2d(in_channels, model.input_blocks[0][0].out_channels,\n                                     kernel_size=model.input_blocks[0][0].kernel_size,\n                                     stride=model.input_blocks[0][0].stride,\n                                     padding=model.input_blocks[0][0].padding,\n                                     bias=(model.input_blocks[0][0].bias is not None))\nmodel.out[-1] = nn.Conv2d(model.out[-1].in_channels, in_channels,\n                          kernel_size=model.out[-1].kernel_size,\n                          stride=model.out[-1].stride,\n                          padding=model.out[-1].padding,\n                          bias=(model.out[-1].bias is not None))\nmodel = model.to(device); model.eval()\n\n# pick best EMA checkpoint from ./checkpoints (lowest val)\nckdir = Path('checkpoints')\nbests = sorted(ckdir.glob('ct_unet_ema_best_val*.pth'))\n\ndef _val(p):\n    import re\n    m = re.search(r\"best_val([0-9.]+)\\.pth$\", p.name)\n    return float(m.group(1)) if m else float('inf')\n\nckpt = min(bests, key=_val) if bests else sorted(ckdir.glob('ct_unet_ema_last_e*.pth'), key=lambda p: p.stat().st_mtime, reverse=True)[0]\nprint('Using CT EMA checkpoint:', ckpt)\nck = torch.load(str(ckpt), map_location='cpu')\nsd = ck.get('state_dict', ck)\nmissing, unexpected = model.load_state_dict(sd, strict=False)\nprint(f'CT UNet load_state: missing={len(missing)}, unexpected={len(unexpected)}')\ntotal_params = sum(p.numel() for p in model.parameters())\nprint(f'CT UNet params: {total_params:,} (~{total_params*4/1024/1024:.1f} MB fp32), in_channels={in_channels}')\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Methods \u2014 Instrumentation\n\nWe instrument the ScoreDiffusionModel to print concise tensor shapes during data\nloading, CAE encode/decode, diffusion input/output, and key module internals.\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Utilities: concise shape logging for batches and modules\nimport os, torch, pytorch_lightning as pl\nfrom typing import Any\n\ndef shape_of(x: Any):\n    return tuple(x.shape) if hasattr(x, 'shape') else type(x).__name__\n\nclass ShapeLogger(pl.Callback):\n    \"\"\"Log batch shapes at each epoch/step/GPU (local rank)\"\"\"\n    def __init__(self, prefix=\"[Data]\"):\n        self.prefix=prefix\n    def on_train_batch_start(self, trainer, pl_module, batch, batch_idx):\n        lr = getattr(trainer.strategy, \"local_rank\", 0)\n        e = trainer.current_epoch\n        s = trainer.global_step\n        if isinstance(batch, dict):\n            shapes = {k: shape_of(v) for k,v in batch.items() if hasattr(v,\"shape\")}\n        elif isinstance(batch, (list, tuple)):\n            shapes = [shape_of(v) for v in batch]\n        else:\n            shapes = shape_of(batch)\n        print(f\"{self.prefix} epoch={e} step={s} gpu={lr} batch_idx={batch_idx} shapes={shapes}\")\n    def on_validation_batch_start(self, trainer, pl_module, batch, batch_idx, dataloader_idx=0):\n        lr = getattr(trainer.strategy, \"local_rank\", 0)\n        e = trainer.current_epoch\n        s = trainer.global_step\n        if isinstance(batch, dict):\n            shapes = {k: shape_of(v) for k,v in batch.items() if hasattr(v,\"shape\")}\n        elif isinstance(batch, (list, tuple)):\n            shapes = [shape_of(v) for v in batch]\n        else:\n            shapes = shape_of(batch)\n        print(f\"[ValData] epoch={e} step={s} gpu={lr} batch_idx={batch_idx} shapes={shapes}\")\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Parameter shapes of the whole model\nsd = model.state_dict()\nprint(\"[Model Param Shapes] name -> shape\")\nfor k,v in sd.items():\n    s = tuple(v.shape) if hasattr(v, 'shape') else str(type(v))\n    print(f\"{k}: {s}\")\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Model Components\n- CAE (`EncoderDecoder`): encodes per-stem waveform to `(C_lat, L)` and decodes back.\n- `ScoreDiffusionModel`: LightningModule that orchestrates CAE, UNet, and diffusion.\n- `AudioDiffusionModel_2d`: wraps UNet and diffusion objective; consumes 2D latents.\n- `UNetModel` (OpenAI): core network; `in_channels = num_stems * cae_z_channels`.\n- `KarrasSchedule`, `ADPM2Sampler`: noise schedule and sampler for training/sampling.\n- DataModule: yields batches with `waveform_stems` `(B, S, T)` and optional `waveform` `(B, T)`.\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Experiments \u2014 Training Setup\n\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Optional: short training with per-epoch/step/GPU data-shape logging\nimport pytorch_lightning as pl, torch\nuse_gpu = torch.cuda.is_available()\ntrainer = pl.Trainer(accelerator=(\"gpu\" if use_gpu else \"cpu\"), devices=(1 if use_gpu else 1),\n                    max_epochs=1, limit_train_batches=3, limit_val_batches=0, logger=False,\n                    enable_progress_bar=False, callbacks=[ShapeLogger()])\n# Uncomment to run a short fit that prints shapes per step/GPU:\n# trainer.fit(model, dm)\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### 5.3 Train (Lightning)\n\nThis training snippet is adapted from `/data1/yuchen/cd4mt_copy/scripts/train_cd4mt.py` and\nuses the same config and data module. It trains `ScoreDiffusionModel` with PyTorch Lightning\nand saves checkpoints under the configured log directory.\n"]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["# Training (adapted from cd4mt_copy/scripts/train_cd4mt.py)\n", "from pytorch_lightning import Trainer, seed_everything\n", "from pytorch_lightning.loggers import CSVLogger\n", "from pytorch_lightning.callbacks import ModelCheckpoint, LearningRateMonitor\n", "from ldm.modules.util import instantiate_from_config\n", "from ldm.models.diffusion.cd4mt_diffusion import ScoreDiffusionModel, create_model_from_config\n", "\n", "seed_everything(int(cfg.get('seed', 42)))\n", "\n", "# DataModule\n", "dm = instantiate_from_config(cfg['data'])\n", "dm.prepare_data(); dm.setup(stage='fit')\n", "\n", "# Model\n", "model = create_model_from_config(cfg['model']['params'])\n", "print(f'Model params: {sum(p.numel() for p in model.parameters()):,}')\n", "\n", "# Logger + callbacks\n", "csv_logger = CSVLogger(save_dir=cfg['log_directory'], name=cfg['id']['name'], version=cfg['id'].get('version') or None)\n", "checkpoint_callback = ModelCheckpoint(\n", "    dirpath=csv_logger.log_dir,\n", "    filename='{epoch:02d}-{val_loss:.4f}',\n", "    monitor='val/loss',\n", "    save_top_k=3,\n", "    mode='min',\n", "    save_last=True,\n", "    auto_insert_metric_name=False,\n", ")\n", "lr_monitor = LearningRateMonitor(logging_interval='step')\n", "\n", "# Trainer\n", "trainer = Trainer(\n", "    accelerator=cfg['trainer'].get('accelerator','gpu'),\n", "    devices=cfg['trainer'].get('devices',[0]),\n", "    max_epochs=cfg['trainer'].get('max_epochs', 10),\n", "    val_check_interval=cfg['trainer'].get('val_every_n_steps', 1000),\n", "    limit_train_batches=cfg['trainer'].get('limit_train_batches', 1.0),\n", "    limit_val_batches=cfg['trainer'].get('limit_val_batches', 1.0),\n", "    gradient_clip_val=cfg['trainer'].get('gradient_clip_val', 1.0),\n", "    precision=cfg['trainer'].get('precision', 32),\n", "    logger=[csv_logger],\n", "    callbacks=[checkpoint_callback, lr_monitor],\n", ")\n", "\n", "print('Trainer ready. Uncomment the next line to start training.')\n", "# trainer.fit(model, dm)\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Experiments \u2014 Audio Generation Test\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Explainer - Sampling Note (How to Generate):\n", "\n", "- The raw CT UNet doesn\u2019t implement `sample(...)`.\n", "- Use the CT helpers (see `test.py`): `KarrasDenoiser` + `karras_sample(...)`, then invert 2D latents to `(B,S,C_lat,L)` and CAE-decode.\n", "- Or instantiate Lightning `ScoreDiffusionModel` and call `.sample(...)` which wraps schedule/sampler."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from src.cm.karras_diffusion import KarrasDenoiser, karras_sample\nimport math, numpy as np, torch\n\nwith torch.no_grad():\n    # Derive channels and latent length\n    S_local = int(S) if 'S' in globals() else int(cfg_local['model']['params'].get('num_stems', 4))\n    C_local = int(C) if 'C' in globals() else int(cfg_local['model']['params'].get('cae_z_channels', 64))\n    if 'latents_stacked' in globals():\n        L_local = int(latents_stacked.shape[-1])\n    else:\n        L_local = int(cfg_local.get('sampling', {}).get('length', 127))\n    side = int(math.sqrt(L_local))\n    if side * side < L_local:\n        side += 1\n    in_channels = S_local * C_local\n    gen_batch_size = 1\n    gen_steps = 10\n\n    sigma_min = float(cfg_local['model']['params'].get('sigma_min', 1e-4))\n    sigma_max = float(cfg_local['model']['params'].get('sigma_max', 3.0))\n    sigma_data = float(cfg_local['model']['params'].get('diffusion_sigma_data', 0.5))\n\n    print('Generate with Karras sampler')\n    print(f'  2D shape: (B={gen_batch_size}, SC={in_channels}, H=W={side}), steps={gen_steps}')\n\n    diffusion = KarrasDenoiser(\n        sigma_data=sigma_data,\n        sigma_min=sigma_min,\n        sigma_max=sigma_max,\n        weight_schedule='karras',\n        loss_norm='l2',\n    )\n\n    shape = (gen_batch_size, in_channels, side, side)\n    torch.manual_seed(12345)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(12345)\n\n    gen_imgs = karras_sample(\n        diffusion,\n        model,\n        shape=shape,\n        steps=gen_steps,\n        device=device,\n        sigma_min=sigma_min,\n        sigma_max=sigma_max,\n        model_kwargs={},\n    )\n\n    # Invert to latents and decode\n    flat = gen_imgs.float().cpu().view(gen_batch_size, in_channels, side*side)\n    if flat.shape[-1] >= L_local:\n        flat = flat[..., :L_local]\n    else:\n        reps = (L_local + flat.shape[-1] - 1) // flat.shape[-1]\n        flat = torch.cat([flat] * reps, dim=-1)[..., :L_local]\n    gen_latents = flat.view(gen_batch_size, S_local, C_local, L_local)\n\n    # Decode (first sample) to audio stems\n    gen_wavs = []\n    T_target = int(wav_stems.shape[-1]) if 'wav_stems' in globals() else None\n    for s in range(S_local):\n        stem_lat = gen_latents[0, s].numpy()\n        audio = ae.decode(stem_lat, denoising_steps=1)\n        if isinstance(audio, torch.Tensor):\n            at = audio.detach().float().cpu()\n            if at.ndim == 2 and at.shape[0] <= 16 and at.shape[0] < at.shape[1]:\n                at = at.transpose(0, 1)\n            wav = at.numpy()\n        else:\n            wav = np.asarray(audio, dtype=np.float32)\n            if wav.ndim == 2 and wav.shape[0] <= 16 and wav.shape[0] < wav.shape[1]:\n                wav = wav.T\n        wav = np.squeeze(wav).astype(np.float32)\n        if T_target is not None:\n            if wav.shape[-1] > T_target:\n                wav = wav[:T_target]\n            elif wav.shape[-1] < T_target:\n                pad = T_target - wav.shape[-1]\n                wav = np.pad(wav, (0, pad), mode='constant')\n        gen_wavs.append(wav)\n    gen_wavs = np.stack(gen_wavs, axis=0)\n    gen_aud = torch.from_numpy(gen_wavs[None, ...])\n    print(f'Generated audio tensor: {tuple(gen_aud.shape)}')\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Results \u2014 Visualization & Comparison\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["vis_batch_idx = 0 # bass, frums, guitar, piano\nvis_stem_idx = 1  \nvis_length = 44100 * 3  \nsample_rate = cfg['data']['params']['preprocessing']['audio']['sampling_rate']\n\norig_aud = wav_stems[vis_batch_idx, vis_stem_idx].cpu().numpy()[:vis_length]\n\nif recst_aud.shape[2] >= vis_length:\n    recst_stem = recst_aud[vis_batch_idx, vis_stem_idx, :vis_length]\nelse:\n    recst_stem = np.pad(recst_aud[vis_batch_idx, vis_stem_idx], \n                               (0, max(0, vis_length - recst_aud.shape[2])), 'constant')\n    recst_stem = recst_stem[:vis_length]\n\nif gen_aud.shape[2] >= vis_length:\n    gen_stem = gen_aud[0, vis_stem_idx].cpu().numpy()[:vis_length]\nelse:\n    gen_stem = np.pad(gen_aud[0, vis_stem_idx].cpu().numpy(), \n                           (0, max(0, vis_length - gen_aud.shape[2])), \n                           'constant')\n    gen_stem = gen_stem[:vis_length]\n\nprint(f\"vis param:\")\nprint(f\"   track: {stem_names[vis_stem_idx]}\")\nprint(f\"   dur: {vis_length/sample_rate:.1f}s\")\nprint(f\"   sr: {sample_rate}Hz\")\n\ntime_axis = np.linspace(0, vis_length/sample_rate, vis_length)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["print(time_axis.shape)\nprint(orig_aud.shape)\nprint(recst_stem.shape)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["plt.figure(figsize=(15, 10))\n\nplt.subplot(3, 1, 1)\nplt.plot(time_axis, orig_aud, color='blue', alpha=0.8, linewidth=0.5)\nplt.title(f'Original - {stem_names[vis_stem_idx]} Track', fontsize=14, fontweight='bold')\nplt.ylabel('Amplitude')\nplt.grid(True, alpha=0.3)\nplt.xlim(0, vis_length/sample_rate)\n\nplt.subplot(3, 1, 2)\nplt.plot(time_axis, recst_stem, color='green', alpha=0.8, linewidth=0.5)\nplt.title(f'CAE Recst - {stem_names[vis_stem_idx]} Track', fontsize=14, fontweight='bold')\nplt.ylabel('Amplitude')\nplt.grid(True, alpha=0.3)\nplt.xlim(0, vis_length/sample_rate)\n\nplt.subplot(3, 1, 3)\nplt.plot(time_axis, gen_stem, color='red', alpha=0.8, linewidth=0.5)\nplt.title(f'CD4MT Gen - {stem_names[vis_stem_idx]} Track', fontsize=14, fontweight='bold')\nplt.ylabel('Amplitude')\nplt.xlabel('Time (seconds)')\nplt.grid(True, alpha=0.3)\nplt.xlim(0, vis_length/sample_rate)\n\nplt.tight_layout()\nplt.suptitle('CD4MT Aud Pipeline Comparison', fontsize=16, fontweight='bold', y=1.02)\nplt.savefig(f'fig/wave_comp_{stem_names[vis_stem_idx]}.png',\n            dpi=300, bbox_inches='tight', facecolor='white')\nplt.show()\n\nprint(\"aud stats:\")\nprint(f\"{'type':<12} | {'mean':<10} | {'std':<10} | {'min':<10} | {'max':<10}\")\nprint(\"-\" * 65)\nprint(f\"{'orig':<12} | {orig_aud.mean():<10.6f} | {orig_aud.std():<10.6f} | {orig_aud.min():<10.6f} | {orig_aud.max():<10.6f}\")\nprint(f\"{'recst':<12} | {recst_stem.mean():<10.6f} | {recst_stem.std():<10.6f} | {recst_stem.min():<10.6f} | {recst_stem.max():<10.6f}\")\nprint(f\"{'gen':<12} | {gen_stem.mean():<10.6f} | {gen_stem.std():<10.6f} | {gen_stem.min():<10.6f} | {gen_stem.max():<10.6f}\")"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from scipy import signal\n\nplt.figure(figsize=(15, 12))\n\ndef compute_spectrogram(aud, sr, title):\n    f, t, Sxx = signal.spectrogram(aud, sr, nperseg=1024, noverlap=512)\n    return f, t, 10 * np.log10(Sxx + 1e-10)\n\nplt.subplot(3, 1, 1)\nf_orig, t_orig, Sxx_orig = compute_spectrogram(orig_aud, sample_rate, 'Original')\nplt.pcolormesh(t_orig, f_orig[:200], Sxx_orig[:200], shading='gouraud', cmap='viridis')\nplt.title(f'Orig Spec - {stem_names[vis_stem_idx]}', fontweight='bold')\nplt.ylabel('Frequency (Hz)')\nplt.colorbar(label='Power (dB)')\n\nplt.subplot(3, 1, 2)\nf_recon, t_recon, Sxx_recon = compute_spectrogram(recst_stem, sample_rate, 'Reconstructed')\nplt.pcolormesh(t_recon, f_recon[:200], Sxx_recon[:200], shading='gouraud', cmap='viridis')\nplt.title(f'CAE Recst Spect - {stem_names[vis_stem_idx]}', fontweight='bold')\nplt.ylabel('Frequency (Hz)')\nplt.colorbar(label='Power (dB)')\n\nplt.subplot(3, 1, 3)\nf_gen, t_gen, Sxx_gen = compute_spectrogram(gen_stem, sample_rate, 'Generated')\nplt.pcolormesh(t_gen, f_gen[:200], Sxx_gen[:200], shading='gouraud', cmap='viridis')\nplt.title(f'CD4MT Gen Spect - {stem_names[vis_stem_idx]}', fontweight='bold')\nplt.ylabel('Frequency (Hz)')\nplt.xlabel('Time (seconds)')\nplt.colorbar(label='Power (dB)')\n\nplt.tight_layout()\nplt.suptitle('Frequency Domain Analysis', fontsize=16, fontweight='bold', y=1.02)\nplt.show()\nplt.savefig(f'fig/mel_comp_{stem_names[vis_stem_idx]}.png',\n            dpi=300, bbox_inches='tight', facecolor='white')\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Results \u2014 Multi-track Comparison\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["plt.figure(figsize=(16, 12))\n\nvis_duration = 2.0\nvis_samples = int(vis_duration * sample_rate)\ntime_axis_short = np.linspace(0, vis_duration, vis_samples)\n\nfor s in range(S):\n    stem_name = stem_names[s] if s < len(stem_names) else f\"Stem {s}\"\n\n    plt.subplot(S, 3, s*3 + 1)\n    orig_short = wav_stems[vis_batch_idx, s].cpu().numpy()[:vis_samples]\n    plt.plot(time_axis_short, orig_short, color='blue', alpha=0.8, linewidth=0.5)\n    plt.title(f'{stem_name} - Original')\n    plt.ylabel('Amplitude')\n    if s == 0:\n        plt.text(0.5, 1.1, 'Original Audio', transform=plt.gca().transAxes, \n                ha='center', fontweight='bold', fontsize=12)\n    plt.subplot(S, 3, s*3 + 2)\n    if s < recst_aud.shape[1]:\n        recon_short = recst_aud[vis_batch_idx, s, :vis_samples]\n        if len(recon_short) < vis_samples:\n            recon_short = np.pad(recon_short, (0, vis_samples - len(recon_short)), 'constant')\n        plt.plot(time_axis_short, recon_short, color='green', alpha=0.8, linewidth=0.5)\n    plt.title(f'{stem_name} - Reconstructed')\n    if s == 0:\n        plt.text(0.5, 1.1, 'CAE Reconstructed', transform=plt.gca().transAxes, \n                ha='center', fontweight='bold', fontsize=12)\n\n    plt.subplot(S, 3, s*3 + 3)\n    if s < gen_aud.shape[1]:\n        gen_short = gen_aud[0, s].cpu().numpy()[:vis_samples]\n        if len(gen_short) < vis_samples:\n            gen_short = np.pad(gen_short, (0, vis_samples - len(gen_short)), 'constant')\n        plt.plot(time_axis_short, gen_short, color='red', alpha=0.8, linewidth=0.5)\n    plt.title(f'{stem_name} - Generated')\n    if s == 0:\n        plt.text(0.5, 1.1, 'CD4MT Generated', transform=plt.gca().transAxes, \n                ha='center', fontweight='bold', fontsize=12)\n    \n    if s == S-1: \n        plt.xlabel('Time (seconds)')\n\nplt.tight_layout()\nplt.suptitle('Multi-Track Comparison: All Instruments', fontsize=16, fontweight='bold', y=1.02)\nplt.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Results \u2014 Audio Playback\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Explainer \u2014 Audio Players:\n\n- Plays short previews (default ~5s) for each stem.\n- Currently Original and CAE Reconstructed; add Generated once sampling is wired.\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Audio playback\nprint(\" Preparing audio playback...\")\n\n# Playback params\nplay_duration = 5.0  # play 5 seconds\nplay_samples = int(play_duration * sample_rate)\n\ndisplay(HTML(\"<h3>Audio Playback Comparison</h3>\"))\n\n# Stem\ndisplay(HTML(\"<h4> Original Audio Tracks</h4>\"))\nfor s in range(S):\n    stem_name = stem_names[s] if s < len(stem_names) else f\"Stem {s}\"\n    audio_data = wav_stems[vis_batch_idx, s].cpu().numpy()[:play_samples]\n    \n    display(HTML(f\"<p><strong>{stem_name} (Original):</strong></p>\"))\n    display(Audio(audio_data, rate=sample_rate))\n\n# Stem\ndisplay(HTML(\"<h4> CAE Reconstructed Tracks</h4>\"))\nfor s in range(min(S, recst_aud.shape[1])):\n    stem_name = stem_names[s] if s < len(stem_names) else f\"Stem {s}\"\n    audio_data = recst_aud[vis_batch_idx, s, :play_samples]\n    if len(audio_data) < play_samples:\n        audio_data = np.pad(audio_data, (0, play_samples - len(audio_data)), 'constant')\n    \n    display(HTML(f\"<p><strong>{stem_name} (Reconstructed):</strong></p>\"))\n    display(Audio(audio_data, rate=sample_rate))\n\n# Stem\ndisplay(HTML(\"<h4> CD4MT Generated Tracks</h4>\"))\nfor s in range(min(S, gen_aud.shape[1])):\n    stem_name = stem_names[s] if s < len(stem_names) else f\"Stem {s}\"\n    audio_data = gen_aud[0, s].cpu().numpy()[:play_samples]\n    if len(audio_data) < play_samples:\n        audio_data = np.pad(audio_data, (0, play_samples - len(audio_data)), 'constant')\n    \n    display(HTML(f\"<p><strong>{stem_name} (Generated):</strong></p>\"))\n    display(Audio(audio_data, rate=sample_rate))"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Mixed audio playback\ndisplay(HTML(\"<h4>Mixed Audio Comparison</h4>\"))\n\n# Original mix\noriginal_mix = wav_stems[vis_batch_idx].sum(dim=0).cpu().numpy()[:play_samples]\ndisplay(HTML(\"<p><strong>Original Mix (All Tracks):</strong></p>\"))\ndisplay(Audio(original_mix, rate=sample_rate))\n\n# \nreconstructed_mix = recst_aud[vis_batch_idx].sum(axis=0)[:play_samples]\nif len(reconstructed_mix) < play_samples:\n    reconstructed_mix = np.pad(reconstructed_mix, (0, play_samples - len(reconstructed_mix)), 'constant')\ndisplay(HTML(\"<p><strong>CAE Reconstructed Mix:</strong></p>\"))\ndisplay(Audio(reconstructed_mix, rate=sample_rate))\n\n# \ngenerated_mix_audio = gen_aud[0].sum(dim=0).cpu().numpy()[:play_samples]\nif len(generated_mix_audio) < play_samples:\n    generated_mix_audio = np.pad(generated_mix_audio, (0, play_samples - len(generated_mix_audio)), 'constant')\ndisplay(HTML(\"<p><strong>CD4MT Generated Mix:</strong></p>\"))\ndisplay(Audio(generated_mix_audio, rate=sample_rate))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Results \u2014 Latent Space Analysis\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Latent space visualization\nprint(\"Latent space analysis...\")\n\nplt.figure(figsize=(16, 10))\n\n# show latent per stem\nfor s in range(S):\n    stem_name = stem_names[s] if s < len(stem_names) else f\"Stem {s}\"\n    \n    plt.subplot(2, S, s + 1)\n    latent_data = latents[vis_batch_idx, s].cpu().numpy()  # (C, L)\n    plt.imshow(latent_data, aspect='auto', cmap='viridis', interpolation='nearest')\n    plt.title(f'{stem_name}\\nLatent Space')\n    plt.ylabel('Channels (64)')\n    if s == 0:\n        plt.colorbar(label='Latent Value')\n    \n    # \n    plt.subplot(2, S, s + S + 1)\n    latent_flat = latent_data.flatten()\n    plt.hist(latent_flat, bins=50, alpha=0.7, color=f'C{s}', density=True)\n    plt.title(f'{stem_name}\\nValue Distribution')\n    plt.xlabel('Latent Value')\n    plt.ylabel('Density')\n    \n    # \n    mean_val = latent_flat.mean()\n    std_val = latent_flat.std()\n    plt.axvline(mean_val, color='red', linestyle='--', alpha=0.8, label=f'Mean: {mean_val:.3f}')\n    plt.legend()\n\nplt.tight_layout()\nplt.suptitle('CAE Latent Space Analysis', fontsize=16, fontweight='bold', y=1.02)\nplt.show()\n\n# \nprint(\"\\nLatent space summary:\")\nprint(f\"{'Stem':<10} | {'Mean':<10} | {'std':<10} | {'Min':<10} | {'Max':<10} | {'Range':<10}\")\nprint(\"-\" * 75)\n\nfor s in range(S):\n    stem_name = stem_names[s] if s < len(stem_names) else f\"Stem_{s}\"\n    latent_data = latents[vis_batch_idx, s].cpu().numpy().flatten()\n    \n    mean_val = latent_data.mean()\n    std_val = latent_data.std()\n    min_val = latent_data.min()\n    max_val = latent_data.max()\n    range_val = max_val - min_val\n    \n    print(f\"{stem_name:<10} | {mean_val:<10.4f} | {std_val:<10.4f} | {min_val:<10.4f} | {max_val:<10.4f} | {range_val:<10.4f}\")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Results \u2014 Diffusion Process Visualization\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Diffusion input format analysis\nprint(\"Diffusion process visualization...\")\n\nplt.figure(figsize=(16, 8))\n\n# show diffusion input 2D format\nplt.subplot(2, 3, 1)\ndiffusion_sample = diffusion_input[0, :64]  # show first 64 channels\nplt.imshow(diffusion_sample.cpu().numpy(), aspect='auto', cmap='RdBu_r')\nplt.title('Diffusion Input\\n(First 64 Channels)')\nplt.ylabel('Channels')\nplt.colorbar()\n\nplt.subplot(2, 3, 2)\ndiffusion_sample = diffusion_input[0, 64:128]  # 65-128\nplt.imshow(diffusion_sample.cpu().numpy(), aspect='auto', cmap='RdBu_r')\nplt.title('Diffusion Input\\n(Channels 65-128)')\nplt.colorbar()\n\nplt.subplot(2, 3, 3)\ndiffusion_sample = diffusion_input[0, 128:192]  # 129-192\nplt.imshow(diffusion_sample.cpu().numpy(), aspect='auto', cmap='RdBu_r')\nplt.title('Diffusion Input\\n(Channels 129-192)')\nplt.colorbar()\n\n# \nplt.subplot(2, 3, 4)\nchannel_means = diffusion_input[0].mean(dim=(1, 2)).cpu().numpy()\nplt.plot(channel_means, 'o-', alpha=0.7)\nplt.title('Channel-wise Mean Values')\nplt.xlabel('Channel Index')\nplt.ylabel('Mean Value')\nplt.grid(True, alpha=0.3)\n\nplt.subplot(2, 3, 5)\nchannel_stds = diffusion_input[0].std(dim=(1, 2)).cpu().numpy()\nplt.plot(channel_stds, 'o-', alpha=0.7, color='orange')\nplt.title('Channel-wise Standard Deviation')\nplt.xlabel('Channel Index')\nplt.ylabel('Std Value')\nplt.grid(True, alpha=0.3)\n\nplt.subplot(2, 3, 6)\n# StemRange\nfor s in range(S):\n    start_ch = s * 64\n    end_ch = (s + 1) * 64\n    stem_name = stem_names[s] if s < len(stem_names) else f\"Stem {s}\"\n    \n    stem_channels = diffusion_input[0, start_ch:end_ch]\n    stem_mean = stem_channels.mean().item()\n    stem_std = stem_channels.std().item()\n    \n    plt.bar(s, stem_mean, yerr=stem_std, capsize=5, alpha=0.7, label=stem_name)\n\nplt.title('Per-Stem Statistics in Diffusion Input')\nplt.xlabel('Stem Index')\nplt.ylabel('Mean  Std')\nplt.xticks(range(S), [stem_names[s] if s < len(stem_names) else f\"S{s}\" for s in range(S)])\nplt.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.suptitle('Diffusion Model Input Analysis', fontsize=16, fontweight='bold', y=1.02)\nplt.show()\n\n# \nprint(f\"\\n :\")\nprint(f\"   : {diffusion_input.shape}\")\nprint(f\"   : {diffusion_input.dtype}\")\nprint(f\"   Device: {diffusion_input.device}\")\nprint(f\"   : {diffusion_input.numel() * 4 / 1024 / 1024:.2f} MB\")\nprint(f\"   Range: [{diffusion_input.min().item():.6f}, {diffusion_input.max().item():.6f}]\")\nprint(f\"   Mean: {diffusion_input.mean().item():.6f}\")\nprint(f\"   std: {diffusion_input.std().item():.6f}\")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Results \u2014 Performance & Quality\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Audio quality metrics\nprint(\"Audio quality analysis...\")\n\ndef calculate_snr(original, reconstructed):\n    \"\"\"Compute SNR\"\"\"\n    signal_power = np.mean(original ** 2)\n    noise_power = np.mean((original - reconstructed) ** 2)\n    if noise_power == 0:\n        return float('inf')\n    return 10 * np.log10(signal_power / noise_power)\n\ndef calculate_correlation(x, y):\n    \"\"\"\"\"\"\n    return np.corrcoef(x.flatten(), y.flatten())[0, 1]\n\n# \nquality_metrics = []\n\nfor s in range(S):\n    stem_name = stem_names[s] if s < len(stem_names) else f\"Stem {s}\"\n    \n    # \n    original = wav_stems[vis_batch_idx, s].cpu().numpy()\n    \n    # CAE\n    if s < recst_aud.shape[1]:\n        reconstructed = recst_aud[vis_batch_idx, s]\n        min_len = min(len(original), len(reconstructed))\n        \n        orig_crop = original[:min_len]\n        recon_crop = reconstructed[:min_len]\n        \n        # \n        mse = np.mean((orig_crop - recon_crop) ** 2)\n        snr = calculate_snr(orig_crop, recon_crop)\n        corr = calculate_correlation(orig_crop, recon_crop)\n        \n        quality_metrics.append({\n            'stem': stem_name,\n            'type': 'CAE Reconstruction',\n            'mse': mse,\n            'snr': snr,\n            'correlation': corr\n        })\n    \n    # \n    if s < gen_aud.shape[1]:\n        generated = gen_aud[0, s].cpu().numpy()\n        min_len = min(len(original), len(generated))\n        \n        orig_crop = original[:min_len]\n        gen_crop = generated[:min_len]\n        \n        # \n        mse = np.mean((orig_crop - gen_crop) ** 2)\n        snr = calculate_snr(orig_crop, gen_crop)\n        corr = calculate_correlation(orig_crop, gen_crop)\n        \n        quality_metrics.append({\n            'stem': stem_name,\n            'type': 'CD4MT Generation',\n            'mse': mse,\n            'snr': snr,\n            'correlation': corr\n        })\n\n# \nprint(\"\\n :\")\nprint(f\"{'Stem':<10} | {'':<18} | {'MSE':<12} | {'SNR (dB)':<10} | {'':<10}\")\nprint(\"-\" * 75)\n\nfor metric in quality_metrics:\n    snr_str = f\"{metric['snr']:.2f}\" if not np.isinf(metric['snr']) else \"\"\n    corr_str = f\"{metric['correlation']:.4f}\" if not np.isnan(metric['correlation']) else \"N/A\"\n    print(f\"{metric['stem']:<10} | {metric['type']:<18} | {metric['mse']:<12.6f} | {snr_str:<10} | {corr_str:<10}\")"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Metrics visualization\nplt.figure(figsize=(15, 10))\n\n# prepare data\nstems = []\nmse_recon = []\nmse_gen = []\nsnr_recon = []\nsnr_gen = []\ncorr_recon = []\ncorr_gen = []\n\nfor s in range(S):\n    stem_name = stem_names[s] if s < len(stem_names) else f\"Stem {s}\"\n    stems.append(stem_name)\n    \n    # \n    recon_metrics = next((m for m in quality_metrics if m['stem'] == stem_name and 'Reconstruction' in m['type']), None)\n    gen_metrics = next((m for m in quality_metrics if m['stem'] == stem_name and 'Generation' in m['type']), None)\n    \n    mse_recon.append(recon_metrics['mse'] if recon_metrics else 0)\n    mse_gen.append(gen_metrics['mse'] if gen_metrics else 0)\n    \n    snr_recon.append(recon_metrics['snr'] if recon_metrics and not np.isinf(recon_metrics['snr']) else 0)\n    snr_gen.append(gen_metrics['snr'] if gen_metrics and not np.isinf(gen_metrics['snr']) else 0)\n    \n    corr_recon.append(recon_metrics['correlation'] if recon_metrics and not np.isnan(recon_metrics['correlation']) else 0)\n    corr_gen.append(gen_metrics['correlation'] if gen_metrics and not np.isnan(gen_metrics['correlation']) else 0)\n\nx = np.arange(len(stems))\nwidth = 0.35\n\n# MSE\nplt.subplot(2, 2, 1)\nplt.bar(x - width/2, mse_recon, width, label='CAE Reconstruction', alpha=0.8)\nplt.bar(x + width/2, mse_gen, width, label='CD4MT Generation', alpha=0.8)\nplt.title('Mean Squared Error Comparison')\nplt.xlabel('Stems')\nplt.ylabel('MSE')\nplt.xticks(x, stems)\nplt.legend()\nplt.yscale('log')\nplt.grid(True, alpha=0.3)\n\n# SNR\nplt.subplot(2, 2, 2)\nplt.bar(x - width/2, snr_recon, width, label='CAE Reconstruction', alpha=0.8)\nplt.bar(x + width/2, snr_gen, width, label='CD4MT Generation', alpha=0.8)\nplt.title('Signal-to-Noise Ratio Comparison')\nplt.xlabel('Stems')\nplt.ylabel('SNR (dB)')\nplt.xticks(x, stems)\nplt.legend()\nplt.grid(True, alpha=0.3)\n\n# \nplt.subplot(2, 2, 3)\nplt.bar(x - width/2, corr_recon, width, label='CAE Reconstruction', alpha=0.8)\nplt.bar(x + width/2, corr_gen, width, label='CD4MT Generation', alpha=0.8)\nplt.title('Correlation Coefficient Comparison')\nplt.xlabel('Stems')\nplt.ylabel('Correlation')\nplt.xticks(x, stems)\nplt.legend()\nplt.ylim(-1, 1)\nplt.grid(True, alpha=0.3)\n\n# \nplt.subplot(2, 2, 4)\n# \nquality_scores_recon = []\nquality_scores_gen = []\n\nfor i in range(len(stems)):\n    #  (SNRMSE)\n    if mse_recon[i] > 0:\n        score_recon = (snr_recon[i] + abs(corr_recon[i]) * 50) / (1 + np.log10(mse_recon[i] + 1e-10))\n    else:\n        score_recon = 0\n    quality_scores_recon.append(max(0, score_recon))\n    \n    # \n    if mse_gen[i] > 0:\n        score_gen = (snr_gen[i] + abs(corr_gen[i]) * 50) / (1 + np.log10(mse_gen[i] + 1e-10))\n    else:\n        score_gen = 0\n    quality_scores_gen.append(max(0, score_gen))\n\nplt.bar(x - width/2, quality_scores_recon, width, label='CAE Reconstruction', alpha=0.8)\nplt.bar(x + width/2, quality_scores_gen, width, label='CD4MT Generation', alpha=0.8)\nplt.title('Composite Quality Score')\nplt.xlabel('Stems')\nplt.ylabel('Quality Score')\nplt.xticks(x, stems)\nplt.legend()\nplt.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.suptitle('Audio Quality Metrics Analysis', fontsize=16, fontweight='bold', y=1.02)\nplt.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Conclusion\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["\n# Generate analysis report\nprint('CD4MT System Report')\nprint('=' * 50)\n\nprint('\nModel configuration:')\nprint(f\"   - Num stems: {S} ({', '.join(stem_names[:S])})\")\nprint(f'   - Sample rate: {sample_rate} Hz')\nprint('   - CAE latent dim: 64')\nif model is not None:\n    print(f\"   - Parameters: {sum(p.numel() for p in model.parameters()):,}\")\n    if hasattr(model, 'sampling_steps'):\n        print(f'   - Sampling steps: {getattr(model, \"sampling_steps\")}')\n\nprint('\nData pipeline:')\nprint('   1) Waveforms (B,S,T) -> CAE latents (B,S,C,L)')\nprint('   2) Latents reshaped to 2D maps for UNet')\nprint('   3) Diffusion denoising in latent space')\nprint('   4) CAE decode back to waveforms')\n\nprint('\nBatch shapes:')\nprint(f'   - wav_stems: {tuple(wav_stems.shape)}')\nprint(f'   - latents_stacked: {tuple(latents_stacked.shape)}')\n\nprint('Notes:')\nprint('   - All results shown here are for demonstration only.')\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Environment fix\nimport os, torch\n\n# disable MPI checks for DDP\nos.environ['RANK'] = '0'\nos.environ['WORLD_SIZE'] = '1'\nos.environ['MASTER_ADDR'] = 'localhost'\nos.environ['MASTER_PORT'] = '12355'\n\n# avoid MPI-related issues\nos.environ['PL_TORCH_DISTRIBUTED_BACKEND'] = 'nccl'\nos.environ['OMPI_COMM_WORLD_RANK'] = '0'\nos.environ['OMPI_COMM_WORLD_SIZE'] = '1'\n\n# CUDA settings (single GPU for demo)\nif torch.cuda.is_available():\n    os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n\nprint('Env vars set to avoid MPI-related errors')\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Experiments \u2014 Training Run\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import logging\nimport sys\nfrom datetime import datetime\nfrom pathlib import Path\nimport pytorch_lightning as pl\nfrom pytorch_lightning.loggers import CSVLogger\nfrom pytorch_lightning.callbacks import ModelCheckpoint, LearningRateMonitor\n\n# Reset root logger handlers to avoid duplicates\nfor handler in logging.root.handlers[:]:\n    logging.root.removeHandler(handler)\n\n# Training config\nTRAIN_EPOCHS = 200  # demo value; increase for real runs\nTRAIN_LOG_DIR = \"./training_logs\"\nTRAIN_LOG_FILE = f\"{TRAIN_LOG_DIR}/cd4mt_training_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt\"\n\n# Create log dir if missing\nPath(TRAIN_LOG_DIR).mkdir(exist_ok=True)\n\n# Simple file logger wrapper\nclass TrainingLogger:\n    def __init__(self, log_file):\n        self.log_file = log_file\n        self.logger = logging.getLogger(f'CD4MT_Training_{datetime.now().strftime(\"%H%M%S\")}')\n        self.logger.setLevel(logging.INFO)\n        self.logger.handlers.clear()\n        self.file_handler = logging.FileHandler(log_file, encoding='utf-8')\n        formatter = logging.Formatter('%(asctime)s | %(levelname)s | %(message)s',\n                                      datefmt='%Y-%m-%d %H:%M:%S')\n        self.file_handler.setFormatter(formatter)\n        self.logger.addHandler(self.file_handler)\n        self.logger.propagate = False\n    def info(self, msg):\n        self.logger.info(msg)\n        print(f\"INFO: {msg}\")\n    def error(self, msg):\n        self.logger.error(msg)\n        print(f\"ERROR: {msg}\")\n    def warning(self, msg):\n        self.logger.warning(msg)\n        print(f\"WARNING: {msg}\")\n\n# Initialize training logger\ntrain_logger = TrainingLogger(TRAIN_LOG_FILE)\ntrain_logger.info(\" Start CD4MT training\")\ntrain_logger.info(f\"Log file: {TRAIN_LOG_FILE}\")\ntrain_logger.info(f\"Epochs: {TRAIN_EPOCHS}\")\ntrain_logger.info(f\"Stems: {cfg['model']['params']['stem_names']}\")\ntrain_logger.info(f\"Batch size: {cfg['data']['params']['batch_size']}\")\n\nprint(f\" Training logs will be saved to: {TRAIN_LOG_FILE}\")\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Training progress callback and loss tracking\nclass TrainingProgressCallback(pl.Callback):\n    def __init__(self, logger):\n        self.logger = logger\n        self.start_time = None\n    def on_train_start(self, trainer, pl_module):\n        self.start_time = datetime.now()\n        self.logger.info(\"=\" * 60)\n        self.logger.info(\" Training started\")\n        self.logger.info(f\"   Params: {sum(p.numel() for p in pl_module.parameters()):,}\")\n        self.logger.info(f\"   Train device: {trainer.strategy.root_device}\")\n        self.logger.info(f\"   Max epochs: {trainer.max_epochs}\")\n        self.logger.info(\"=\" * 60)\n    def on_train_epoch_start(self, trainer, pl_module):\n        epoch = trainer.current_epoch + 1\n        self.logger.info(f\" Epoch {epoch}/{trainer.max_epochs}\")\n    def on_train_batch_end(self, trainer, pl_module, outputs, batch, batch_idx):\n        # Log every 50 batches\n        if batch_idx % 50 == 0:\n            if isinstance(outputs, dict) and 'loss' in outputs:\n                loss = outputs['loss'].item()\n            elif hasattr(outputs, 'item'):\n                loss = outputs.item()\n            else:\n                loss = float('nan')\n            self.logger.info(f\"   Batch {batch_idx}: loss={loss:.6f}\")\n    def on_train_epoch_end(self, trainer, pl_module):\n        epoch = trainer.current_epoch + 1\n        elapsed = datetime.now() - self.start_time\n        train_loss = trainer.callback_metrics.get('train/loss_epoch', float('nan'))\n        val_loss = trainer.callback_metrics.get('val/loss', float('nan'))\n        self.logger.info(f\" Epoch {epoch} done\")\n        self.logger.info(f\"   Train loss: {train_loss:.6f}\")\n        self.logger.info(f\"   Val loss: {val_loss:.6f}\")\n        self.logger.info(f\"   Elapsed: {elapsed}\")\n    def on_train_end(self, trainer, pl_module):\n        total_time = datetime.now() - self.start_time\n        self.logger.info(\"=\" * 60)\n        self.logger.info(\" Training finished!\")\n        self.logger.info(f\"   Total training time: {total_time}\")\n        self.logger.info(f\"   Final train loss: {trainer.callback_metrics.get('train/loss_epoch', 'N/A')}\")\n        self.logger.info(f\"   Final val loss: {trainer.callback_metrics.get('val/loss', 'N/A')}\")\n        self.logger.info(\"=\" * 60)\n    def on_exception(self, trainer, pl_module, exception):\n        self.logger.error(f\"Training exception: {exception}\")\n\nclass LossTracker:\n    def __init__(self, log_file):\n        self.log_file = log_file\n        self.losses = []\n    def add_loss(self, epoch, batch, train_loss, val_loss=None):\n        self.losses.append({\n            'epoch': epoch,\n            'batch': batch,\n            'train_loss': train_loss,\n            'val_loss': val_loss,\n            'timestamp': datetime.now().isoformat()\n        })\n    def save_losses(self):\n        import json\n        loss_file = self.log_file.replace('.txt', '_losses.json')\n        with open(loss_file, 'w', encoding='utf-8') as f:\n            json.dump(self.losses, f, indent=2, ensure_ascii=False)\n        print(f\" Saved losses to: {loss_file}\")\n\nloss_tracker = LossTracker(TRAIN_LOG_FILE)\nprogress_callback = TrainingProgressCallback(train_logger)\nprint(\" Training callbacks and loss tracker are initialized\")"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["\n# Post-training analysis and summary\nif 'train_model' in locals():\n    print('Post-training analysis:')\n    print(f\"   Model state: {'training' if train_model.training else 'eval'}\")\n    print(f'   Device: {next(train_model.parameters()).device}')\n\n    print('\nComparison:')\n    if 'gen_aud' in locals() and 'test_generated' in locals():\n        print(f'   Before training: mean={gen_aud.mean().item():.6f}, std={gen_aud.std().item():.6f}')\n        print(f'   After training:  mean={test_generated.mean().item():.6f}, std={test_generated.std().item():.6f}')\n\n        if gen_aud.shape == test_generated.shape:\n            diff = torch.mean(torch.abs(gen_aud - test_generated)).item()\n            print(f'   Generation difference: {diff:.6f}')\n            if diff > 0.01:\n                print('   Model changed; training had an effect.')\n            else:\n                print('   Results are similar; consider more epochs.')\n\n    print('\nCD4MT training test complete.')\n    print('   Model can train and generate. Use more epochs/data for real results.')\nelse:\n    print('Train model not found; run the training cell first.')\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["\n# Check Checkpoint Files\nimport os\nfrom pathlib import Path\n\npossible_dirs = [\n    './training_logs/checkpoints',\n    './lightning_logs',\n    './checkpoints',\n    '.'\n]\n\nprint('Searching checkpoint files...')\nfound = []\nfor check_dir in possible_dirs:\n    p = Path(check_dir)\n    if p.exists():\n        for pat in ('*.ckpt','*.pth'):\n            found.extend([str(x) for x in p.rglob(pat)])\n\nif not found:\n    print('\nNo checkpoint files found')\n    print('Tip: After training, files will be saved at:')\n    print('   ./training_logs/checkpoints/')\nelse:\n    print(f\"\nFound {len(found)} checkpoint file(s)\")\n    latest = max(found, key=lambda f: os.path.getmtime(f))\n    print('Latest checkpoint: ' + latest)\n\nlog_dir = Path('./training_logs')\nif log_dir.exists():\n    log_files = sorted(log_dir.glob('*.txt'), key=lambda f: f.stat().st_mtime)[-3:]\n    if log_files:\n        print('\nTraining log files:')\n        for log_file in log_files:\n            print('   - ' + str(log_file))\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Environment fix\nimport os, torch\n\n# disable MPI checks for DDP\nos.environ['RANK'] = '0'\nos.environ['WORLD_SIZE'] = '1'\nos.environ['MASTER_ADDR'] = 'localhost'\nos.environ['MASTER_PORT'] = '12355'\n\n# avoid MPI-related issues\nos.environ['PL_TORCH_DISTRIBUTED_BACKEND'] = 'nccl'\nos.environ['OMPI_COMM_WORLD_RANK'] = '0'\nos.environ['OMPI_COMM_WORLD_SIZE'] = '1'\n\n# CUDA settings (single GPU for demo)\nif torch.cuda.is_available():\n    os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n\nprint('Env vars set to avoid MPI-related errors')\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": [""]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": [""]}], "metadata": {"kernelspec": {"display_name": "cdp10", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.10.18"}}, "nbformat": 4, "nbformat_minor": 4}