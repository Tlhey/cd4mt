# Medium config for 4090 24GB - safer option if large_cfg OOMs
# Based on mt-musicldm architecture (model_channels=192)
# Estimated ~80-100M parameters

data:
  target: DataLoader.multitrack_datamodule.DataModuleFromConfig
  params:
    batch_size: 16
    num_workers: 4

    augmentation:
      mixup: 0.0
      return_all_wav: false
      balanced_sampling: false

    path:
      dataset_type: "MultiSource_Slakh"
      train_data: dataset/slakh_44100/train
      valid_data: dataset/slakh_44100/validation
      test_data: dataset/slakh_44100/test
      stems: [bass, drums, guitar, piano]
      label_data: ""
      tempo_data: ""
      tempo_map: ""

    preprocessing:
      audio:
        sampling_rate: 44100
        segment_duration: 12.0
        normalize: true

model:
  cae:
    latent_dim: 64
    num_stems: 4

  diffusion:
    sigma_distribution:
      mean: -1.2
      std: 1.2
    sigma_data: 1.0
    dynamic_threshold: 0.0

  unet_1d:
    # Medium model - matches mt-musicldm scale
    base_channels: 256
    patch_blocks: 1
    patch_factor: 1
    kernel_sizes_init: [1, 3, 7]

    # Following mt-musicldm pattern [1,2,3,5]
    # Channels: 256 -> 512 -> 768 -> 1280
    multipliers: [1, 2, 3, 5]
    factors:     [2, 2, 2]

    # 2 blocks per level (standard)
    num_blocks:  [2, 2, 2]

    # Attention at deeper levels
    attentions:  [0, 1, 1]

    attention_heads: 8
    attention_features: 64
    attention_multiplier: 2
    resnet_groups: 8
    kernel_multiplier_downsample: 2
    use_nearest_upsample: false
    use_skip_scale: true
    use_attention_bottleneck: true
    use_context_time: true
    time_emb_type: "LearnedPositional"

train:
  seed: 42
  lr: 1.0e-4
  max_steps: 150000
  log_every: 50
  save_every: 2000
  eval_every: 50
  grad_clip_norm: 1.0
  grad_accumulation_steps: 2  # Effective batch = 32
  out_dir: "checkpoints/medium_run"

  swanlab:
    project: "cd4mt-medium"

inference:
  num_steps: 50
  sigma_min: 0.002
  sigma_max: 80.0
