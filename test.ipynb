{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CD4MT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# old ver: from flash_attn.flash_attention import FlashAttention\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "2.7.1+cu126\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device: {device}\")\n",
    "import torch\n",
    "print(torch.__version__)\n",
    "# env: cdp10\n",
    "# Python 3.10.18\n",
    "# torch 2.8.0\n",
    "# cu12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install /data1/yuchen/cd4mt/src/env/torchvision-0.22.0+cu118-cp39-cp39-manylinux_2_28_x86_64.whl\n",
    "# !pip install /data1/yuchen/cd4mt/src/env/torchvision-0.22.0+cu118-cp39-cp39-manylinux_2_28_x86_64.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working directory: /data1/yuchen/cd4mt\n",
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import os, sys, yaml, torch, numpy as np, matplotlib.pyplot as plt\n",
    "import soundfile as sf\n",
    "from pathlib import Path\n",
    "from IPython.display import Audio, display, HTML, Markdown\n",
    "import matplotlib.font_manager as fm\n",
    "\n",
    "# 设置项目根路径\n",
    "ROOT = \"/data1/yuchen/cd4mt\"\n",
    "sys.path.append(\"/data1/yuchen/MusicLDM-Ext/src\")\n",
    "sys.path.append(ROOT)\n",
    "sys.path.append(f\"{ROOT}/ldm\")\n",
    "os.chdir(ROOT)\n",
    "\n",
    "print(f\"Working directory: {os.getcwd()}\")\n",
    "from src.music2latent.music2latent import EncoderDecoder\n",
    "\n",
    "# from ldm.models.diffusion.cd4mt_diffusion import ScoreDiffusionModel\n",
    "from ldm.data.multitrack_datamodule import DataModuleFromConfig\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. config and load_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📄 配置文件: configs/cd4mt_small.yaml\n",
      "🎯 项目名称: cd4mt\n",
      "🎵 音轨类型: ['bass', 'drums', 'guitar', 'piano']\n",
      "📊 批大小: 4\n",
      "🔊 采样率: 44100Hz\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "### 🔧 关键配置参数\n",
       "| 参数 | 值 | 说明 |\n",
       "|------|----|----- |\n",
       "| 音轨数量 | 4 | bass, drums, guitar, piano |\n",
       "| CAE潜在维度 | 64 | CAE编码器输出通道数 |\n",
       "| UNet通道数 | 64 | 扩散模型基础通道数 |\n",
       "| 采样步数 | 30 | 扩散采样步数 |\n",
       "| 学习率 | 1e-4 | 训练学习率 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "CFG_PATH = \"configs/cd4mt_small.yaml\"\n",
    "\n",
    "with open(CFG_PATH, 'r') as f:\n",
    "    cfg = yaml.safe_load(f)\n",
    "\n",
    "print(f\"📄 配置文件: {CFG_PATH}\")\n",
    "print(f\"🎯 项目名称: {cfg['project_name']}\")\n",
    "print(f\"🎵 音轨类型: {cfg['data']['params']['path']['stems']}\")\n",
    "print(f\"📊 批大小: {cfg['data']['params']['batch_size']}\")\n",
    "print(f\"🔊 采样率: {cfg['data']['params']['preprocessing']['audio']['sampling_rate']}Hz\")\n",
    "\n",
    "display(Markdown(f\"\"\"\n",
    "### 🔧 关键配置参数\n",
    "| 参数 | 值 | 说明 |\n",
    "|------|----|----- |\n",
    "| 音轨数量 | {cfg['model']['params']['num_stems']} | bass, drums, guitar, piano |\n",
    "| CAE潜在维度 | {cfg['model']['params']['cae_latent_dim']} | CAE编码器输出通道数 |\n",
    "| UNet通道数 | {cfg['model']['params']['unet_']['params']['model_channels']} | 扩散模型基础通道数 |\n",
    "| 采样步数 | {cfg['model']['params']['sampling_steps']} | 扩散采样步数 |\n",
    "| 学习率 | {cfg['model']['params']['base_learning_rate']} | 训练学习率 |\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1290 tracks.\n",
      "sr=44100, min: 10, max: 600\n",
      "Keeping 1289 of 1290 tracks\n",
      "Data size: 26309\n",
      "Use mixup rate of 0.0; Use SpecAug (T,F) of (0, 0); Use blurring effect or not False\n",
      "| Audiostock Dataset Length:26309 | Epoch Length: 26309\n",
      "Found 271 tracks.\n",
      "sr=44100, min: 10, max: 600\n",
      "Keeping 270 of 271 tracks\n",
      "Data size: 5422\n",
      "Use mixup rate of 0.0; Use SpecAug (T,F) of (0, 0); Use blurring effect or not False\n",
      "| Audiostock Dataset Length:5422 | Epoch Length: 5422\n",
      "Found 152 tracks.\n",
      "sr=44100, min: 10, max: 600\n",
      "Keeping 151 of 152 tracks\n",
      "Data size: 3249\n",
      "Use mixup rate of 0.0; Use SpecAug (T,F) of (0, 0); Use blurring effect or not False\n",
      "| Audiostock Dataset Length:3249 | Epoch Length: 3249\n",
      "train_loader: 6578\n",
      "batch.keys(): ['fname', 'fbank_stems', 'waveform_stems', 'waveform', 'fbank']\n",
      "wav_stems: torch.Size([4, 4, 524288])\n",
      "wav_mix: torch.Size([4, 524288])\n",
      "Batch=4, Stems=4, Time=524288\n"
     ]
    }
   ],
   "source": [
    "dm = DataModuleFromConfig(**cfg[\"data\"][\"params\"])\n",
    "dm.prepare_data()\n",
    "dm.setup(stage=\"fit\")\n",
    "\n",
    "train_loader = dm.train_dataloader()\n",
    "print(f\"train_loader: {len(train_loader)}\")\n",
    "batch = next(iter(train_loader))\n",
    "print(f\"batch.keys(): {list(batch.keys())}\")\n",
    "\n",
    "wav_stems = batch[\"waveform_stems\"]  # (B, S, T)\n",
    "wav_mix = batch.get(\"waveform\", None)  # (B, T)\n",
    "\n",
    "print(f\"wav_stems: {wav_stems.shape}\")\n",
    "if wav_mix is not None:\n",
    "    print(f\"wav_mix: {wav_mix.shape}\")\n",
    "\n",
    "B, S, T = wav_stems.shape\n",
    "print(f\"Batch={B}, Stems={S}, Time={T}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. CAE 音频编码器测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data1/yuchen/cd4mt/src/music2latent/music2latent\n",
      "\n",
      " stem_num 4 \n",
      "\n",
      " bass stem 0:\n",
      "stem_audio : (4, 524288)\n",
      "stem_latents.shape torch.Size([4, 64, 127]), tem_latents.dtype torch.float16, range: [-4.422, 5.113]\n",
      "\n",
      " drums stem 1:\n",
      "stem_audio : (4, 524288)\n",
      "stem_latents.shape torch.Size([4, 64, 127]), tem_latents.dtype torch.float16, range: [-4.461, 3.371]\n",
      "\n",
      " guitar stem 2:\n",
      "stem_audio : (4, 524288)\n",
      "stem_latents.shape torch.Size([4, 64, 127]), tem_latents.dtype torch.float16, range: [-4.555, 4.465]\n",
      "\n",
      " piano stem 3:\n",
      "stem_audio : (4, 524288)\n",
      "stem_latents.shape torch.Size([4, 64, 127]), tem_latents.dtype torch.float16, range: [-4.352, 4.945]\n",
      "latents_stacked.shape: torch.Size([4, 4, 64, 127])\n",
      " Batch=4, Stems=4, Channels=64, Length=127\n",
      " latents on : cuda\n"
     ]
    }
   ],
   "source": [
    "ae = EncoderDecoder()\n",
    "print(f\"\\n stem_num {S} \")\n",
    "\n",
    "stem_names = cfg['model']['params']['stem_names']\n",
    "latents_list = []\n",
    "encode_shapes = []\n",
    "\n",
    "for s in range(S):\n",
    "    stem_name = stem_names[s] if s < len(stem_names) else f\"stem_{s}\"\n",
    "    print(f\"\\n {stem_name} stem {s}:\")\n",
    "    stem_audio = wav_stems[:, s].cpu().numpy()  \n",
    "    print(f\"stem_audio : {stem_audio.shape}\")\n",
    "\n",
    "    stem_latents = ae.encode(stem_audio)\n",
    "    if isinstance(stem_latents, np.ndarray):\n",
    "        stem_latents = torch.from_numpy(stem_latents)\n",
    "    \n",
    "    print(f\"stem_latents.shape {stem_latents.shape}, tem_latents.dtype {stem_latents.dtype}, range: [{stem_latents.min():.3f}, {stem_latents.max():.3f}]\")\n",
    "    latents_list.append(stem_latents)\n",
    "    encode_shapes.append(stem_latents.shape)\n",
    "\n",
    "\n",
    "latents_stacked = torch.stack(latents_list, dim=1)  # (B, S, C, L)\n",
    "print(f\"latents_stacked.shape: {latents_stacked.shape}\")\n",
    "print(f\" Batch={latents_stacked.shape[0]}, Stems={latents_stacked.shape[1]}, Channels={latents_stacked.shape[2]}, Length={latents_stacked.shape[3]}\")\n",
    "latents = latents_stacked.to(device)\n",
    "print(f\" latents on : {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Decode bass\n",
      "recst.shape: torch.Size([4, 521728])\n",
      "range: [-0.327, 0.358]\n",
      "\n",
      "Decode drums\n",
      "recst.shape: torch.Size([4, 521728])\n",
      "range: [-0.609, 0.536]\n",
      "\n",
      "Decode guitar\n",
      "recst.shape: torch.Size([4, 521728])\n",
      "range: [-0.236, 0.235]\n",
      "\n",
      "Decode piano\n",
      "recst.shape: torch.Size([4, 521728])\n",
      "range: [-0.296, 0.270]\n",
      "\n",
      "recst shape: (4, 4, 524288)\n",
      "original lenght: 524288, recst length: 524288\n",
      "MSE: 0.001459\n"
     ]
    }
   ],
   "source": [
    "recst_list = []\n",
    "for s in range(S):\n",
    "    stem_name = stem_names[s] if s < len(stem_names) else f\"stem_{s}\"\n",
    "    print(f\"\\nDecode {stem_name}\")\n",
    "    stem_latents = latents[:, s].cpu().numpy()  # (B, C, L)\n",
    "    \n",
    "    try:\n",
    "        recst = ae.decode(stem_latents)\n",
    "        print(f\"recst.shape: {recst.shape}\")\n",
    "        print(f\"range: [{recst.min():.3f}, {recst.max():.3f}]\")\n",
    "\n",
    "        if isinstance(recst, torch.Tensor):\n",
    "            recst = recst.cpu().numpy() \n",
    "        current_length = recst.shape[-1]\n",
    "        if current_length > T:\n",
    "            excess = current_length - T\n",
    "            start_trim = excess // 2\n",
    "            end_trim = excess - start_trim\n",
    "            recst = recst[..., start_trim:current_length-end_trim]\n",
    "            \n",
    "        elif current_length < T:\n",
    "            deficit = T - current_length\n",
    "            pad_left = deficit // 2\n",
    "            pad_right = deficit - pad_left\n",
    "            recst = np.pad(recst, ((0,0), (pad_left, pad_right)), mode='constant', constant_values=0)\n",
    "        \n",
    "        recst_list.append(recst)\n",
    "        \n",
    "    except Exception as e:\n",
    "        raise e\n",
    "\n",
    "recst_aud = np.stack(recst_list, axis=1)  # (B, S, T')\n",
    "recst_tensor = torch.from_numpy(recst_aud).to(device)\n",
    "\n",
    "print(f\"\\nrecst shape: {recst_aud.shape}\")\n",
    "print(f\"original lenght: {T}, recst length: {recst_aud.shape[2]}\")\n",
    "\n",
    "if recst_aud.shape[2] == T:\n",
    "    mse_error = np.mean((wav_stems.cpu().numpy() - recst_aud)**2)\n",
    "    print(f\"MSE: {mse_error:.6f}\")\n",
    "else:\n",
    "    print(\"length error\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. CD4MT 扩散模型初始化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install flash_attn\n",
    "# !pip install piq\n",
    "# !pip install blobfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "#### 🔧 Consistency Model (CM) 超参数建议\n",
       "| 参数 | 设置 | 说明 |\n",
       "|------|------|------|\n",
       "| image_size | 32 | 匹配CAE latent的二维reshape |\n",
       "| num_channels | 128 | 基础通道数，兼顾建模能力与显存 |\n",
       "| num_res_blocks | 2 | 更深的残差堆叠，提升表达力 |\n",
       "| channel_mult | 1,2,4 | 多尺度特征宽度安排 |\n",
       "| attention_resolutions | 8,4,2 | 在关键信号尺度加入注意力 |\n",
       "| dropout | 0.1 | 与原UNet保持一致，抑制过拟合 |\n",
       "| sigma_min/max | (0.0001, 3.0) | 对齐音频扩散噪声范围 |\n",
       "| sigma_data | 0.5 | 控制latent噪声幅度，与CAE训练对齐 |"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://github.com/photosynthesis-team/photosynthesis.metrics/releases/download/v0.4.0/lpips_weights.pt\" to /home/yuchen/.cache/torch/hub/checkpoints/lpips_weights.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yuchen/.local/share/mamba/envs/cdp10/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/yuchen/.local/share/mamba/envs/cdp10/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "模型参数统计:\n",
      "   总参数: 65,973,664\n",
      "   可训练参数: 65,973,664\n",
      "   模型大小: ~251.7 MB (FP32)\n",
      "模型适配信息:\n",
      "   输入通道数: 256\n",
      "   输出通道数: 32\n",
      "   目标音轨: ['bass', 'drums', 'guitar', 'piano']\n",
      "   CAE潜在维度: 64\n",
      "   采样步数: 30\n",
      "   Diffusion sigma_data: 0.5\n",
      "   sigma 区间: [0.00010, 3.0] (scheduler=karras)\n"
     ]
    }
   ],
   "source": [
    "from src.cm.script_util import create_model_and_diffusion, model_and_diffusion_defaults\n",
    "import torch.nn as nn\n",
    "\n",
    "with open(CFG_PATH, \"r\") as f:\n",
    "    cfg_fresh = yaml.safe_load(f)\n",
    "\n",
    "cm_model_params = model_and_diffusion_defaults()\n",
    "cm_model_params.update({\n",
    "    \"image_size\": cfg_fresh[\"model\"][\"params\"][\"unet_\"][\"params\"][\"image_size\"],\n",
    "    \"num_channels\": 128,\n",
    "    \"num_res_blocks\": 2,\n",
    "    \"channel_mult\": \"1,2,4\",\n",
    "    \"num_heads\": 16,\n",
    "    \"num_head_channels\": -1,\n",
    "    \"num_heads_upsample\": -1,\n",
    "    \"attention_resolutions\": \"8,4,2\",\n",
    "    \"dropout\": cfg_fresh[\"model\"][\"params\"][\"unet_\"][\"params\"][\"dropout\"],\n",
    "    \"class_cond\": False,\n",
    "    \"use_checkpoint\": False,\n",
    "    \"use_scale_shift_norm\": True,\n",
    "    \"resblock_updown\": False,\n",
    "    \"use_fp16\": False,\n",
    "    \"use_new_attention_order\": False,\n",
    "    \"learn_sigma\": False,\n",
    "    \"weight_schedule\": \"karras\",\n",
    "    \"sigma_min\": cfg_fresh[\"model\"][\"params\"].get(\"sigma_min\", cm_model_params[\"sigma_min\"]),\n",
    "    \"sigma_max\": cfg_fresh[\"model\"][\"params\"].get(\"sigma_max\", cm_model_params[\"sigma_max\"]),\n",
    "})\n",
    "\n",
    "diffusion_in_channels = cfg_fresh[\"model\"][\"params\"][\"unet_\"][\"params\"][\"in_channels\"]\n",
    "diffusion_out_channels = cfg_fresh[\"model\"][\"params\"][\"unet_\"][\"params\"][\"out_channels\"]\n",
    "sigma_data = cfg_fresh[\"model\"][\"params\"].get(\"diffusion_sigma_data\", 0.5)\n",
    "\n",
    "hyperparam_lines = [\n",
    "    \"#### 🔧 Consistency Model (CM) 超参数建议\",\n",
    "    \"| 参数 | 设置 | 说明 |\",\n",
    "    \"|------|------|------|\",\n",
    "    f\"| image_size | {cm_model_params['image_size']} | 匹配CAE latent的二维reshape |\",\n",
    "    f\"| num_channels | {cm_model_params['num_channels']} | 基础通道数，兼顾建模能力与显存 |\",\n",
    "    f\"| num_res_blocks | {cm_model_params['num_res_blocks']} | 更深的残差堆叠，提升表达力 |\",\n",
    "    f\"| channel_mult | {cm_model_params['channel_mult']} | 多尺度特征宽度安排 |\",\n",
    "    f\"| attention_resolutions | {cm_model_params['attention_resolutions']} | 在关键信号尺度加入注意力 |\",\n",
    "    f\"| dropout | {cm_model_params['dropout']} | 与原UNet保持一致，抑制过拟合 |\",\n",
    "    f\"| sigma_min/max | ({cm_model_params['sigma_min']}, {cm_model_params['sigma_max']}) | 对齐音频扩散噪声范围 |\",\n",
    "    f\"| sigma_data | {sigma_data} | 控制latent噪声幅度，与CAE训练对齐 |\",\n",
    "]\n",
    "display(Markdown(\"\\n\".join(hyperparam_lines)))\n",
    "\n",
    "\n",
    "#\n",
    "model, diffusion = create_model_and_diffusion(distillation=True, **cm_model_params)\n",
    "diffusion.sigma_data = sigma_data\n",
    "\n",
    "new_input_conv = nn.Conv2d(\n",
    "    diffusion_in_channels,\n",
    "    model.input_blocks[0][0].out_channels,\n",
    "    kernel_size=model.input_blocks[0][0].kernel_size,\n",
    "    stride=model.input_blocks[0][0].stride,\n",
    "    padding=model.input_blocks[0][0].padding,\n",
    "    bias=model.input_blocks[0][0].bias is not None,\n",
    ")\n",
    "nn.init.kaiming_normal_(new_input_conv.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n",
    "if new_input_conv.bias is not None:\n",
    "    nn.init.zeros_(new_input_conv.bias)\n",
    "model.input_blocks[0][0] = new_input_conv\n",
    "\n",
    "new_output_conv = nn.Conv2d(\n",
    "    model.out[-1].in_channels,\n",
    "    diffusion_out_channels,\n",
    "    kernel_size=model.out[-1].kernel_size,\n",
    "    stride=model.out[-1].stride,\n",
    "    padding=model.out[-1].padding,\n",
    "    bias=model.out[-1].bias is not None,\n",
    ")\n",
    "nn.init.zeros_(new_output_conv.weight)\n",
    "if new_output_conv.bias is not None:\n",
    "    nn.init.zeros_(new_output_conv.bias)\n",
    "model.out[-1] = new_output_conv\n",
    "\n",
    "model = model.to(device).eval()\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(\"模型参数统计:\")\n",
    "print(f\"   总参数: {total_params:,}\")\n",
    "print(f\"   可训练参数: {trainable_params:,}\")\n",
    "print(f\"   模型大小: ~{total_params * 4 / 1024 / 1024:.1f} MB (FP32)\")\n",
    "\n",
    "print(\"模型适配信息:\")\n",
    "print(f\"   输入通道数: {diffusion_in_channels}\")\n",
    "print(f\"   输出通道数: {diffusion_out_channels}\")\n",
    "print(f\"   目标音轨: {cfg_fresh['model']['params']['stem_names']}\")\n",
    "print(f\"   CAE潜在维度: {cfg_fresh['model']['params']['cae_latent_dim']}\")\n",
    "print(f\"   采样步数: {cfg_fresh['model']['params']['sampling_steps']}\")\n",
    "print(f\"   Diffusion sigma_data: {diffusion.sigma_data}\")\n",
    "print(f\"   sigma 区间: [{diffusion.sigma_min:.5f}, {diffusion.sigma_max:.1f}] (scheduler={diffusion.weight_schedule})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 扩散模型数据准备"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 音频生成测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if model is not None:\n",
    "    try:\n",
    "        with torch.no_grad():\n",
    "            gen_batch_size = 1\n",
    "            gen_stems = 4\n",
    "            gen_channels = 64\n",
    "            gen_length = 127\n",
    "            gen_steps = 10 \n",
    "            gen_shape = (gen_batch_size, gen_stems, gen_channels, gen_length)\n",
    "            \n",
    "            print(f\"生成参数:\")\n",
    "            print(f\"   gen_shape: {gen_shape}, gen_steps: {gen_steps}\")\n",
    "            \n",
    "            print(f\"\\nGen aud ing\")\n",
    "            gen_aud = model.sample(\n",
    "                shape=gen_shape,\n",
    "                num_steps=gen_steps\n",
    "            )\n",
    "            print(f\"gen aud shape: {gen_aud.shape}, (Batch={gen_aud.shape[0]}, Stems={gen_aud.shape[1]}, Time={gen_aud.shape[2]})\")\n",
    "            \n",
    "            print(f\"\\n gen aud:\")\n",
    "            for s in range(gen_aud.shape[1]):\n",
    "                stem_name = stem_names[s] if s < len(stem_names) else f\"stem_{s}\"\n",
    "                stem_aud = gen_aud[0, s]\n",
    "                print(f\"   {stem_name}: 均值={stem_aud.mean().item():.6f}, 标准差={stem_aud.std().item():.6f}, 范围=[{stem_aud.min().item():.3f}, {stem_aud.max().item():.3f}]\")\n",
    "            \n",
    "            gen_mix = gen_aud.sum(dim=1)  # (B, T)\n",
    "            print(f\"混合aud形状: {gen_mix.shape}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        raise e\n",
    "else:\n",
    "    print(\"no model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 音频可视化与对比分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis_batch_idx = 0 # bass, frums, guitar, piano\n",
    "vis_stem_idx = 1  \n",
    "vis_length = 44100 * 3  \n",
    "sample_rate = cfg['data']['params']['preprocessing']['audio']['sampling_rate']\n",
    "\n",
    "orig_aud = wav_stems[vis_batch_idx, vis_stem_idx].cpu().numpy()[:vis_length]\n",
    "\n",
    "if recst_aud.shape[2] >= vis_length:\n",
    "    recst_stem = recst_aud[vis_batch_idx, vis_stem_idx, :vis_length]\n",
    "else:\n",
    "    recst_stem = np.pad(recst_aud[vis_batch_idx, vis_stem_idx], \n",
    "                               (0, max(0, vis_length - recst_aud.shape[2])), 'constant')\n",
    "    recst_stem = recst_stem[:vis_length]\n",
    "\n",
    "if gen_aud.shape[2] >= vis_length:\n",
    "    gen_stem = gen_aud[0, vis_stem_idx].cpu().numpy()[:vis_length]\n",
    "else:\n",
    "    gen_stem = np.pad(gen_aud[0, vis_stem_idx].cpu().numpy(), \n",
    "                           (0, max(0, vis_length - gen_aud.shape[2])), \n",
    "                           'constant')\n",
    "    gen_stem = gen_stem[:vis_length]\n",
    "\n",
    "print(f\"vis param:\")\n",
    "print(f\"   track: {stem_names[vis_stem_idx]}\")\n",
    "print(f\"   dur: {vis_length/sample_rate:.1f}s\")\n",
    "print(f\"   sr: {sample_rate}Hz\")\n",
    "\n",
    "time_axis = np.linspace(0, vis_length/sample_rate, vis_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(time_axis.shape)\n",
    "print(orig_aud.shape)\n",
    "print(recst_stem.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "plt.subplot(3, 1, 1)\n",
    "plt.plot(time_axis, orig_aud, color='blue', alpha=0.8, linewidth=0.5)\n",
    "plt.title(f'Original - {stem_names[vis_stem_idx]} Track', fontsize=14, fontweight='bold')\n",
    "plt.ylabel('Amplitude')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.xlim(0, vis_length/sample_rate)\n",
    "\n",
    "plt.subplot(3, 1, 2)\n",
    "plt.plot(time_axis, recst_stem, color='green', alpha=0.8, linewidth=0.5)\n",
    "plt.title(f'CAE Recst - {stem_names[vis_stem_idx]} Track', fontsize=14, fontweight='bold')\n",
    "plt.ylabel('Amplitude')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.xlim(0, vis_length/sample_rate)\n",
    "\n",
    "plt.subplot(3, 1, 3)\n",
    "plt.plot(time_axis, gen_stem, color='red', alpha=0.8, linewidth=0.5)\n",
    "plt.title(f'CD4MT Gen - {stem_names[vis_stem_idx]} Track', fontsize=14, fontweight='bold')\n",
    "plt.ylabel('Amplitude')\n",
    "plt.xlabel('Time (seconds)')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.xlim(0, vis_length/sample_rate)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle('CD4MT Aud Pipeline Comparison', fontsize=16, fontweight='bold', y=1.02)\n",
    "plt.savefig(f'fig/wave_comp_{stem_names[vis_stem_idx]}.png',\n",
    "            dpi=300, bbox_inches='tight', facecolor='white')\n",
    "plt.show()\n",
    "\n",
    "print(\"aud stats:\")\n",
    "print(f\"{'type':<12} | {'mean':<10} | {'std':<10} | {'min':<10} | {'max':<10}\")\n",
    "print(\"-\" * 65)\n",
    "print(f\"{'orig':<12} | {orig_aud.mean():<10.6f} | {orig_aud.std():<10.6f} | {orig_aud.min():<10.6f} | {orig_aud.max():<10.6f}\")\n",
    "print(f\"{'recst':<12} | {recst_stem.mean():<10.6f} | {recst_stem.std():<10.6f} | {recst_stem.min():<10.6f} | {recst_stem.max():<10.6f}\")\n",
    "print(f\"{'gen':<12} | {gen_stem.mean():<10.6f} | {gen_stem.std():<10.6f} | {gen_stem.min():<10.6f} | {gen_stem.max():<10.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import signal\n",
    "\n",
    "plt.figure(figsize=(15, 12))\n",
    "\n",
    "def compute_spectrogram(aud, sr, title):\n",
    "    f, t, Sxx = signal.spectrogram(aud, sr, nperseg=1024, noverlap=512)\n",
    "    return f, t, 10 * np.log10(Sxx + 1e-10)\n",
    "\n",
    "plt.subplot(3, 1, 1)\n",
    "f_orig, t_orig, Sxx_orig = compute_spectrogram(orig_aud, sample_rate, 'Original')\n",
    "plt.pcolormesh(t_orig, f_orig[:200], Sxx_orig[:200], shading='gouraud', cmap='viridis')\n",
    "plt.title(f'Orig Spec - {stem_names[vis_stem_idx]}', fontweight='bold')\n",
    "plt.ylabel('Frequency (Hz)')\n",
    "plt.colorbar(label='Power (dB)')\n",
    "\n",
    "plt.subplot(3, 1, 2)\n",
    "f_recon, t_recon, Sxx_recon = compute_spectrogram(recst_stem, sample_rate, 'Reconstructed')\n",
    "plt.pcolormesh(t_recon, f_recon[:200], Sxx_recon[:200], shading='gouraud', cmap='viridis')\n",
    "plt.title(f'CAE Recst Spect - {stem_names[vis_stem_idx]}', fontweight='bold')\n",
    "plt.ylabel('Frequency (Hz)')\n",
    "plt.colorbar(label='Power (dB)')\n",
    "\n",
    "plt.subplot(3, 1, 3)\n",
    "f_gen, t_gen, Sxx_gen = compute_spectrogram(gen_stem, sample_rate, 'Generated')\n",
    "plt.pcolormesh(t_gen, f_gen[:200], Sxx_gen[:200], shading='gouraud', cmap='viridis')\n",
    "plt.title(f'CD4MT Gen Spect - {stem_names[vis_stem_idx]}', fontweight='bold')\n",
    "plt.ylabel('Frequency (Hz)')\n",
    "plt.xlabel('Time (seconds)')\n",
    "plt.colorbar(label='Power (dB)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle('Frequency Domain Analysis', fontsize=16, fontweight='bold', y=1.02)\n",
    "plt.show()\n",
    "plt.savefig(f'fig/mel_comp_{stem_names[vis_stem_idx]}.png',\n",
    "            dpi=300, bbox_inches='tight', facecolor='white')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. 多音轨对比分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 12))\n",
    "\n",
    "vis_duration = 2.0\n",
    "vis_samples = int(vis_duration * sample_rate)\n",
    "time_axis_short = np.linspace(0, vis_duration, vis_samples)\n",
    "\n",
    "for s in range(S):\n",
    "    stem_name = stem_names[s] if s < len(stem_names) else f\"Stem {s}\"\n",
    "\n",
    "    plt.subplot(S, 3, s*3 + 1)\n",
    "    orig_short = wav_stems[vis_batch_idx, s].cpu().numpy()[:vis_samples]\n",
    "    plt.plot(time_axis_short, orig_short, color='blue', alpha=0.8, linewidth=0.5)\n",
    "    plt.title(f'{stem_name} - Original')\n",
    "    plt.ylabel('Amplitude')\n",
    "    if s == 0:\n",
    "        plt.text(0.5, 1.1, 'Original Audio', transform=plt.gca().transAxes, \n",
    "                ha='center', fontweight='bold', fontsize=12)\n",
    "    plt.subplot(S, 3, s*3 + 2)\n",
    "    if s < recst_aud.shape[1]:\n",
    "        recon_short = recst_aud[vis_batch_idx, s, :vis_samples]\n",
    "        if len(recon_short) < vis_samples:\n",
    "            recon_short = np.pad(recon_short, (0, vis_samples - len(recon_short)), 'constant')\n",
    "        plt.plot(time_axis_short, recon_short, color='green', alpha=0.8, linewidth=0.5)\n",
    "    plt.title(f'{stem_name} - Reconstructed')\n",
    "    if s == 0:\n",
    "        plt.text(0.5, 1.1, 'CAE Reconstructed', transform=plt.gca().transAxes, \n",
    "                ha='center', fontweight='bold', fontsize=12)\n",
    "\n",
    "    plt.subplot(S, 3, s*3 + 3)\n",
    "    if s < gen_aud.shape[1]:\n",
    "        gen_short = gen_aud[0, s].cpu().numpy()[:vis_samples]\n",
    "        if len(gen_short) < vis_samples:\n",
    "            gen_short = np.pad(gen_short, (0, vis_samples - len(gen_short)), 'constant')\n",
    "        plt.plot(time_axis_short, gen_short, color='red', alpha=0.8, linewidth=0.5)\n",
    "    plt.title(f'{stem_name} - Generated')\n",
    "    if s == 0:\n",
    "        plt.text(0.5, 1.1, 'CD4MT Generated', transform=plt.gca().transAxes, \n",
    "                ha='center', fontweight='bold', fontsize=12)\n",
    "    \n",
    "    if s == S-1: \n",
    "        plt.xlabel('Time (seconds)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle('Multi-Track Comparison: All Instruments', fontsize=16, fontweight='bold', y=1.02)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. 交互式音频播放"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 音频播放功能\n",
    "print(\"🎵 准备音频播放...\")\n",
    "\n",
    "# 播放参数\n",
    "play_duration = 5.0  # 播放5秒\n",
    "play_samples = int(play_duration * sample_rate)\n",
    "\n",
    "display(HTML(\"<h3>🎵 Audio Playback Comparison</h3>\"))\n",
    "\n",
    "# 播放原始音频的每个音轨\n",
    "display(HTML(\"<h4>📻 Original Audio Tracks</h4>\"))\n",
    "for s in range(S):\n",
    "    stem_name = stem_names[s] if s < len(stem_names) else f\"Stem {s}\"\n",
    "    audio_data = wav_stems[vis_batch_idx, s].cpu().numpy()[:play_samples]\n",
    "    \n",
    "    display(HTML(f\"<p><strong>{stem_name} (Original):</strong></p>\"))\n",
    "    display(Audio(audio_data, rate=sample_rate))\n",
    "\n",
    "# 播放重建音频的每个音轨\n",
    "display(HTML(\"<h4>🔄 CAE Reconstructed Tracks</h4>\"))\n",
    "for s in range(min(S, recst_aud.shape[1])):\n",
    "    stem_name = stem_names[s] if s < len(stem_names) else f\"Stem {s}\"\n",
    "    audio_data = recst_aud[vis_batch_idx, s, :play_samples]\n",
    "    if len(audio_data) < play_samples:\n",
    "        audio_data = np.pad(audio_data, (0, play_samples - len(audio_data)), 'constant')\n",
    "    \n",
    "    display(HTML(f\"<p><strong>{stem_name} (Reconstructed):</strong></p>\"))\n",
    "    display(Audio(audio_data, rate=sample_rate))\n",
    "\n",
    "# 播放生成音频的每个音轨\n",
    "display(HTML(\"<h4>🎼 CD4MT Generated Tracks</h4>\"))\n",
    "for s in range(min(S, gen_aud.shape[1])):\n",
    "    stem_name = stem_names[s] if s < len(stem_names) else f\"Stem {s}\"\n",
    "    audio_data = gen_aud[0, s].cpu().numpy()[:play_samples]\n",
    "    if len(audio_data) < play_samples:\n",
    "        audio_data = np.pad(audio_data, (0, play_samples - len(audio_data)), 'constant')\n",
    "    \n",
    "    display(HTML(f\"<p><strong>{stem_name} (Generated):</strong></p>\"))\n",
    "    display(Audio(audio_data, rate=sample_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 混合音频播放\n",
    "display(HTML(\"<h4>🎵 Mixed Audio Comparison</h4>\"))\n",
    "\n",
    "# 原始混合\n",
    "original_mix = wav_stems[vis_batch_idx].sum(dim=0).cpu().numpy()[:play_samples]\n",
    "display(HTML(\"<p><strong>Original Mix (All Tracks):</strong></p>\"))\n",
    "display(Audio(original_mix, rate=sample_rate))\n",
    "\n",
    "# 重建混合\n",
    "reconstructed_mix = recst_aud[vis_batch_idx].sum(axis=0)[:play_samples]\n",
    "if len(reconstructed_mix) < play_samples:\n",
    "    reconstructed_mix = np.pad(reconstructed_mix, (0, play_samples - len(reconstructed_mix)), 'constant')\n",
    "display(HTML(\"<p><strong>CAE Reconstructed Mix:</strong></p>\"))\n",
    "display(Audio(reconstructed_mix, rate=sample_rate))\n",
    "\n",
    "# 生成混合\n",
    "generated_mix_audio = gen_aud[0].sum(dim=0).cpu().numpy()[:play_samples]\n",
    "if len(generated_mix_audio) < play_samples:\n",
    "    generated_mix_audio = np.pad(generated_mix_audio, (0, play_samples - len(generated_mix_audio)), 'constant')\n",
    "display(HTML(\"<p><strong>CD4MT Generated Mix:</strong></p>\"))\n",
    "display(Audio(generated_mix_audio, rate=sample_rate))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. 潜在空间分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 潜在空间可视化\n",
    "print(\"🔍 潜在空间分析...\")\n",
    "\n",
    "plt.figure(figsize=(16, 10))\n",
    "\n",
    "# 显示每个音轨的潜在表示\n",
    "for s in range(S):\n",
    "    stem_name = stem_names[s] if s < len(stem_names) else f\"Stem {s}\"\n",
    "    \n",
    "    plt.subplot(2, S, s + 1)\n",
    "    latent_data = latents[vis_batch_idx, s].cpu().numpy()  # (C, L)\n",
    "    plt.imshow(latent_data, aspect='auto', cmap='viridis', interpolation='nearest')\n",
    "    plt.title(f'{stem_name}\\nLatent Space')\n",
    "    plt.ylabel('Channels (64)')\n",
    "    if s == 0:\n",
    "        plt.colorbar(label='Latent Value')\n",
    "    \n",
    "    # 潜在表示的统计分布\n",
    "    plt.subplot(2, S, s + S + 1)\n",
    "    latent_flat = latent_data.flatten()\n",
    "    plt.hist(latent_flat, bins=50, alpha=0.7, color=f'C{s}', density=True)\n",
    "    plt.title(f'{stem_name}\\nValue Distribution')\n",
    "    plt.xlabel('Latent Value')\n",
    "    plt.ylabel('Density')\n",
    "    \n",
    "    # 添加统计信息\n",
    "    mean_val = latent_flat.mean()\n",
    "    std_val = latent_flat.std()\n",
    "    plt.axvline(mean_val, color='red', linestyle='--', alpha=0.8, label=f'Mean: {mean_val:.3f}')\n",
    "    plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle('CAE Latent Space Analysis', fontsize=16, fontweight='bold', y=1.02)\n",
    "plt.show()\n",
    "\n",
    "# 潜在空间统计摘要\n",
    "print(\"\\n📊 潜在空间统计摘要:\")\n",
    "print(f\"{'音轨':<10} | {'均值':<10} | {'标准差':<10} | {'最小值':<10} | {'最大值':<10} | {'范围':<10}\")\n",
    "print(\"-\" * 75)\n",
    "\n",
    "for s in range(S):\n",
    "    stem_name = stem_names[s] if s < len(stem_names) else f\"Stem_{s}\"\n",
    "    latent_data = latents[vis_batch_idx, s].cpu().numpy().flatten()\n",
    "    \n",
    "    mean_val = latent_data.mean()\n",
    "    std_val = latent_data.std()\n",
    "    min_val = latent_data.min()\n",
    "    max_val = latent_data.max()\n",
    "    range_val = max_val - min_val\n",
    "    \n",
    "    print(f\"{stem_name:<10} | {mean_val:<10.4f} | {std_val:<10.4f} | {min_val:<10.4f} | {max_val:<10.4f} | {range_val:<10.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. 扩散过程可视化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 扩散输入格式分析\n",
    "print(\"🔄 扩散过程可视化...\")\n",
    "\n",
    "plt.figure(figsize=(16, 8))\n",
    "\n",
    "# 显示扩散输入的2D格式\n",
    "plt.subplot(2, 3, 1)\n",
    "diffusion_sample = diffusion_input[0, :64]  # 显示前64个通道\n",
    "plt.imshow(diffusion_sample.cpu().numpy(), aspect='auto', cmap='RdBu_r')\n",
    "plt.title('Diffusion Input\\n(First 64 Channels)')\n",
    "plt.ylabel('Channels')\n",
    "plt.colorbar()\n",
    "\n",
    "plt.subplot(2, 3, 2)\n",
    "diffusion_sample = diffusion_input[0, 64:128]  # 显示第65-128个通道\n",
    "plt.imshow(diffusion_sample.cpu().numpy(), aspect='auto', cmap='RdBu_r')\n",
    "plt.title('Diffusion Input\\n(Channels 65-128)')\n",
    "plt.colorbar()\n",
    "\n",
    "plt.subplot(2, 3, 3)\n",
    "diffusion_sample = diffusion_input[0, 128:192]  # 显示第129-192个通道\n",
    "plt.imshow(diffusion_sample.cpu().numpy(), aspect='auto', cmap='RdBu_r')\n",
    "plt.title('Diffusion Input\\n(Channels 129-192)')\n",
    "plt.colorbar()\n",
    "\n",
    "# 通道维度的分布分析\n",
    "plt.subplot(2, 3, 4)\n",
    "channel_means = diffusion_input[0].mean(dim=(1, 2)).cpu().numpy()\n",
    "plt.plot(channel_means, 'o-', alpha=0.7)\n",
    "plt.title('Channel-wise Mean Values')\n",
    "plt.xlabel('Channel Index')\n",
    "plt.ylabel('Mean Value')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(2, 3, 5)\n",
    "channel_stds = diffusion_input[0].std(dim=(1, 2)).cpu().numpy()\n",
    "plt.plot(channel_stds, 'o-', alpha=0.7, color='orange')\n",
    "plt.title('Channel-wise Standard Deviation')\n",
    "plt.xlabel('Channel Index')\n",
    "plt.ylabel('Std Value')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(2, 3, 6)\n",
    "# 显示每个音轨对应的通道范围\n",
    "for s in range(S):\n",
    "    start_ch = s * 64\n",
    "    end_ch = (s + 1) * 64\n",
    "    stem_name = stem_names[s] if s < len(stem_names) else f\"Stem {s}\"\n",
    "    \n",
    "    stem_channels = diffusion_input[0, start_ch:end_ch]\n",
    "    stem_mean = stem_channels.mean().item()\n",
    "    stem_std = stem_channels.std().item()\n",
    "    \n",
    "    plt.bar(s, stem_mean, yerr=stem_std, capsize=5, alpha=0.7, label=stem_name)\n",
    "\n",
    "plt.title('Per-Stem Statistics in Diffusion Input')\n",
    "plt.xlabel('Stem Index')\n",
    "plt.ylabel('Mean ± Std')\n",
    "plt.xticks(range(S), [stem_names[s] if s < len(stem_names) else f\"S{s}\" for s in range(S)])\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle('Diffusion Model Input Analysis', fontsize=16, fontweight='bold', y=1.02)\n",
    "plt.show()\n",
    "\n",
    "# 打印扩散输入的详细信息\n",
    "print(f\"\\n📊 扩散输入详细信息:\")\n",
    "print(f\"   形状: {diffusion_input.shape}\")\n",
    "print(f\"   数据类型: {diffusion_input.dtype}\")\n",
    "print(f\"   设备: {diffusion_input.device}\")\n",
    "print(f\"   内存占用: {diffusion_input.numel() * 4 / 1024 / 1024:.2f} MB\")\n",
    "print(f\"   数值范围: [{diffusion_input.min().item():.6f}, {diffusion_input.max().item():.6f}]\")\n",
    "print(f\"   均值: {diffusion_input.mean().item():.6f}\")\n",
    "print(f\"   标准差: {diffusion_input.std().item():.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. 性能和质量分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 音频质量指标计算\n",
    "print(\"📈 音频质量分析...\")\n",
    "\n",
    "def calculate_snr(original, reconstructed):\n",
    "    \"\"\"计算信噪比\"\"\"\n",
    "    signal_power = np.mean(original ** 2)\n",
    "    noise_power = np.mean((original - reconstructed) ** 2)\n",
    "    if noise_power == 0:\n",
    "        return float('inf')\n",
    "    return 10 * np.log10(signal_power / noise_power)\n",
    "\n",
    "def calculate_correlation(x, y):\n",
    "    \"\"\"计算相关系数\"\"\"\n",
    "    return np.corrcoef(x.flatten(), y.flatten())[0, 1]\n",
    "\n",
    "# 计算各项指标\n",
    "quality_metrics = []\n",
    "\n",
    "for s in range(S):\n",
    "    stem_name = stem_names[s] if s < len(stem_names) else f\"Stem {s}\"\n",
    "    \n",
    "    # 获取音频数据\n",
    "    original = wav_stems[vis_batch_idx, s].cpu().numpy()\n",
    "    \n",
    "    # CAE重建质量\n",
    "    if s < recst_aud.shape[1]:\n",
    "        reconstructed = recst_aud[vis_batch_idx, s]\n",
    "        min_len = min(len(original), len(reconstructed))\n",
    "        \n",
    "        orig_crop = original[:min_len]\n",
    "        recon_crop = reconstructed[:min_len]\n",
    "        \n",
    "        # 计算指标\n",
    "        mse = np.mean((orig_crop - recon_crop) ** 2)\n",
    "        snr = calculate_snr(orig_crop, recon_crop)\n",
    "        corr = calculate_correlation(orig_crop, recon_crop)\n",
    "        \n",
    "        quality_metrics.append({\n",
    "            'stem': stem_name,\n",
    "            'type': 'CAE Reconstruction',\n",
    "            'mse': mse,\n",
    "            'snr': snr,\n",
    "            'correlation': corr\n",
    "        })\n",
    "    \n",
    "    # 生成音频质量（与原始对比）\n",
    "    if s < gen_aud.shape[1]:\n",
    "        generated = gen_aud[0, s].cpu().numpy()\n",
    "        min_len = min(len(original), len(generated))\n",
    "        \n",
    "        orig_crop = original[:min_len]\n",
    "        gen_crop = generated[:min_len]\n",
    "        \n",
    "        # 计算指标\n",
    "        mse = np.mean((orig_crop - gen_crop) ** 2)\n",
    "        snr = calculate_snr(orig_crop, gen_crop)\n",
    "        corr = calculate_correlation(orig_crop, gen_crop)\n",
    "        \n",
    "        quality_metrics.append({\n",
    "            'stem': stem_name,\n",
    "            'type': 'CD4MT Generation',\n",
    "            'mse': mse,\n",
    "            'snr': snr,\n",
    "            'correlation': corr\n",
    "        })\n",
    "\n",
    "# 显示质量指标表格\n",
    "print(\"\\n📊 音频质量指标:\")\n",
    "print(f\"{'音轨':<10} | {'类型':<18} | {'MSE':<12} | {'SNR (dB)':<10} | {'相关系数':<10}\")\n",
    "print(\"-\" * 75)\n",
    "\n",
    "for metric in quality_metrics:\n",
    "    snr_str = f\"{metric['snr']:.2f}\" if not np.isinf(metric['snr']) else \"∞\"\n",
    "    corr_str = f\"{metric['correlation']:.4f}\" if not np.isnan(metric['correlation']) else \"N/A\"\n",
    "    print(f\"{metric['stem']:<10} | {metric['type']:<18} | {metric['mse']:<12.6f} | {snr_str:<10} | {corr_str:<10}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 质量指标可视化\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# 准备数据\n",
    "stems = []\n",
    "mse_recon = []\n",
    "mse_gen = []\n",
    "snr_recon = []\n",
    "snr_gen = []\n",
    "corr_recon = []\n",
    "corr_gen = []\n",
    "\n",
    "for s in range(S):\n",
    "    stem_name = stem_names[s] if s < len(stem_names) else f\"Stem {s}\"\n",
    "    stems.append(stem_name)\n",
    "    \n",
    "    # 查找对应的指标\n",
    "    recon_metrics = next((m for m in quality_metrics if m['stem'] == stem_name and 'Reconstruction' in m['type']), None)\n",
    "    gen_metrics = next((m for m in quality_metrics if m['stem'] == stem_name and 'Generation' in m['type']), None)\n",
    "    \n",
    "    mse_recon.append(recon_metrics['mse'] if recon_metrics else 0)\n",
    "    mse_gen.append(gen_metrics['mse'] if gen_metrics else 0)\n",
    "    \n",
    "    snr_recon.append(recon_metrics['snr'] if recon_metrics and not np.isinf(recon_metrics['snr']) else 0)\n",
    "    snr_gen.append(gen_metrics['snr'] if gen_metrics and not np.isinf(gen_metrics['snr']) else 0)\n",
    "    \n",
    "    corr_recon.append(recon_metrics['correlation'] if recon_metrics and not np.isnan(recon_metrics['correlation']) else 0)\n",
    "    corr_gen.append(gen_metrics['correlation'] if gen_metrics and not np.isnan(gen_metrics['correlation']) else 0)\n",
    "\n",
    "x = np.arange(len(stems))\n",
    "width = 0.35\n",
    "\n",
    "# MSE对比\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.bar(x - width/2, mse_recon, width, label='CAE Reconstruction', alpha=0.8)\n",
    "plt.bar(x + width/2, mse_gen, width, label='CD4MT Generation', alpha=0.8)\n",
    "plt.title('Mean Squared Error Comparison')\n",
    "plt.xlabel('Stems')\n",
    "plt.ylabel('MSE')\n",
    "plt.xticks(x, stems)\n",
    "plt.legend()\n",
    "plt.yscale('log')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# SNR对比\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.bar(x - width/2, snr_recon, width, label='CAE Reconstruction', alpha=0.8)\n",
    "plt.bar(x + width/2, snr_gen, width, label='CD4MT Generation', alpha=0.8)\n",
    "plt.title('Signal-to-Noise Ratio Comparison')\n",
    "plt.xlabel('Stems')\n",
    "plt.ylabel('SNR (dB)')\n",
    "plt.xticks(x, stems)\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# 相关系数对比\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.bar(x - width/2, corr_recon, width, label='CAE Reconstruction', alpha=0.8)\n",
    "plt.bar(x + width/2, corr_gen, width, label='CD4MT Generation', alpha=0.8)\n",
    "plt.title('Correlation Coefficient Comparison')\n",
    "plt.xlabel('Stems')\n",
    "plt.ylabel('Correlation')\n",
    "plt.xticks(x, stems)\n",
    "plt.legend()\n",
    "plt.ylim(-1, 1)\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# 综合质量评分（基于多个指标的加权平均）\n",
    "plt.subplot(2, 2, 4)\n",
    "# 标准化指标并计算综合评分\n",
    "quality_scores_recon = []\n",
    "quality_scores_gen = []\n",
    "\n",
    "for i in range(len(stems)):\n",
    "    # 重建质量评分 (SNR高好，MSE低好，相关系数高好)\n",
    "    if mse_recon[i] > 0:\n",
    "        score_recon = (snr_recon[i] + abs(corr_recon[i]) * 50) / (1 + np.log10(mse_recon[i] + 1e-10))\n",
    "    else:\n",
    "        score_recon = 0\n",
    "    quality_scores_recon.append(max(0, score_recon))\n",
    "    \n",
    "    # 生成质量评分\n",
    "    if mse_gen[i] > 0:\n",
    "        score_gen = (snr_gen[i] + abs(corr_gen[i]) * 50) / (1 + np.log10(mse_gen[i] + 1e-10))\n",
    "    else:\n",
    "        score_gen = 0\n",
    "    quality_scores_gen.append(max(0, score_gen))\n",
    "\n",
    "plt.bar(x - width/2, quality_scores_recon, width, label='CAE Reconstruction', alpha=0.8)\n",
    "plt.bar(x + width/2, quality_scores_gen, width, label='CD4MT Generation', alpha=0.8)\n",
    "plt.title('Composite Quality Score')\n",
    "plt.xlabel('Stems')\n",
    "plt.ylabel('Quality Score')\n",
    "plt.xticks(x, stems)\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle('Audio Quality Metrics Analysis', fontsize=16, fontweight='bold', y=1.02)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. 总结和结论"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 生成分析报告\n",
    "print(\"📋 CD4MT 系统分析报告\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(f\"\\n🎯 模型配置:\")\n",
    "print(f\"   - 音轨数量: {S} ({', '.join(stem_names[:S])})\")\n",
    "print(f\"   - 采样率: {sample_rate} Hz\")\n",
    "print(f\"   - CAE潜在维度: 64\")\n",
    "if model is not None:\n",
    "    print(f\"   - 模型参数: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "    print(f\"   - 采样步数: {model.sampling_steps}\")\n",
    "\n",
    "print(f\"\\n📊 数据处理流程:\")\n",
    "print(f\"   1. 原始音频: {wav_stems.shape}\")\n",
    "print(f\"   2. CAE编码: {latents.shape}\")\n",
    "print(f\"   3. 扩散输入: {diffusion_input.shape}\")\n",
    "print(f\"   4. 生成音频: {gen_aud.shape}\")\n",
    "\n",
    "print(f\"\\n🔍 质量评估:\")\n",
    "avg_mse_recon = np.mean([m for m in mse_recon if m > 0])\n",
    "avg_mse_gen = np.mean([m for m in mse_gen if m > 0])\n",
    "avg_corr_recon = np.mean([c for c in corr_recon if c != 0])\n",
    "avg_corr_gen = np.mean([c for c in corr_gen if c != 0])\n",
    "\n",
    "print(f\"   - CAE重建MSE: {avg_mse_recon:.6f}\")\n",
    "print(f\"   - CD4MT生成MSE: {avg_mse_gen:.6f}\")\n",
    "print(f\"   - CAE重建相关性: {avg_corr_recon:.4f}\")\n",
    "print(f\"   - CD4MT生成相关性: {avg_corr_gen:.4f}\")\n",
    "\n",
    "print(f\"\\n✅ 系统状态:\")\n",
    "print(f\"   - CAE编码器: {'✅ 正常' if 'ae' in locals() else '❌ 未加载'}\")\n",
    "print(f\"   - CD4MT模型: {'✅ 正常' if model is not None else '❌ 未加载'}\")\n",
    "print(f\"   - 数据加载: {'✅ 正常' if 'batch' in locals() else '❌ 失败'}\")\n",
    "print(f\"   - GPU加速: {'✅ 可用' if torch.cuda.is_available() else '❌ 不可用'}\")\n",
    "\n",
    "print(f\"\\n🎵 音频特征:\")\n",
    "for s in range(S):\n",
    "    stem_name = stem_names[s] if s < len(stem_names) else f\"Stem {s}\"\n",
    "    original_energy = np.sqrt(np.mean(wav_stems[vis_batch_idx, s].cpu().numpy() ** 2))\n",
    "    generated_energy = np.sqrt(np.mean(gen_aud[0, s].cpu().numpy() ** 2)) if s < gen_aud.shape[1] else 0\n",
    "    print(f\"   - {stem_name}: 原始能量={original_energy:.6f}, 生成能量={generated_energy:.6f}\")\n",
    "\n",
    "print(f\"\\n🎼 潜在空间统计:\")\n",
    "print(f\"   - 维度: {latents.shape}\")\n",
    "print(f\"   - 数值范围: [{latents.min().item():.3f}, {latents.max().item():.3f}]\")\n",
    "print(f\"   - 平均值: {latents.mean().item():.6f}\")\n",
    "print(f\"   - 标准差: {latents.std().item():.6f}\")\n",
    "\n",
    "print(f\"\\n💡 使用建议:\")\n",
    "if avg_mse_recon < 0.01:\n",
    "    print(f\"   ✅ CAE重建质量良好\")\n",
    "else:\n",
    "    print(f\"   ⚠️  CAE重建质量有待改善\")\n",
    "\n",
    "if model is not None:\n",
    "    print(f\"   ✅ 可以进行音频生成实验\")\n",
    "    print(f\"   💡 建议使用30-50采样步数获得更好质量\")\n",
    "else:\n",
    "    print(f\"   ⚠️  需要训练好的模型checkpoint\")\n",
    "\n",
    "print(f\"\\n📝 注意事项:\")\n",
    "print(f\"   - 本演示使用的是{'真实' if 'dm' in locals() and hasattr(dm, 'train_dataloader') else '模拟'}数据\")\n",
    "print(f\"   - 生成质量取决于模型训练程度\")\n",
    "print(f\"   - 实际应用建议使用完整训练的模型\")\n",
    "\n",
    "print(f\"\\n\" + \"=\" * 50)\n",
    "print(f\"CD4MT 可视化分析完成! 🎉\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 修复环境问题\n",
    "import os\n",
    "\n",
    "# 禁用分布式训练相关的MPI检查\n",
    "os.environ['RANK'] = '0'\n",
    "os.environ['WORLD_SIZE'] = '1'\n",
    "os.environ['MASTER_ADDR'] = 'localhost'\n",
    "os.environ['MASTER_PORT'] = '12355'\n",
    "\n",
    "# 避免 MPI 相关错误\n",
    "os.environ['PL_TORCH_DISTRIBUTED_BACKEND'] = 'nccl'\n",
    "os.environ['OMPI_COMM_WORLD_RANK'] = '0'\n",
    "os.environ['OMPI_COMM_WORLD_SIZE'] = '1'\n",
    "\n",
    "# 设置CUDA相关\n",
    "if torch.cuda.is_available():\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "\n",
    "print(\"🔧 环境变量已设置，避免MPI相关错误\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. 模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers import CSVLogger\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, LearningRateMonitor\n",
    "\n",
    "# 清理之前的logger\n",
    "for handler in logging.root.handlers[:]:\n",
    "    logging.root.removeHandler(handler)\n",
    "\n",
    "# 训练配置\n",
    "TRAIN_EPOCHS = 200  # 测试训练，建议正式训练用20+\n",
    "TRAIN_LOG_DIR = \"./training_logs\"\n",
    "TRAIN_LOG_FILE = f\"{TRAIN_LOG_DIR}/cd4mt_training_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt\"\n",
    "\n",
    "# 创建日志目录\n",
    "Path(TRAIN_LOG_DIR).mkdir(exist_ok=True)\n",
    "\n",
    "# 设置日志\n",
    "class TrainingLogger:\n",
    "    def __init__(self, log_file):\n",
    "        self.log_file = log_file\n",
    "        # 创建独立的logger，避免重复\n",
    "        self.logger = logging.getLogger(f'CD4MT_Training_{datetime.now().strftime(\"%H%M%S\")}')\n",
    "        self.logger.setLevel(logging.INFO)\n",
    "        \n",
    "        # 清除已有的handlers\n",
    "        self.logger.handlers.clear()\n",
    "        \n",
    "        # 文件handler\n",
    "        self.file_handler = logging.FileHandler(log_file, encoding='utf-8')\n",
    "        formatter = logging.Formatter(\n",
    "            '%(asctime)s | %(levelname)s | %(message)s',\n",
    "            datefmt='%Y-%m-%d %H:%M:%S'\n",
    "        )\n",
    "        self.file_handler.setFormatter(formatter)\n",
    "        self.logger.addHandler(self.file_handler)\n",
    "        \n",
    "        # 防止向root logger传播\n",
    "        self.logger.propagate = False\n",
    "    \n",
    "    def info(self, msg):\n",
    "        self.logger.info(msg)\n",
    "        print(f\"INFO: {msg}\")  # 直接打印到控制台\n",
    "        \n",
    "    def error(self, msg):\n",
    "        self.logger.error(msg)\n",
    "        print(f\"ERROR: {msg}\")\n",
    "        \n",
    "    def warning(self, msg):\n",
    "        self.logger.warning(msg)\n",
    "        print(f\"WARNING: {msg}\")\n",
    "\n",
    "# 初始化训练日志\n",
    "train_logger = TrainingLogger(TRAIN_LOG_FILE)\n",
    "train_logger.info(\"🚀 开始 CD4MT 模型训练\")\n",
    "train_logger.info(f\"📄 日志文件: {TRAIN_LOG_FILE}\")\n",
    "train_logger.info(f\"🎯 训练轮数: {TRAIN_EPOCHS}\")\n",
    "train_logger.info(f\"🎵 音轨: {cfg['model']['params']['stem_names']}\")\n",
    "train_logger.info(f\"📊 批大小: {cfg['data']['params']['batch_size']}\")\n",
    "\n",
    "print(f\"📝 训练日志将保存到: {TRAIN_LOG_FILE}\")\n",
    "print(f\"✅ 所有训练变量已正确定义\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练进度监控和损失追踪\n",
    "class TrainingProgressCallback(pl.Callback):\n",
    "    def __init__(self, logger):\n",
    "        self.logger = logger\n",
    "        self.start_time = None\n",
    "        \n",
    "    def on_train_start(self, trainer, pl_module):\n",
    "        self.start_time = datetime.now()\n",
    "        self.logger.info(\"=\" * 60)\n",
    "        self.logger.info(\"🎯 训练开始\")\n",
    "        self.logger.info(f\"   模型参数: {sum(p.numel() for p in pl_module.parameters()):,}\")\n",
    "        self.logger.info(f\"   训练设备: {trainer.strategy.root_device}\")\n",
    "        self.logger.info(f\"   最大epochs: {trainer.max_epochs}\")\n",
    "        self.logger.info(\"=\" * 60)\n",
    "        \n",
    "    def on_train_epoch_start(self, trainer, pl_module):\n",
    "        epoch = trainer.current_epoch + 1\n",
    "        self.logger.info(f\"\\n📅 Epoch {epoch}/{trainer.max_epochs}\")\n",
    "        \n",
    "    def on_train_batch_end(self, trainer, pl_module, outputs, batch, batch_idx):\n",
    "        # 每50个batch记录一次进度\n",
    "        if batch_idx % 50 == 0:\n",
    "            if isinstance(outputs, dict) and 'loss' in outputs:\n",
    "                loss = outputs['loss'].item()\n",
    "            elif hasattr(outputs, 'item'):\n",
    "                loss = outputs.item()\n",
    "            else:\n",
    "                loss = float('nan')\n",
    "            self.logger.info(f\"   Batch {batch_idx}: loss={loss:.6f}\")\n",
    "            \n",
    "    def on_train_epoch_end(self, trainer, pl_module):\n",
    "        epoch = trainer.current_epoch + 1\n",
    "        elapsed = datetime.now() - self.start_time\n",
    "        \n",
    "        # 获取训练损失\n",
    "        train_loss = trainer.callback_metrics.get('train/loss_epoch', float('nan'))\n",
    "        val_loss = trainer.callback_metrics.get('val/loss', float('nan'))\n",
    "        \n",
    "        self.logger.info(f\"✅ Epoch {epoch} 完成\")\n",
    "        self.logger.info(f\"   训练损失: {train_loss:.6f}\")\n",
    "        self.logger.info(f\"   验证损失: {val_loss:.6f}\")\n",
    "        self.logger.info(f\"   累计时间: {elapsed}\")\n",
    "        \n",
    "    def on_train_end(self, trainer, pl_module):\n",
    "        total_time = datetime.now() - self.start_time\n",
    "        self.logger.info(\"=\" * 60)\n",
    "        self.logger.info(\"🎉 训练完成!\")\n",
    "        self.logger.info(f\"   总训练时间: {total_time}\")\n",
    "        self.logger.info(f\"   最终训练损失: {trainer.callback_metrics.get('train/loss_epoch', 'N/A')}\")\n",
    "        self.logger.info(f\"   最终验证损失: {trainer.callback_metrics.get('val/loss', 'N/A')}\")\n",
    "        self.logger.info(\"=\" * 60)\n",
    "        \n",
    "    def on_exception(self, trainer, pl_module, exception):\n",
    "        self.logger.error(f\"❌ 训练异常: {exception}\")\n",
    "\n",
    "class LossTracker:\n",
    "    def __init__(self, log_file):\n",
    "        self.log_file = log_file\n",
    "        self.losses = []\n",
    "        \n",
    "    def add_loss(self, epoch, batch, train_loss, val_loss=None):\n",
    "        self.losses.append({\n",
    "            'epoch': epoch,\n",
    "            'batch': batch,\n",
    "            'train_loss': train_loss,\n",
    "            'val_loss': val_loss,\n",
    "            'timestamp': datetime.now().isoformat()\n",
    "        })\n",
    "        \n",
    "    def save_losses(self):\n",
    "        import json\n",
    "        loss_file = self.log_file.replace('.txt', '_losses.json')\n",
    "        with open(loss_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(self.losses, f, indent=2, ensure_ascii=False)\n",
    "        print(f\"📊 损失数据保存到: {loss_file}\")\n",
    "\n",
    "# 检查必需变量并初始化训练组件\n",
    "try:\n",
    "    if 'TRAIN_LOG_FILE' not in globals():\n",
    "        raise NameError(\"TRAIN_LOG_FILE 未定义\")\n",
    "    if 'train_logger' not in globals():\n",
    "        raise NameError(\"train_logger 未定义\")\n",
    "    \n",
    "    loss_tracker = LossTracker(TRAIN_LOG_FILE)\n",
    "    progress_callback = TrainingProgressCallback(train_logger)\n",
    "    print(\"✅ 训练回调和损失追踪器已初始化\")\n",
    "    \n",
    "except NameError as e:\n",
    "    print(f\"❌ 错误: {e}\")\n",
    "    print(\"请先运行训练初始化单元格！\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 简化的训练准备和执行\n",
    "try:\n",
    "    print(\"🏗️ 创建训练模型...\")\n",
    "    \n",
    "    # 重新创建模型用于训练\n",
    "    with open(CFG_PATH, 'r') as f:\n",
    "        train_cfg = yaml.safe_load(f)\n",
    "    \n",
    "    # 创建模型\n",
    "    train_model_config = train_cfg['model']['params'].copy()\n",
    "    train_unet_config = train_model_config.pop('unet_')\n",
    "    \n",
    "    train_model = ScoreDiffusionModel(\n",
    "        unet_config=train_unet_config,\n",
    "        **train_model_config\n",
    "    )\n",
    "    train_model = train_model.to(device)\n",
    "    \n",
    "    print(f\"✅ 训练模型创建成功\")\n",
    "    print(f\"   模型参数: {sum(p.numel() for p in train_model.parameters()):,}\")\n",
    "    \n",
    "    # 配置训练器 - 修复checkpoint配置\n",
    "    print(\"⚙️ 配置训练器...\")\n",
    "    \n",
    "    # 创建checkpoint目录\n",
    "    checkpoint_dir = \"./training_logs/checkpoints\"\n",
    "    Path(checkpoint_dir).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # 修复checkpoint回调配置\n",
    "    checkpoint_callback = ModelCheckpoint(\n",
    "        dirpath=checkpoint_dir,\n",
    "        filename=\"cd4mt-test-{epoch:02d}-{step:04d}\",\n",
    "        save_top_k=-1,  # 保存所有checkpoint，或者设为1只保存最后一个\n",
    "        save_last=True,  # 保存最后一个\n",
    "        every_n_epochs=1,  # 每个epoch保存一次\n",
    "        verbose=True\n",
    "    )\n",
    "    \n",
    "    # 创建简单的训练器\n",
    "    trainer = pl.Trainer(\n",
    "        accelerator=\"gpu\" if torch.cuda.is_available() else \"cpu\",\n",
    "        devices=1,  # 单GPU\n",
    "        max_epochs=2,  # 减少到2个epoch进行测试\n",
    "        val_check_interval=0.5,  # 每个epoch中间验证一次\n",
    "        limit_train_batches=10,  # 只用10个batch测试\n",
    "        limit_val_batches=5,     # 只用5个batch验证\n",
    "        enable_progress_bar=True,\n",
    "        log_every_n_steps=1,\n",
    "        callbacks=[checkpoint_callback],  # 添加修复后的checkpoint回调\n",
    "        logger=False,  # 禁用复杂logger避免问题\n",
    "        num_sanity_val_steps=0  # 跳过验证检查\n",
    "    )\n",
    "    \n",
    "    print(\"✅ 训练器配置完成\")\n",
    "    print(f\"📁 Checkpoint保存目录: {checkpoint_dir}\")\n",
    "    print(\"🚀 开始快速训练测试...\")\n",
    "    \n",
    "    # 测试训练几个步骤\n",
    "    trainer.fit(train_model, dm)\n",
    "    \n",
    "    print(\"🎉 训练测试完成!\")\n",
    "    \n",
    "    # 显示保存的checkpoint信息\n",
    "    print(f\"\\n📋 Checkpoint信息:\")\n",
    "    print(f\"   保存目录: {checkpoint_dir}\")\n",
    "    if hasattr(checkpoint_callback, 'best_model_path') and checkpoint_callback.best_model_path:\n",
    "        print(f\"   最佳模型: {checkpoint_callback.best_model_path}\")\n",
    "    if hasattr(checkpoint_callback, 'last_model_path') and checkpoint_callback.last_model_path:\n",
    "        print(f\"   最新模型: {checkpoint_callback.last_model_path}\")\n",
    "    \n",
    "    # 列出所有保存的checkpoint文件\n",
    "    import glob\n",
    "    ckpt_files = glob.glob(f\"{checkpoint_dir}/*.ckpt\")\n",
    "    if ckpt_files:\n",
    "        print(f\"   所有checkpoint文件:\")\n",
    "        for ckpt in ckpt_files:\n",
    "            file_size = os.path.getsize(ckpt) / (1024*1024)  # MB\n",
    "            print(f\"     - {ckpt} ({file_size:.1f} MB)\")\n",
    "    else:\n",
    "        print(\"   ⚠️ 未找到checkpoint文件\")\n",
    "    \n",
    "    # 测试训练后的模型生成\n",
    "    print(\"\\n🧪 测试训练后的模型...\")\n",
    "    train_model.eval()\n",
    "    with torch.no_grad():\n",
    "        test_shape = (1, 4, 64, 127)\n",
    "        test_generated = train_model.sample(shape=test_shape, num_steps=5)  # 减少采样步数\n",
    "        print(f\"✅ 生成测试成功: {test_generated.shape}\")\n",
    "        print(f\"   生成音频统计: 均值={test_generated.mean().item():.6f}, 标准差={test_generated.std().item():.6f}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ 训练测试失败: {e}\")\n",
    "    import traceback\n",
    "    print(\"详细错误:\")\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练完成后的分析和总结\n",
    "if 'train_model' in locals():\n",
    "    print(\"📊 训练后分析:\")\n",
    "    print(f\"   模型状态: {'训练模式' if train_model.training else '评估模式'}\")\n",
    "    print(f\"   设备: {next(train_model.parameters()).device}\")\n",
    "    \n",
    "    # 对比训练前后的生成效果\n",
    "    print(\"\\n🔍 对比分析:\")\n",
    "    if 'gen_aud' in locals() and 'test_generated' in locals():\n",
    "        print(f\"   训练前生成: 均值={gen_aud.mean().item():.6f}, 标准差={gen_aud.std().item():.6f}\")\n",
    "        print(f\"   训练后生成: 均值={test_generated.mean().item():.6f}, 标准差={test_generated.std().item():.6f}\")\n",
    "        \n",
    "        # 计算差异\n",
    "        if gen_aud.shape == test_generated.shape:\n",
    "            diff = torch.mean(torch.abs(gen_aud - test_generated)).item()\n",
    "            print(f\"   生成差异: {diff:.6f}\")\n",
    "            if diff > 0.01:\n",
    "                print(\"   ✅ 模型已发生变化，训练有效\")\n",
    "            else:\n",
    "                print(\"   ⚠️  生成结果相近，可能需要更多训练\")\n",
    "    \n",
    "    print(f\"\\n🎉 CD4MT训练测试完成!\")\n",
    "    print(f\"   ✅ 模型可以正常训练\")\n",
    "    print(f\"   ✅ 生成功能正常\")\n",
    "    print(f\"   💡 建议: 使用更多epochs和完整数据进行正式训练\")\n",
    "    \n",
    "else:\n",
    "    print(\"❌ 训练模型不存在，请先运行训练单元格\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 检查 Checkpoint 文件\n",
    "import glob\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# 检查可能的checkpoint保存位置\n",
    "possible_dirs = [\n",
    "    \"./training_logs/checkpoints\",\n",
    "    \"./lightning_logs\",\n",
    "    \"./checkpoints\",\n",
    "    \".\"\n",
    "]\n",
    "\n",
    "print(\"🔍 搜索 Checkpoint 文件...\")\n",
    "found_checkpoints = []\n",
    "\n",
    "for check_dir in possible_dirs:\n",
    "    if os.path.exists(check_dir):\n",
    "        ckpt_files = glob.glob(f\"{check_dir}/**/*.ckpt\", recursive=True)\n",
    "        if ckpt_files:\n",
    "            print(f\"\\n📁 在 {check_dir} 发现 checkpoint:\")\n",
    "            for ckpt in ckpt_files:\n",
    "                file_size = os.path.getsize(ckpt) / (1024*1024)  # MB\n",
    "                mod_time = os.path.getmtime(ckpt)\n",
    "                from datetime import datetime\n",
    "                mod_time_str = datetime.fromtimestamp(mod_time).strftime('%Y-%m-%d %H:%M:%S')\n",
    "                print(f\"   - {ckpt}\")\n",
    "                print(f\"     大小: {file_size:.1f} MB\")\n",
    "                print(f\"     修改时间: {mod_time_str}\")\n",
    "                found_checkpoints.append(ckpt)\n",
    "\n",
    "if not found_checkpoints:\n",
    "    print(\"\\n❌ 未找到任何 checkpoint 文件\")\n",
    "    print(\"💡 提示: 运行训练单元格后会在以下位置保存:\")\n",
    "    print(\"   ./training_logs/checkpoints/\")\n",
    "else:\n",
    "    print(f\"\\n✅ 总共找到 {len(found_checkpoints)} 个 checkpoint 文件\")\n",
    "    \n",
    "    # 显示最新的checkpoint\n",
    "    if found_checkpoints:\n",
    "        latest_ckpt = max(found_checkpoints, key=os.path.getmtime)\n",
    "        print(f\"🕒 最新的 checkpoint: {latest_ckpt}\")\n",
    "\n",
    "# 检查训练日志目录\n",
    "log_dir = \"./training_logs\"\n",
    "if os.path.exists(log_dir):\n",
    "    log_files = glob.glob(f\"{log_dir}/*.txt\")\n",
    "    if log_files:\n",
    "        print(f\"\\n📝 训练日志文件:\")\n",
    "        for log_file in log_files[-3:]:  # 显示最近3个\n",
    "            print(f\"   - {log_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "i6ofdp42qpd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 修复环境问题\n",
    "import os\n",
    "\n",
    "# 禁用分布式训练相关的MPI检查\n",
    "os.environ['RANK'] = '0'\n",
    "os.environ['WORLD_SIZE'] = '1'\n",
    "os.environ['MASTER_ADDR'] = 'localhost'\n",
    "os.environ['MASTER_PORT'] = '12355'\n",
    "\n",
    "# 避免 MPI 相关错误\n",
    "os.environ['PL_TORCH_DISTRIBUTED_BACKEND'] = 'nccl'\n",
    "os.environ['OMPI_COMM_WORLD_RANK'] = '0'\n",
    "os.environ['OMPI_COMM_WORLD_SIZE'] = '1'\n",
    "\n",
    "# 设置CUDA相关\n",
    "if torch.cuda.is_available():\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "\n",
    "print(\"🔧 环境变量已设置，避免MPI相关错误\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2qpxaz434bx",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rfhn3tn98v",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (cdp10)",
   "language": "python",
   "name": "cdp10"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
