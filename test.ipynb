{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CD4MT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# old ver: from flash_attn.flash_attention import FlashAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected CUDA:3 (free ~23.2 GB)\n"
     ]
    }
   ],
   "source": [
    "# Auto-select CUDA device with max free memory\n",
    "import os, torch\n",
    "\n",
    "def auto_select_cuda(min_free_gb=4.0):\n",
    "    if not torch.cuda.is_available():\n",
    "        print(\"CUDA not available -> cpu\")\n",
    "        return torch.device(\"cpu\")\n",
    "    best, best_free = None, -1.0\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        free_b, total_b = torch.cuda.mem_get_info(i)\n",
    "        free_gb = free_b / (1024**3)\n",
    "        if free_gb > best_free and free_gb >= min_free_gb:\n",
    "            best, best_free = i, free_gb\n",
    "    if best is None:\n",
    "        for i in range(torch.cuda.device_count()):\n",
    "            free_b, _ = torch.cuda.mem_get_info(i)\n",
    "            free_gb = free_b / (1024**3)\n",
    "            if free_gb > best_free:\n",
    "                best, best_free = i, free_gb\n",
    "    print(f\"Selected CUDA:{best} (free ~{best_free:.1f} GB)\")\n",
    "    return torch.device(f\"cuda:{best}\")\n",
    "\n",
    "device = auto_select_cuda(min_free_gb=4.0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env: cdp10\n",
    "# Python 3.10.18\n",
    "# torch 2.8.0\n",
    "# cu12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working directory: /data1/yuchen/cd4mt\n",
      "Device: cuda:3\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import os, sys, yaml, torch, numpy as np, matplotlib.pyplot as plt\n",
    "import soundfile as sf\n",
    "from pathlib import Path\n",
    "from IPython.display import Audio, display, HTML, Markdown\n",
    "import matplotlib.font_manager as fm\n",
    "\n",
    "ROOT = \"/data1/yuchen/cd4mt\"\n",
    "sys.path.append(\"/data1/yuchen/MusicLDM-Ext/src\")\n",
    "sys.path.append(ROOT)\n",
    "sys.path.append(f\"{ROOT}/ldm\")\n",
    "os.chdir(ROOT)\n",
    "\n",
    "print(f\"Working directory: {os.getcwd()}\")\n",
    "from src.music2latent.music2latent import EncoderDecoder\n",
    "\n",
    "from ldm.data.multitrack_datamodule import DataModuleFromConfig\n",
    "device = device  # unified\n",
    "print(f\"Device: {device}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. config and load_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "CFG_PATH = \"configs/cd4mt_medium.yaml\"\n",
    "\n",
    "with open(CFG_PATH, 'r') as f:\n",
    "    cfg = yaml.safe_load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ct_hparams loaded from /data1/yuchen/cd4mt/configs/cd4mt_medium.yaml\n",
      "ct_hparams: {'batch_size': 6, 'lr': 0.0001, 'ema_decay': 0.95, 'num_scales': 32, 'sigma_min': 0.002, 'sigma_max': 80.0, 'sigma_data': 0.5, 'epochs': 120, 'grad_clip': 1.0, 'log_interval': 20} | types: lr float batch_size int sigma_data float\n"
     ]
    }
   ],
   "source": [
    "# Load ct_config and ct_hparams from medium YAML (for CT) with robust typing\n",
    "import yaml, torch, numpy as np\n",
    "HCONF_PATH = '/data1/yuchen/cd4mt/configs/cd4mt_medium.yaml'\n",
    "with open(HCONF_PATH, 'r') as _f:\n",
    "    ct_config = yaml.safe_load(_f)\n",
    "\n",
    "def _get(d, path, default=None):\n",
    "    cur = d\n",
    "    for k in path.split('.'):\n",
    "        if not isinstance(cur, dict) or k not in cur:\n",
    "            return default\n",
    "        cur = cur[k]\n",
    "    return cur\n",
    "\n",
    "# Robust type casting (YAML可能把科学计数值读成字符串)\n",
    "_lr_raw = _get(ct_config, 'model.params.base_learning_rate', 5e-5)\n",
    "try:\n",
    "    lr = float(_lr_raw)\n",
    "except Exception:\n",
    "    print('[ct_hparams] WARN: invalid lr, fallback to 5e-5; raw=', _lr_raw)\n",
    "    lr = 5e-5\n",
    "\n",
    "_bs_raw = _get(ct_config, 'data.params.batch_size', 4)\n",
    "try:\n",
    "    batch_size = int(_bs_raw)\n",
    "except Exception:\n",
    "    print('[ct_hparams] WARN: invalid batch_size, fallback to 4; raw=', _bs_raw)\n",
    "    batch_size = 4\n",
    "\n",
    "_sigma_data_raw = _get(ct_config, 'model.params.diffusion_sigma_data', 0.5)\n",
    "try:\n",
    "    sigma_data = float(_sigma_data_raw)\n",
    "except Exception:\n",
    "    print('[ct_hparams] WARN: invalid sigma_data, fallback to 0.5; raw=', _sigma_data_raw)\n",
    "    sigma_data = 0.5\n",
    "\n",
    "ct_hparams = {\n",
    "    'batch_size': batch_size,\n",
    "    'lr': lr,\n",
    "    'ema_decay': 0.95,\n",
    "    'num_scales': 32,\n",
    "    'sigma_min': 0.002,\n",
    "    'sigma_max': 80.0,\n",
    "    'sigma_data': sigma_data,\n",
    "    'epochs': 120,\n",
    "    'grad_clip': 1.0,\n",
    "    'log_interval': 20,\n",
    "}\n",
    "print('ct_hparams loaded from', HCONF_PATH)\n",
    "print('ct_hparams:', ct_hparams, '| types: lr', type(lr).__name__, 'batch_size', type(batch_size).__name__, 'sigma_data', type(sigma_data).__name__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1290 tracks.\n",
      "sr=44100, min: 10, max: 600\n",
      "Keeping 1289 of 1290 tracks\n",
      "Data size: 26309\n",
      "Use mixup rate of 0.0; Use SpecAug (T,F) of (0, 0); Use blurring effect or not False\n",
      "| Audiostock Dataset Length:26309 | Epoch Length: 26309\n",
      "Found 271 tracks.\n",
      "sr=44100, min: 10, max: 600\n",
      "Keeping 270 of 271 tracks\n",
      "Data size: 5422\n",
      "Use mixup rate of 0.0; Use SpecAug (T,F) of (0, 0); Use blurring effect or not False\n",
      "| Audiostock Dataset Length:5422 | Epoch Length: 5422\n",
      "Found 152 tracks.\n",
      "sr=44100, min: 10, max: 600\n",
      "Keeping 151 of 152 tracks\n",
      "Data size: 3249\n",
      "Use mixup rate of 0.0; Use SpecAug (T,F) of (0, 0); Use blurring effect or not False\n",
      "| Audiostock Dataset Length:3249 | Epoch Length: 3249\n",
      "train_loader: 4385\n",
      "batch.keys(): ['fname', 'fbank_stems', 'waveform_stems', 'waveform', 'fbank']\n",
      "wav_stems: torch.Size([6, 4, 524288])\n",
      "wav_mix: torch.Size([6, 524288])\n",
      "Batch=6, Stems=4, Time=524288\n"
     ]
    }
   ],
   "source": [
    "dm = DataModuleFromConfig(**cfg[\"data\"][\"params\"])\n",
    "dm.prepare_data()\n",
    "dm.setup(stage=\"fit\")\n",
    "\n",
    "train_loader = dm.train_dataloader()\n",
    "print(f\"train_loader: {len(train_loader)}\")\n",
    "batch = next(iter(train_loader))\n",
    "print(f\"batch.keys(): {list(batch.keys())}\")\n",
    "\n",
    "wav_stems = batch[\"waveform_stems\"]  # (B, S, T)\n",
    "wav_mix = batch.get(\"waveform\", None)  # (B, T)\n",
    "\n",
    "print(f\"wav_stems: {wav_stems.shape}\")\n",
    "if wav_mix is not None:\n",
    "    print(f\"wav_mix: {wav_mix.shape}\")\n",
    "\n",
    "B, S, T = wav_stems.shape\n",
    "print(f\"Batch={B}, Stems={S}, Time={T}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. CAE test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data1/yuchen/cd4mt/src/music2latent/music2latent\n",
      "\n",
      " stem_num 4 \n",
      "\n",
      " bass stem 0:\n",
      "stem_audio : (6, 524288)\n",
      "stem_latents.shape torch.Size([6, 64, 127]), tem_latents.dtype torch.float16, range: [-4.656, 4.070]\n",
      "\n",
      " drums stem 1:\n",
      "stem_audio : (6, 524288)\n",
      "stem_latents.shape torch.Size([6, 64, 127]), tem_latents.dtype torch.float16, range: [-4.254, 3.434]\n",
      "\n",
      " guitar stem 2:\n",
      "stem_audio : (6, 524288)\n",
      "stem_latents.shape torch.Size([6, 64, 127]), tem_latents.dtype torch.float16, range: [-4.598, 3.605]\n",
      "\n",
      " piano stem 3:\n",
      "stem_audio : (6, 524288)\n",
      "stem_latents.shape torch.Size([6, 64, 127]), tem_latents.dtype torch.float16, range: [-5.102, 4.488]\n",
      "latents_stacked.shape: torch.Size([6, 4, 64, 127])\n",
      " Batch=6, Stems=4, Channels=64, Length=127\n",
      " latents on : cuda:3\n"
     ]
    }
   ],
   "source": [
    "ae = EncoderDecoder(device=device)\n",
    "print(f\"\\n stem_num {S} \")\n",
    "\n",
    "stem_names = cfg['model']['params']['stem_names']\n",
    "latents_list = []\n",
    "encode_shapes = []\n",
    "\n",
    "for s in range(S):\n",
    "    stem_name = stem_names[s] if s < len(stem_names) else f\"stem_{s}\"\n",
    "    print(f\"\\n {stem_name} stem {s}:\")\n",
    "    stem_audio = wav_stems[:, s].cpu().numpy()  \n",
    "    print(f\"stem_audio : {stem_audio.shape}\")\n",
    "\n",
    "    stem_latents = ae.encode(stem_audio)\n",
    "    if isinstance(stem_latents, np.ndarray):\n",
    "        stem_latents = torch.from_numpy(stem_latents)\n",
    "    \n",
    "    print(f\"stem_latents.shape {stem_latents.shape}, tem_latents.dtype {stem_latents.dtype}, range: [{stem_latents.min():.3f}, {stem_latents.max():.3f}]\")\n",
    "    latents_list.append(stem_latents)\n",
    "    encode_shapes.append(stem_latents.shape)\n",
    "\n",
    "\n",
    "latents_stacked = torch.stack(latents_list, dim=1)  # (B, S, C, L)\n",
    "print(f\"latents_stacked.shape: {latents_stacked.shape}\")\n",
    "print(f\" Batch={latents_stacked.shape[0]}, Stems={latents_stacked.shape[1]}, Channels={latents_stacked.shape[2]}, Length={latents_stacked.shape[3]}\")\n",
    "latents = latents_stacked.to(device)\n",
    "print(f\" latents on : {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Decode bass\n",
      "recst.shape: torch.Size([6, 521728])\n",
      "range: [-0.262, 0.248]\n",
      "\n",
      "Decode drums\n",
      "recst.shape: torch.Size([6, 521728])\n",
      "range: [-0.529, 0.462]\n",
      "\n",
      "Decode guitar\n",
      "recst.shape: torch.Size([6, 521728])\n",
      "range: [-0.510, 0.624]\n",
      "\n",
      "Decode piano\n",
      "recst.shape: torch.Size([6, 521728])\n",
      "range: [-0.340, 0.366]\n",
      "\n",
      "recst shape: (6, 4, 524288)\n",
      "original lenght: 524288, recst length: 524288\n",
      "MSE: 0.002353\n"
     ]
    }
   ],
   "source": [
    "recst_list = []\n",
    "for s in range(S):\n",
    "    stem_name = stem_names[s] if s < len(stem_names) else f\"stem_{s}\"\n",
    "    print(f\"\\nDecode {stem_name}\")\n",
    "    stem_latents = latents[:, s].cpu().numpy()  # (B, C, L)\n",
    "    \n",
    "    try:\n",
    "        recst = ae.decode(stem_latents)\n",
    "        print(f\"recst.shape: {recst.shape}\")\n",
    "        print(f\"range: [{recst.min():.3f}, {recst.max():.3f}]\")\n",
    "\n",
    "        if isinstance(recst, torch.Tensor):\n",
    "            recst = recst.cpu().numpy() \n",
    "        current_length = recst.shape[-1]\n",
    "        if current_length > T:\n",
    "            excess = current_length - T\n",
    "            start_trim = excess // 2\n",
    "            end_trim = excess - start_trim\n",
    "            recst = recst[..., start_trim:current_length-end_trim]\n",
    "            \n",
    "        elif current_length < T:\n",
    "            deficit = T - current_length\n",
    "            pad_left = deficit // 2\n",
    "            pad_right = deficit - pad_left\n",
    "            recst = np.pad(recst, ((0,0), (pad_left, pad_right)), mode='constant', constant_values=0)\n",
    "        \n",
    "        recst_list.append(recst)\n",
    "        \n",
    "    except Exception as e:\n",
    "        raise e\n",
    "\n",
    "recst_aud = np.stack(recst_list, axis=1)  # (B, S, T')\n",
    "recst_tensor = torch.from_numpy(recst_aud).to(device)\n",
    "\n",
    "print(f\"\\nrecst shape: {recst_aud.shape}\")\n",
    "print(f\"original lenght: {T}, recst length: {recst_aud.shape[2]}\")\n",
    "\n",
    "if recst_aud.shape[2] == T:\n",
    "    mse_error = np.mean((wav_stems.cpu().numpy() - recst_aud)**2)\n",
    "    print(f\"MSE: {mse_error:.6f}\")\n",
    "else:\n",
    "    print(\"length error\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. CD4MT 扩散模型初始化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install flash_attn\n",
    "# !pip install piq\n",
    "# !pip install blobfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CT] diffusion.loss_norm = l2\n",
      "[CT] model.convert_to_fp16() done\n",
      "CM UNet params: {'sigma_min': 0.0001, 'sigma_max': 3.0, 'image_size': 32, 'num_channels': 192, 'num_res_blocks': 2, 'num_heads': 6, 'num_heads_upsample': -1, 'num_head_channels': 32, 'attention_resolutions': '16,8,4', 'channel_mult': '1,2,4', 'dropout': 0.1, 'class_cond': False, 'use_checkpoint': False, 'use_scale_shift_norm': True, 'resblock_updown': False, 'use_fp16': False, 'use_new_attention_order': False, 'learn_sigma': False, 'weight_schedule': 'karras'}\n"
     ]
    }
   ],
   "source": [
    "from src.cm.script_util import create_model_and_diffusion, model_and_diffusion_defaults\n",
    "import torch.nn as nn\n",
    "\n",
    "with open(CFG_PATH, \"r\") as f:\n",
    "    cfg_fresh = yaml.safe_load(f)\n",
    "\n",
    "cm_model_params = model_and_diffusion_defaults()\n",
    "\n",
    "# Merge override from YAML if present, else derive from UNet config\n",
    "ovr = (cfg_fresh.get('model',{}).get('params',{}).get('cm_model_override'))\n",
    "if not ovr:\n",
    "    u = cfg_fresh['model']['params']['unet_']['params']\n",
    "    def _to_str_list(v):\n",
    "        if isinstance(v,str): return v\n",
    "        if isinstance(v,(list,tuple)): return ','.join(str(int(x)) for x in v)\n",
    "        return str(v)\n",
    "    ovr = {\n",
    "        'image_size': u.get('image_size', cm_model_params['image_size']),\n",
    "        'num_channels': u.get('model_channels', cm_model_params['num_channels']),\n",
    "        'num_res_blocks': u.get('num_res_blocks', cm_model_params['num_res_blocks']),\n",
    "        'channel_mult': _to_str_list(u.get('channel_mult','1,2,4')),\n",
    "        'num_heads': u.get('num_heads', cm_model_params['num_heads']),\n",
    "        'num_head_channels': u.get('num_head_channels', 32),\n",
    "        'num_heads_upsample': -1,\n",
    "        'attention_resolutions': _to_str_list(u.get('attention_resolutions',[8,4,2])),\n",
    "        'dropout': u.get('dropout', 0.1),\n",
    "        'class_cond': False,\n",
    "        'use_checkpoint': False,\n",
    "        'use_scale_shift_norm': True,\n",
    "        'resblock_updown': False,\n",
    "        'use_fp16': False,\n",
    "        'use_new_attention_order': False,\n",
    "        'learn_sigma': False,\n",
    "        'weight_schedule': 'karras',\n",
    "        'sigma_min': cfg_fresh['model']['params'].get('sigma_min', cm_model_params['sigma_min']),\n",
    "        'sigma_max': cfg_fresh['model']['params'].get('sigma_max', cm_model_params['sigma_max']),\n",
    "    }\n",
    "else:\n",
    "    ovr = dict(ovr)\n",
    "# Enforce flash-attn constraint\n",
    "ovr['num_head_channels'] = int(ovr.get('num_head_channels',32))\n",
    "if ovr['num_head_channels'] not in (16,32,64):\n",
    "    ovr['num_head_channels'] = 32\n",
    "# Normalize string fields\n",
    "for k in ['channel_mult','attention_resolutions']:\n",
    "    if k in ovr and not isinstance(ovr[k], str):\n",
    "        if isinstance(ovr[k], (list,tuple)):\n",
    "            ovr[k] = ','.join(str(int(x)) for x in ovr[k])\n",
    "        else:\n",
    "            ovr[k] = str(ovr[k])\n",
    "cm_model_params.update(ovr)\n",
    "\n",
    "# Sanity check divisibility per level\n",
    "_ch_mult = [int(x) for x in cm_model_params['channel_mult'].split(',')]\n",
    "assert all((cm_model_params['num_channels']*m) % cm_model_params['num_head_channels'] == 0 for m in _ch_mult),     f\"num_channels*channel_mult must be divisible by num_head_channels; got num_channels={cm_model_params['num_channels']}, channel_mult={_ch_mult}, num_head_channels={cm_model_params['num_head_channels']}\"\n",
    "\n",
    "# Build model and diffusion\n",
    "in_ch = cfg_fresh['model']['params']['unet_']['params']['in_channels']\n",
    "out_ch = cfg_fresh['model']['params']['unet_']['params']['out_channels']\n",
    "sigma_data = cfg_fresh['model']['params'].get('diffusion_sigma_data', 0.5)\n",
    "model, diffusion = create_model_and_diffusion(distillation=False, **cm_model_params)\n",
    "diffusion.sigma_data = sigma_data\n",
    "\n",
    "# Speed-ups: loss=L2, convert model to fp16 torso\n",
    "try:\n",
    "    diffusion.loss_norm = 'l2'\n",
    "    print('[CT] diffusion.loss_norm = l2')\n",
    "except Exception as e:\n",
    "    print('[CT] set loss_norm failed:', e)\n",
    "try:\n",
    "    if hasattr(model,'convert_to_fp16'): model.convert_to_fp16()\n",
    "    print('[CT] model.convert_to_fp16() done')\n",
    "except Exception as e:\n",
    "    print('[CT] convert_to_fp16 failed:', e)\n",
    "\n",
    "# Patch I/O convs to match diffusion channels\n",
    "new_in = nn.Conv2d(in_ch, model.input_blocks[0][0].out_channels,\n",
    "                   kernel_size=model.input_blocks[0][0].kernel_size,\n",
    "                   stride=model.input_blocks[0][0].stride,\n",
    "                   padding=model.input_blocks[0][0].padding,\n",
    "                   bias=model.input_blocks[0][0].bias is not None)\n",
    "nn.init.kaiming_normal_(new_in.weight, mode='fan_out', nonlinearity='relu')\n",
    "if new_in.bias is not None: nn.init.zeros_(new_in.bias)\n",
    "model.input_blocks[0][0] = new_in\n",
    "\n",
    "new_out = nn.Conv2d(model.out[-1].in_channels, out_ch,\n",
    "                    kernel_size=model.out[-1].kernel_size,\n",
    "                    stride=model.out[-1].stride,\n",
    "                    padding=model.out[-1].padding,\n",
    "                    bias=model.out[-1].bias is not None)\n",
    "nn.init.zeros_(new_out.weight)\n",
    "if new_out.bias is not None: nn.init.zeros_(new_out.bias)\n",
    "model.out[-1] = new_out\n",
    "\n",
    "model = model.to(device).eval()\n",
    "print('CM UNet params:', cm_model_params)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Consistency Training (CAE → Karras)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import copy\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "import swanlab\n",
    "\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "def reshape_latents(latent_tensor: torch.Tensor):\n",
    "    B, S, C, L = latent_tensor.shape\n",
    "    flat = latent_tensor.view(B, S * C, L)\n",
    "    side = int(math.sqrt(L))\n",
    "    if side * side < L:\n",
    "        side += 1\n",
    "    pad = side * side - L\n",
    "    if pad > 0:\n",
    "        flat = F.pad(flat, (0, pad))\n",
    "    imgs = flat.view(B, S * C, side, side)\n",
    "    return imgs, {'side': side, 'pad': pad, 'latent_len': L}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper: encode a dataloader batch of waveforms to 2D imgs for CM (no interface mismatch)\n",
    "import math\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def encode_batch_to_imgs(ae, batch, to_float32=True):\n",
    "    # batch['waveform_stems']: (B, S, T)\n",
    "    wav_stems = batch['waveform_stems']\n",
    "    if isinstance(wav_stems, np.ndarray):\n",
    "        wav_stems = torch.from_numpy(wav_stems)\n",
    "    if wav_stems.dim() != 3:\n",
    "        raise ValueError(f\"expected wav_stems of shape (B,S,T); got {tuple(wav_stems.shape)}\")\n",
    "\n",
    "    B, S, T = wav_stems.shape\n",
    "    latents_list = []\n",
    "    for s in range(S):\n",
    "        # (B, T) -> CAE encode 将 B 视作 audio_channels，输出 (B, C, L)\n",
    "        stem_audio = wav_stems[:, s].cpu().numpy()\n",
    "        stem_lat = ae.encode(stem_audio)  # (B, C, L)\n",
    "        if isinstance(stem_lat, np.ndarray):\n",
    "            stem_lat = torch.from_numpy(stem_lat)\n",
    "        if to_float32:\n",
    "            stem_lat = stem_lat.to(torch.float32)\n",
    "        latents_list.append(stem_lat)\n",
    "\n",
    "    # 堆叠成 (B, S, C, L)\n",
    "    latents = torch.stack(latents_list, dim=1).contiguous()\n",
    "\n",
    "    # reshape: (B, S*C, H, W)，H=W=ceil(sqrt(L))\n",
    "    B, S, C, L = latents.shape\n",
    "    flat = latents.view(B, S * C, L)\n",
    "    side = int(math.sqrt(L))\n",
    "    if side * side < L:\n",
    "        side += 1\n",
    "    pad = side * side - L\n",
    "    if pad > 0:\n",
    "        flat = F.pad(flat, (0, pad))\n",
    "    imgs = flat.view(B, S * C, side, side)\n",
    "    return imgs, {\"side\": side, \"pad\": pad, \"latent_len\": L}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latent reshape -> torch.Size([6, 256, 12, 12]), meta={'side': 12, 'pad': 17, 'latent_len': 127}\n"
     ]
    }
   ],
   "source": [
    "from src.cm.script_util import create_model\n",
    "from src.cm.karras_diffusion import KarrasDenoiser\n",
    "from src.cm.nn import update_ema\n",
    "\n",
    "latents_cpu = latents.detach().to(torch.float32).cpu()\n",
    "imgs_sample, reshape_meta = reshape_latents(latents_cpu)\n",
    "print(f'Latent reshape -> {imgs_sample.shape}, meta={reshape_meta}')\n",
    "\n",
    "# steps_per_epoch will use dm.train_dataloader() later in training cell\n",
    "\n",
    "model = create_model(\n",
    "    image_size=imgs_sample.shape[-1],\n",
    "    num_channels=64,\n",
    "    num_res_blocks=1,\n",
    "    channel_mult='1',\n",
    "    learn_sigma=False,\n",
    "    class_cond=False,\n",
    "    use_checkpoint=False,\n",
    "    attention_resolutions='1024',\n",
    "    num_heads=1,\n",
    "    num_head_channels=-1,\n",
    "    num_heads_upsample=-1,\n",
    "    use_scale_shift_norm=True,\n",
    "    dropout=0.05,\n",
    "    resblock_updown=False,\n",
    "    use_fp16=False,\n",
    "    use_new_attention_order=False,\n",
    ")\n",
    "\n",
    "in_channels = imgs_sample.shape[1]\n",
    "model.input_blocks[0][0] = torch.nn.Conv2d(\n",
    "    in_channels,\n",
    "    model.input_blocks[0][0].out_channels,\n",
    "    kernel_size=model.input_blocks[0][0].kernel_size,\n",
    "    stride=model.input_blocks[0][0].stride,\n",
    "    padding=model.input_blocks[0][0].padding,\n",
    "    bias=model.input_blocks[0][0].bias is not None,\n",
    ")\n",
    "model.out[-1] = torch.nn.Conv2d(\n",
    "    model.out[-1].in_channels,\n",
    "    in_channels,\n",
    "    kernel_size=model.out[-1].kernel_size,\n",
    "    stride=model.out[-1].stride,\n",
    "    padding=model.out[-1].padding,\n",
    "    bias=model.out[-1].bias is not None,\n",
    ")\n",
    "\n",
    "diffusion = KarrasDenoiser(\n",
    "    sigma_data=ct_hparams['sigma_data'],\n",
    "    sigma_min=ct_hparams['sigma_min'],\n",
    "    sigma_max=ct_hparams['sigma_max'],\n",
    "    weight_schedule='karras',\n",
    "    loss_norm='l2',\n",
    ")\n",
    "device = device  # unified\n",
    "model.to(device)\n",
    "\n",
    "target_model = copy.deepcopy(model)\n",
    "for p in target_model.parameters():\n",
    "    p.requires_grad_(False)\n",
    "target_model.to(device)\n",
    "target_model.eval()\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=ct_hparams['lr'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity imgs: torch.Size([6, 256, 12, 12]) {'side': 12, 'pad': 17, 'latent_len': 127}\n",
      "Sanity check OK. loss=0.173927\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_462788/335833977.py:10: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  _tmp_scaler = GradScaler()\n",
      "/tmp/ipykernel_462788/335833977.py:13: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=torch.float16):\n"
     ]
    }
   ],
   "source": [
    "# Sanity check: single batch forward/backward to validate pipeline\n",
    "batch0 = next(iter(dm.train_dataloader()))\n",
    "assert 'waveform_stems' in batch0\n",
    "imgs0, meta0 = encode_batch_to_imgs(ae, batch0)\n",
    "print('Sanity imgs:', imgs0.shape, meta0)\n",
    "imgs0 = imgs0.to(device)\n",
    "\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "_tmp_opt = torch.optim.Adam(model.parameters(), lr=float(ct_hparams['lr']))\n",
    "_tmp_scaler = GradScaler()\n",
    "\n",
    "model.train()\n",
    "with autocast(dtype=torch.float16):\n",
    "    _outs = diffusion.consistency_losses(\n",
    "        model,\n",
    "        imgs0.half(),\n",
    "        num_scales=ct_hparams['num_scales'],\n",
    "        target_model=target_model,\n",
    "        teacher_model=None,\n",
    "        teacher_diffusion=None,\n",
    "    )\n",
    "    _loss = _outs['loss'].mean()\n",
    "\n",
    "_tmp_opt.zero_grad(); _tmp_scaler.scale(_loss).backward(); _tmp_scaler.unscale_(_tmp_opt)\n",
    "import torch as _th\n",
    "_th.nn.utils.clip_grad_norm_(model.parameters(), ct_hparams['grad_clip'])\n",
    "_tmp_scaler.step(_tmp_opt); _tmp_scaler.update()\n",
    "print(f\"Sanity check OK. loss={float(_loss):.6f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # CM training loop (epoch-level avg; deterministic val; best EMA ckpt)\n",
    "# import torch, os, copy, numpy as np\n",
    "# from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "# ae = EncoderDecoder(device=device)\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=float(ct_hparams['lr']))\n",
    "# scaler = GradScaler()\n",
    "\n",
    "# from src.cm.nn import update_ema\n",
    "# # persistent EMA target model\n",
    "# target_model = copy.deepcopy(model).to(device)\n",
    "# for p in target_model.parameters():\n",
    "#     p.requires_grad_(False)\n",
    "# target_model.eval()\n",
    "\n",
    "# best, best_path = float('inf'), None\n",
    "# os.makedirs('checkpoints', exist_ok=True)\n",
    "# import time, swanlab\n",
    "# run = swanlab.init(project=\"cd4mt\", experiment_name=f\"ct_unet_{time.strftime('%m%d_%H%M%S')}\", config={**ct_hparams, \"num_params\": sum(p.numel() for p in model.parameters())})\n",
    "# _total_step = 0\n",
    "\n",
    "\n",
    "# for epoch in range(ct_hparams['epochs']):\n",
    "#     # Train\n",
    "#     model.train();\n",
    "#     train_loader = dm.train_dataloader()\n",
    "#     run, steps = 0.0, 0\n",
    "#     for step, batch in enumerate(train_loader):\n",
    "#         imgs, meta = encode_batch_to_imgs(ae, batch)\n",
    "#         imgs = imgs.to(device)\n",
    "#         with autocast(dtype=torch.float16):\n",
    "#             losses = diffusion.consistency_losses(\n",
    "#                 model,\n",
    "#                 imgs.half(),\n",
    "#                 num_scales=ct_hparams['num_scales'],\n",
    "#                 target_model=target_model,\n",
    "#                 teacher_model=None,\n",
    "#                 teacher_diffusion=None,\n",
    "#             )\n",
    "#             loss = losses['loss'].mean()\n",
    "#         optimizer.zero_grad()\n",
    "#         scaler.scale(loss).backward()\n",
    "#         scaler.unscale_(optimizer)\n",
    "#         torch.nn.utils.clip_grad_norm_(model.parameters(), ct_hparams['grad_clip'])\n",
    "#         scaler.step(optimizer)\n",
    "#         scaler.update()\n",
    "#         update_ema(target_model.parameters(), model.parameters(), rate=ct_hparams['ema_decay'])\n",
    "#         run += float(loss.item()); steps += 1\n",
    "#         _total_step += 1\n",
    "#         if step % ct_hparams['log_interval'] == 0:\n",
    "#             swanlab.log({\"train/loss\": float(loss.item()), \"epoch\": epoch, \"step\": _total_step})\n",
    "#             print(f\"[train] epoch={epoch} step={step}/{len(train_loader)} loss={loss.item():.4f}\")\n",
    "#     avg_train = run / max(1, steps)\n",
    "#     swanlab.log({\"train/avg\": float(avg_train), \"epoch\": epoch})\n",
    "#     print(f\"[train] epoch={epoch} avg={avg_train:.4f}\")\n",
    "\n",
    "#     # Val (deterministic)\n",
    "#     model.eval();\n",
    "#     val_loader = dm.val_dataloader()\n",
    "#     vrun, vsteps = 0.0, 0\n",
    "#     # save/restore RNG\n",
    "#     _cpu = torch.get_rng_state(); _cuda = torch.cuda.get_rng_state_all() if torch.cuda.is_available() else None\n",
    "#     torch.manual_seed(12345); np.random.seed(12345)\n",
    "#     if torch.cuda.is_available(): torch.cuda.manual_seed_all(12345)\n",
    "#     with torch.no_grad():\n",
    "#         for vstep, vbatch in enumerate(val_loader):\n",
    "#             vimgs, vmeta = encode_batch_to_imgs(ae, vbatch)\n",
    "#             vimgs = vimgs.to(device)\n",
    "#             gen = torch.Generator(device=vimgs.device); gen.manual_seed(12345 + vstep)\n",
    "#             vnoise = torch.randn_like(vimgs, generator=gen)\n",
    "#             with autocast(dtype=torch.float16):\n",
    "#                 vloss = diffusion.consistency_losses(\n",
    "#                     model,\n",
    "#                     vimgs.half(),\n",
    "#                     num_scales=ct_hparams['num_scales'],\n",
    "#                     target_model=target_model,\n",
    "#                     teacher_model=None,\n",
    "#                     teacher_diffusion=None,\n",
    "#                     noise=vnoise,\n",
    "#                 )['loss'].mean()\n",
    "#             vrun += float(vloss.item()); vsteps += 1\n",
    "#     # restore RNG\n",
    "#     torch.set_rng_state(_cpu); \n",
    "#     if _cuda is not None: torch.cuda.set_rng_state_all(_cuda)\n",
    "#     avg_val = vrun / max(1, vsteps)\n",
    "#     swanlab.log({\"val/avg\": float(avg_val), \"epoch\": epoch})\n",
    "#     print(f\"[val]  epoch={epoch} avg={avg_val:.4f}\")\n",
    "\n",
    "#     # Save best by val\n",
    "#     if avg_val < best:\n",
    "#         best = avg_val\n",
    "#         best_path = f\"checkpoints/ct_unet_ema_best_val{best:.6f}.pth\"\n",
    "#         torch.save({'state_dict': target_model.state_dict(), 'epoch': epoch, 'ct_hparams': ct_hparams, 'meta': meta}, best_path)\n",
    "#         swanlab.log({\"val/best\": float(best)})\n",
    "#         print(f\"[ckpt] saved best -> {best_path}\")\n",
    "\n",
    "# print(f\"Best ckpt: {best_path}, val={best:.6f}\")\n",
    "\n",
    "# swanlab.finish()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[cleanup] done\n"
     ]
    }
   ],
   "source": [
    "# Cleanup CUDA memory in Jupyter\n",
    "import gc, torch\n",
    "\n",
    "def cleanup_cuda(devices=None, names=None):\n",
    "    # 1) 尝试把可能的大对象搬回 CPU 并移除全局引用\n",
    "    if names is None:\n",
    "        names = [\n",
    "            'model', 'target_model', 'optimizer', 'scaler',\n",
    "            'train_loader', 'val_loader', 'dm',\n",
    "            'imgs', 'vimgs', 'latents', 'latents_cpu', 'latents_stacked', 'imgs_sample',\n",
    "            'ae',  # EncoderDecoder 容器\n",
    "        ]\n",
    "    for n in names:\n",
    "        obj = globals().pop(n, None)\n",
    "        if obj is None:\n",
    "            continue\n",
    "        try:\n",
    "            # nn.Module / Tensor-like\n",
    "            if hasattr(obj, 'to'):\n",
    "                obj.to('cpu')\n",
    "            # EncoderDecoder: 把子模块搬回 CPU\n",
    "            if hasattr(obj, 'gen') and hasattr(obj.gen, 'to'):\n",
    "                obj.gen.to('cpu')\n",
    "        except Exception as e:\n",
    "            print(f'[cleanup] skip moving {n}: {e}')\n",
    "        del obj\n",
    "    # 2) 垃圾回收 + 清空各卡缓存\n",
    "    gc.collect()\n",
    "    if devices is None:\n",
    "        devices = list(range(torch.cuda.device_count()))\n",
    "    for i in devices:\n",
    "        try:\n",
    "            with torch.cuda.device(i):\n",
    "                torch.cuda.empty_cache()\n",
    "        except Exception:\n",
    "            pass\n",
    "    try:\n",
    "        torch.cuda.ipc_collect()\n",
    "    except Exception:\n",
    "        pass\n",
    "    print('[cleanup] done')\n",
    "\n",
    "# 针对你现在的情形（可能 0 和 4 被占）\n",
    "cleanup_cuda(devices=[0, 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using config: configs/cd4mt_small_plus.yaml\n",
      "Found 1290 tracks.\n",
      "sr=44100, min: 10, max: 600\n",
      "Keeping 1289 of 1290 tracks\n",
      "Data size: 26309\n",
      "Use mixup rate of 0.0; Use SpecAug (T,F) of (0, 0); Use blurring effect or not False\n",
      "| Audiostock Dataset Length:26309 | Epoch Length: 26309\n",
      "Found 271 tracks.\n",
      "sr=44100, min: 10, max: 600\n",
      "Keeping 270 of 271 tracks\n",
      "Data size: 5422\n",
      "Use mixup rate of 0.0; Use SpecAug (T,F) of (0, 0); Use blurring effect or not False\n",
      "| Audiostock Dataset Length:5422 | Epoch Length: 5422\n",
      "Found 152 tracks.\n",
      "sr=44100, min: 10, max: 600\n",
      "Keeping 151 of 152 tracks\n",
      "Data size: 3249\n",
      "Use mixup rate of 0.0; Use SpecAug (T,F) of (0, 0); Use blurring effect or not False\n",
      "| Audiostock Dataset Length:3249 | Epoch Length: 3249\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "EncoderDecoder requires explicit 'device' (e.g., torch.device('cuda:4') or 'cpu')",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[71], line 21\u001b[0m\n\u001b[1;32m     19\u001b[0m train_model_config \u001b[38;5;241m=\u001b[39m cfg[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m     20\u001b[0m train_unet_config \u001b[38;5;241m=\u001b[39m train_model_config\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124munet_\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 21\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mScoreDiffusionModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43munet_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_unet_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtrain_model_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m device \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     24\u001b[0m model \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[0;32m/data1/yuchen/cd4mt/ldm/models/diffusion/cd4mt_diffusion.py:75\u001b[0m, in \u001b[0;36mScoreDiffusionModel.__init__\u001b[0;34m(self, unet_config, cae_latent_dim, cae_z_channels, sample_rate, diffusion_sigma_distribution, diffusion_sigma_data, diffusion_dynamic_threshold, lambda_perceptual, num_stems, stem_names, support_mixture, base_learning_rate, sampling_steps, sigma_min, sigma_max, rho, monitor, **kwargs)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave_hyperparameters()\n\u001b[1;32m     72\u001b[0m \u001b[38;5;66;03m# =========================\u001b[39;00m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;66;03m# 1. Audio Auto-Encoder (CAE)\u001b[39;00m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;66;03m# =========================\u001b[39;00m\n\u001b[0;32m---> 75\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mautoencoder \u001b[38;5;241m=\u001b[39m \u001b[43mEncoderDecoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcae_latent_dim \u001b[38;5;241m=\u001b[39m cae_latent_dim\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcae_z_channels \u001b[38;5;241m=\u001b[39m cae_z_channels\n",
      "File \u001b[0;32m/data1/yuchen/cd4mt/src/music2latent/music2latent/inference.py:22\u001b[0m, in \u001b[0;36mEncoderDecoder.__init__\u001b[0;34m(self, load_path_inference, device)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Lightweight wrapper that holds the generator and routes encode/decode calls.\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \n\u001b[1;32m     16\u001b[0m \u001b[38;5;124;03mNote: We intentionally require an explicit `device` to avoid accidentally\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;124;03minitializing CUDA context on the default GPU (cuda:0). Pass a concrete\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;124;03m`torch.device`, e.g. `torch.device('cuda:4')` or `torch.device('cpu')`.\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m device \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;66;03m# Enforce explicit device selection to prevent unintended use of cuda:0\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     23\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEncoderDecoder requires explicit \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m (e.g., torch.device(\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda:4\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m) or \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     24\u001b[0m     )\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice \u001b[38;5;241m=\u001b[39m device\n",
      "\u001b[0;31mValueError\u001b[0m: EncoderDecoder requires explicit 'device' (e.g., torch.device('cuda:4') or 'cpu')"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[amax:471158] tcp_peer_recv_connect_ack: invalid header type: 114\n",
      "[amax:471158] tcp_peer_recv_connect_ack: invalid header type: 114\n",
      "[amax:471158] tcp_peer_recv_connect_ack: invalid header type: 0\n",
      "[amax:471158] tcp_peer_recv_connect_ack: invalid header type: 0\n",
      "[amax:471158] tcp_peer_recv_connect_ack: invalid header type: 97\n",
      "[amax:471158] tcp_peer_recv_connect_ack: invalid header type: 116\n",
      "[amax:471158] tcp_peer_recv_connect_ack: invalid header type: 116\n",
      "[amax:471158] tcp_peer_recv_connect_ack: invalid header type: 97\n",
      "[amax:471158] [[23851,0],0] tcp_peer_recv_blocking: recv() failed for [[513,790],16777472]: Connection reset by peer (104)\n",
      "[amax:471158] tcp_peer_recv_connect_ack: invalid header type: 34\n",
      "[amax:471158] tcp_peer_recv_connect_ack: invalid header type: 49\n",
      "[amax:471158] tcp_peer_recv_connect_ack: invalid header type: 214\n",
      "[amax:471158] tcp_peer_recv_connect_ack: invalid header type: 49\n",
      "[amax:471158] tcp_peer_recv_connect_ack: invalid header type: 122\n",
      "[amax:471158] tcp_peer_recv_connect_ack: invalid header type: 67\n",
      "[amax:471158] tcp_peer_recv_connect_ack: invalid header type: 48\n",
      "[amax:471158] tcp_peer_recv_connect_ack: invalid header type: 67\n",
      "[amax:471158] tcp_peer_recv_connect_ack: invalid header type: 48\n",
      "[amax:471158] tcp_peer_recv_connect_ack: invalid header type: 0\n",
      "[amax:471158] tcp_peer_recv_connect_ack: invalid header type: 0\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Quick small-epoch run with param summary (auto-appended)\n",
    "import os, yaml, torch\n",
    "from ldm.data.multitrack_datamodule import DataModuleFromConfig\n",
    "from ldm.models.diffusion.cd4mt_diffusion import ScoreDiffusionModel\n",
    "from ldm.modules.util import summarize_params\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "CFG_PATH = os.getenv('CFG_PATH', 'configs/cd4mt_small_plus.yaml')\n",
    "print(f'Using config: {CFG_PATH}')\n",
    "with open(CFG_PATH, 'r') as f:\n",
    "    cfg = yaml.safe_load(f)\n",
    "\n",
    "# Data\n",
    "dm = DataModuleFromConfig(**cfg['data']['params'])\n",
    "dm.prepare_data(); dm.setup('fit')\n",
    "\n",
    "# Model\n",
    "train_model_config = cfg['model']['params'].copy()\n",
    "train_unet_config = train_model_config.pop('unet_')\n",
    "model = ScoreDiffusionModel(unet_config=train_unet_config, **train_model_config)\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model = model.to(device)\n",
    "\n",
    "# Param summary\n",
    "try:\n",
    "    summarize_params(model.audio_diffusion, name='AudioDiffusionModel_2d')\n",
    "    if hasattr(model.audio_diffusion, 'unet'):\n",
    "        summarize_params(model.audio_diffusion.unet, name='UNet (inner)')\n",
    "except Exception as e:\n",
    "    print('summarize_params failed:', e)\n",
    "\n",
    "# Trainer (small)\n",
    "checkpoint_dir = './training_logs/checkpoints_nb'\n",
    "ckpt_cb = ModelCheckpoint(dirpath=checkpoint_dir, filename='nb-test-{epoch:02d}-{step:04d}', save_top_k=1, save_last=True, every_n_epochs=1, verbose=True)\n",
    "trainer = pl.Trainer(\n",
    "    accelerator='gpu' if torch.cuda.is_available() else 'cpu',\n",
    "    devices=1,\n",
    "    max_epochs=1,\n",
    "    limit_train_batches=2,\n",
    "    limit_val_batches=1,\n",
    "    num_sanity_val_steps=0,\n",
    "    logger=False,\n",
    "    callbacks=[ckpt_cb],\n",
    "    precision=cfg.get('trainer', {}).get('precision', 16),\n",
    ")\n",
    "\n",
    "trainer.fit(model, dm)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Model] CM.UNet (standalone): params=67,550,336 (trainable=67,550,336), param_mem≈257.7 MB (dtype=torch.float32)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'name': 'CM.UNet (standalone)',\n",
       " 'total_params': 67550336,\n",
       " 'trainable_params': 67550336,\n",
       " 'buffers': 0,\n",
       " 'bytes_per_param': 4,\n",
       " 'param_mem_mb': 257.68408203125,\n",
       " 'dtype': 'torch.float32'}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CM UNet param size only (no training)\n",
    "import os, yaml, torch, numpy as np\n",
    "from src.cm.script_util import create_model\n",
    "from ldm.modules.util import summarize_params\n",
    "\n",
    "CFG_PATH = os.getenv('CFG_PATH', 'configs/cd4mt_small_plus.yaml')\n",
    "with open(CFG_PATH, 'r') as f:\n",
    "    cfg = yaml.safe_load(f)\n",
    "\n",
    "S = int(cfg.get('model', {}).get('params', {}).get('num_stems', 4))\n",
    "C_lat = int(cfg.get('model', {}).get('params', {}).get('cae_z_channels', 64))\n",
    "in_channels = S * C_lat\n",
    "cmov = cfg.get('model', {}).get('params', {}).get('cm_model_override', {})\n",
    "\n",
    "args = dict(\n",
    "    image_size=cmov.get('image_size', 32),\n",
    "    num_channels=cmov.get('num_channels', 128),\n",
    "    num_res_blocks=cmov.get('num_res_blocks', 2),\n",
    "    channel_mult=cmov.get('channel_mult', '1,2,4'),\n",
    "    learn_sigma=cmov.get('learn_sigma', False),\n",
    "    class_cond=cmov.get('class_cond', False),\n",
    "    use_checkpoint=cmov.get('use_checkpoint', False),\n",
    "    attention_resolutions=cmov.get('attention_resolutions', '16,8,4'),\n",
    "    num_heads=cmov.get('num_heads', 4),\n",
    "    num_head_channels=cmov.get('num_head_channels', 32),\n",
    "    num_heads_upsample=cmov.get('num_heads_upsample', -1),\n",
    "    use_scale_shift_norm=cmov.get('use_scale_shift_norm', True),\n",
    "    dropout=cmov.get('dropout', 0.1),\n",
    "    resblock_updown=cmov.get('resblock_updown', False),\n",
    "    use_fp16=cmov.get('use_fp16', False),\n",
    "    use_new_attention_order=cmov.get('use_new_attention_order', False),\n",
    ")\n",
    "\n",
    "cm_unet = create_model(**args)\n",
    "# Adjust I/O conv to match multitrack channels\n",
    "try:\n",
    "    if hasattr(cm_unet, 'input_blocks') and hasattr(cm_unet.input_blocks[0][0], 'weight'):\n",
    "        orig_in = cm_unet.input_blocks[0][0]\n",
    "        cm_unet.input_blocks[0][0] = torch.nn.Conv2d(\n",
    "            in_channels, orig_in.out_channels,\n",
    "            kernel_size=orig_in.kernel_size, stride=orig_in.stride, padding=orig_in.padding,\n",
    "            bias=(orig_in.bias is not None),\n",
    "        )\n",
    "    if hasattr(cm_unet, 'out') and hasattr(cm_unet.out[-1], 'weight'):\n",
    "        orig_out = cm_unet.out[-1]\n",
    "        cm_unet.out[-1] = torch.nn.Conv2d(\n",
    "            orig_out.in_channels, in_channels,\n",
    "            kernel_size=orig_out.kernel_size, stride=orig_out.stride, padding=orig_out.padding,\n",
    "            bias=(orig_out.bias is not None),\n",
    "        )\n",
    "except Exception as e:\n",
    "    print('Adjust I/O conv failed:', e)\n",
    "\n",
    "summarize_params(cm_unet, name='CM.UNet (standalone)')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (cdp10)",
   "language": "python",
   "name": "cdp10"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
